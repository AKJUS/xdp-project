# -*- fill-column: 79; -*-
#+TITLE: Introduction to: XDP and BPF building blocks
#+AUTHOR: Jesper Dangaard Brouer <brouer@redhat.com>
#+EMAIL: brouer@redhat.com
#+REVEAL_THEME: redhat
#+REVEAL_TRANS: linear
#+REVEAL_MARGIN: 0
#+REVEAL_EXTRA_JS: { src: '../reveal.js/js/redhat.js'}
#+REVEAL_ROOT: ../reveal.js
#+OPTIONS: reveal_center:nil reveal_control:t reveal_history:nil
#+OPTIONS: reveal_width:1600 reveal_height:900
#+OPTIONS: ^:nil tags:nil toc:nil num:nil ':t

* For conference: ebplane 2019                                     :noexport:

This presentation will be given at Junipers HQ in Sunnyvale, Oct 21st 2019.

** Abstract

The ebplane project is in an early startup phase. Thus, use-cases and what the
technology is planned to be used for exactly, are still not well defined.

The ebplane project have a clear interest in leveraging eBPF technology within
computer networking area. The two most successfully eBPF networking hooks in
the Linux kernel are XDP (eXpress Data Path) and TC-BPF (Traffic Control).

This presentation serves as an introduction to the BPF network technologies,
with a focus on XDP and TC. Given the lack of clear use-cases, the presentation
will generalise and introduce the technology in form of describing the building
blocks available.

Understanding the building blocks and their limitations are actually essential
for the success of the project. As it requires thinking differently when
developing an "application" with BPF. The key insight is that you are not
developing a new "application" e.g. data plane from scratch. Instead you are
modifying the behaviour of an existing system (the Linux kernel), to do what
you want, via injecting code snippets at different hooks, that are only event
based. The BPF code snippets are by default stateless, but can obtain state and
change runtime behaviour via BPF-maps.

Q: How can we talk about gaps, when use-cases are undefined?

The BPF+XDP technology are under active development, which is both good and
bad. The bad news is that there are likely gaps for e.g. developing a data
plane. But the good news is that we can address these gaps, given upstream
kernel maintainers are participating. The presentation will cover some of these
gaps, and explain how BPF can be extended. With a little clever thinking, some
of these gaps can be addressed by doing fall-back to kernel network stack, for
slow(er) code-path handling.

If timer permits, we will also present some of the planned extensions to XDP
and BPF.

Q: Should we have close to the "code" section? Where we e.g. describe some of
the fundamental data structures?

** Agenda planning

https://pad.sfconservancy.org/p/ebplane-20191021-agenda

** Other material

Juniper slides:
https://docs.google.com/presentation/d/1JHrl8PlLyVRSMvtF8OUa3BW3GcRf4a3Kx2CPw2g7tJg/edit?ts=5d542a23#slide=id.p


* Colors in slides                                                 :noexport:
Text colors on slides are chosen via org-mode italic/bold high-lighting:
 - /italic/ = /green/
 - *bold*   = *yellow*
 - */italic-bold/* = red

* Slides below                                                     :noexport:

Only sections with tag ":export:" will end-up in the presentation. The prefix
"Slide:" is only syntax-sugar for the reader (and it removed before export by
emacs).

* Slide: Overview: What will you learn?                            :noexport:

EMPTY SLIDE
- Need to "finish" slide-deck before doing overview slide

* Slide: The 'ebplane' project                                       :export:

The ebplane project: *early startup phase*
- Initial presentation title: [[https://docs.google.com/presentation/d/1JHrl8PlLyVRSMvtF8OUa3BW3GcRf4a3Kx2CPw2g7tJg/edit?ts=5d542a23#slide=id.p]["Universal Data Plane Proposal"]]
  - Show interest in /leveraging eBPF technology for networking/
- /*Yet to be defined*/: use-cases and network-layers to target

This presentation: /eBPF technology level setting/
- /Building blocks/ and their *limitations*
- Designing with eBPF requires slightly different thinking...
  - essential for success of this project

** Slide: Different design thinking: High level overview            :export:

*/Wrong thinking/*: Designing new data plane *from scratch*

Key insight#1: /Modifying behaviour of existing system/ (Linux kernel)
- Via injecting /code snippets/ at different *hooks*
- BPF code snippets are *event-based* and by default stateless
- Obtain state and change runtime behaviour via shared BPF-*maps*

Key insight#2: /Only load code when actually needed/
- The fastest code is code that doesn't run (or even loaded)
- Design system to *only load code relevant to user* configured use-case
- E.g. don't implement generic parser to handle every know protocol
  - instead create parser specific to users need/config

* Slide: Basic introduction and understanding of eBPF                :export:
:PROPERTIES:
:reveal_extra_attr: class="mid-slide"
:END:

*Technical:* /Level setting slides for eBPF technology/

Basic introduction and understanding of BPF
- eBPF bytecode
- Compiling restricted-C to eBPF
  * compiler storing it in ELF-format
  * loading this into the Linux kernel

** Slide: eBPF bytecode and kernel hooks                            :export:

The eBPF bytecode is:
- /Generic Instruction Set/ Architecture (ISA) with C-calling convention
  * Read: the eBPF assembly language
- Designed to *run in the Linux kernel*
  * It is */not a kernel module/*
  * It is a *sandbox technology*; BPF verfier ensures code safety
  * Kernel provides /eBPF runtime/ environment, via BPF /helper calls/

Different Linux kernel /hooks/ run eBPF-bytecode, /event/ triggered
- Two hooks of special interest XDP and TC-BPF
- Many more eBPF hooks (tracepoint, all function calls via kprobe)

** Slide: Compiling restricted-C to eBPF into ELF                   :export:

/LLVM compiler/ has an eBPF backend (to */avoid writing eBPF assembly/* by hand)
- Write *Restricted C* -- some limits imposed by sandbox BPF-verfier

Compiler produces an standard ELF "executable" file
- Cannot execute this file directly, as the eBPF runtime is inside the kernel
- Need our *own ELF loader* that can:
  * Extract the eBPF bytecode and eBPF maps
  * Do ELF relocation of eBPF maps references in bytecode
  * Create/load eBPF maps and bytecode into kernel
- *Attaching to hook is separate* step

** Slide: Recommend using libbpf                                    :export:

Recommend using /libbpf/ as the *ELF loader for eBPF*
- libbpf is /part of Linux kernel tree/
- Facebook fortunately *exports* this to https://github.com/libbpf
  * XDP-tutorial git repo, use [[https://github.com/libbpf/libbpf][libbpf]] as git-submodule

Please userspace apps: *Everybody should use this library*
- */Unfortunately/* several loaders exists
- Worst case is iproute2 have its own
  * cause incompatible ELF object, if using eBPF maps
  * (plan to converting iproute2 to use libbpf)

** Slide: eBPF concepts: /context/, /maps/ and /helpers/            :export:

Each eBPF /runtime event hook/ gets a *pointer to a* /context/ struct
- BPF bytecode has access to context (read/write limited)
  * verifier may adjust the bytecode for safety

The BPF program itself is *stateless*
- /Concept eBPF maps/ can be used to *create state* and *"config"*
- Maps are basically /key = value/ construct

/BPF helpers/ are used for
- Calling kernel functions, to obtain info/state from kernel


* Slide: Introducing XDP                                             :export:
:PROPERTIES:
:reveal_extra_attr: class="mid-slide"
:END:

ebplane: leverage eBPF technology for networking
- One option is /XDP (eXpress Data Path)/
  - When targeting *network layers L2-L3*
  - L4 use-cases comes with some caveats

** Slide: Framing XDP                                             :noexport:
#+BEGIN_NOTES
SKIP THIS SLIDE - content covered in next slides
#+END_NOTES

XDP: new /in-kernel programmable/ (eBPF) *layer before netstack*
 - Similar speeds as DPDK
XDP ensures that *Linux networking stays relevant*
 - Operates at L2-L3, netstack is L4-L7
XDP is not first mover, but we believe XDP is /different and better/
 - /Killer feature/: Integration with Linux kernel
 - Flexible sharing of NIC resources

** Slide: What is XDP?                                              :export:

XDP (eXpress Data Path) is a Linux *in-kernel* fast-path
 - /New programmable layer in-front/ of traditional network stack
   - Read, modify, drop, redirect or pass
 - For L2-L3 use-cases: seeing x10 performance improvements!
   - Similar speeds as DPDK
 - Can accelerate *in-kernel* L2-L3 use-cases (e.g. forwarding)

What is /AF_XDP/? (the Address Family XDP socket)
 - /Hybrid/ *kernel-bypass* facility via XDP_REDIRECT filter
 - Delivers raw L2 frames into userspace (in SPSC queue)

** Slide: What makes XDP different and better?                      :export:

*Not bypass*, but /in-kernel fast-path/

The killer feature of XDP is integration with Linux kernel,
 - Leverages existing kernel infrastructure, eco-system and market position
 - Programmable flexibility via eBPF sandboxing (kernel infra)
 - Flexible sharing of NIC resources between Linux and XDP
 - Cooperation with netstack via eBPF-helpers and fallback-handling
 - No need to reinject packets (unlike bypass solutions)

/AF_XDP/ for /flexible/ *kernel bypass*
 - Cooperate with use-cases needing fast raw frame access in userspace
 - No kernel reinject, instead choose before doing XDP_REDIRECT

** Slide: Simple view on how XDP gains speed                        :export:

XDP speed gains comes from
- *Avoiding* /memory allocations/
  - no SKB allocations and no-init (memset zero 4 cache-lines)
- /Bulk/ processing of frames
- Very /early access/ to frame (in driver code *after DMA sync*)
- Ability to */skip/ (large parts) of kernel /code/*

** Slide: Skipping code: Efficient optimization                     :export:

@@html:<small>@@
/Encourage adding helpers instead of duplicating data in BPF maps/
@@html:</small>@@

Skipping code: *Imply skipping features* provided by /network stack/
- Gave users freedom to e.g. skip netfilter or route-lookup
- But users have to re-implement features they actually needed
  - Sometimes cumbersome via BPF-maps

/Avoid/ *re-implement features*:
- Evolve XDP via /BPF-helpers/ that can lookup in kernel tables
- Example of BPF-helpers avail today for XDP:
  - FIB routing lookup
  - Socket lookup

** Slide: XDP actions and cooperation                               :export:

@@html:<small>@@
What are the basic XDP building blocks you can use?
@@html:</small>@@

BPF programs return an action or verdict, for XDP 5-actions:
- XDP_ /DROP/,  XDP_ /PASS/,  XDP_ /TX/,  XDP_ /ABORTED/,  XDP_ /REDIRECT/

Ways to /cooperate with network stack/
- Pop/push or /modify headers/: *Change RX-handler* kernel use
  * e.g. handle protocol unknown to running kernel

- Can /propagate/ 32Bytes /metadata/ from XDP stage to network stack
  * TC (cls_bpf) hook can use metadata, e.g. set SKB mark

- /XDP_REDIRECT/ *map* special, can choose where netstack "starts/begin"
  * CPUMAP redirect start netstack on remote CPU
  * veth redirect start inside container

* Slide: Introducing TC-BPF                                          :export:

ebplane: leverage eBPF technology for networking
- Another option are /TC (Traffic Control)/ BPF-hooks
  - When targeting *network layers L4-L7*
  - L2-L3 are of-cause still possible


** Slide: What is TC-BPF?                                           :export:
#+BEGIN_NOTES
Q: What module to promote act_bpf or cls_bpf ?

Two program types:
	case BPF_PROG_TYPE_SCHED_CLS:
	case BPF_PROG_TYPE_SCHED_ACT:
#+END_NOTES

The Linux /TC (Traffic Control)/ system have some BPF-hook points
- In *TC filter* 'classify' step: both /ingress/ and /egress/



* Slide: Design perspective                                          :export:
:PROPERTIES:
:reveal_extra_attr: class="mid-slide"
:END:

Higher level: Design perspective
- from a /BPF view point/

** Slide: BPF view on: data-plane and control-plane                 :export:

@@html:<small>@@
/This covers both XDP and TC networking hooks/
@@html:</small>@@

*Data-plane*: /inside kernel/, split into:
- Kernel-core: Fabric in charge of moving packets quickly
- In-kernel eBPF program:
  * Policy logic decide *action* (e.g. pass/drop/redirect)
  * Read/write access to packet

*Control-plane*: in /userspace/
- Userspace load eBPF program
- Can /control program via/ changing /BPF maps/
- Everything goes through /bpf system call/

** Slide: BPF changing the kABI landscape

@@html:<small>@@
kABI = Kernel Application Binary Interface
@@html:</small>@@

Distro's spend */many/* resources maintaining kABI compatibility
- to satisfy *out-of-tree kernel modules*, calling kernel API / structs
- e.g. [[https://tungsten.io/opencontrail-is-now-tungsten-fabric/][tungsten]] contrail-vrouter [[https://github.com/Juniper/contrail-vrouter/blob/master/linux/vr_host_interface.c#L1154][kernel module]] hook into RX-handler (L2)

BPF /offer way-out/, with some *limits* due to security/safety:
- /Fully programmable/ hooks points (restricted-C, not Turing complete)
- *Access sandboxed* e.g. via context struct and BPF-helpers available
- Possible policy *actions limited by hook*

Userspace "control-plane" API tied to userspace app (not kernel API)

In-principle: BPF-instruction set and BPF-helpers are still kABI

** Deep-dive: contrail-vrouter [[https://github.com/Juniper/contrail-vrouter/blob/master/linux/vr_host_interface.c#L1154][kernel module]]                      :noexport:
#+BEGIN_NOTES
Not for slide-deck = noexport
#+END_NOTES

Code-analysis of vrouter Linux kernel module (out-of-tree)
- Notice project renamed to [[https://tungsten.io/opencontrail-is-now-tungsten-fabric/][Tungsten fabric]] ([[https://github.com/tungstenfabric/][GitHub home]])
- but found kernel module under GitHub [[https://github.com/Juniper/contrail-vrouter][Juniper/contrail-vrouter]]

Taking a deep-dive into: contrail-vrouter code
- https://github.com/Juniper/contrail-vrouter

They are basically hooking into the Linux kernel RX-handler, network L2 level.

See/follow: netdev_rx_handler_register() which call/register their code:
- vhost_rx_handler
- [[https://github.com/Juniper/contrail-vrouter/blob/master/linux/vr_host_interface.c#L1154][linux_rx_handler]]
- Or via their: linux_pkt_dev_init() can also register
  - pkt_gro_dev_rx_handler
  - pkt_rps_dev_rx_handler

They have their own 'vr_packet' data-struct, that is max 48-bytes and stored in
SKB "CB" field (skb->cb).

#+begin_src C
/*
 * NOTE: Please do not add any more fields without ensuring
 * that the size is <= 48 bytes in 64 bit systems.
 */
struct vr_packet {
    unsigned char *vp_head;
    struct vr_interface *vp_if;
    struct vr_nexthop *vp_nh;
    unsigned short vp_data;
    unsigned short vp_tail;
    unsigned short vp_len;
    unsigned short vp_end;
    unsigned short vp_network_h;
    unsigned short vp_flags;
    unsigned short vp_inner_network_h;
    unsigned char vp_cpu;
    unsigned char vp_type;
    unsigned char vp_ttl;
    unsigned char vp_queue;
    unsigned char vp_priority:4,
                  vp_notused:4;
};
#+end_src

Recommend looking at [[https://github.com/Juniper/contrail-vrouter/blob/master/linux/vr_host_interface.c#L938][linux_get_packet()]] to see how an SKB is converted into a
struct vr_packet, via storing it into =skb->cb= area. When they pass-along
their vr_packet, then can get back to the SKB via container_of tricks.

#+begin_src C
static inline struct sk_buff *
vp_os_packet(struct vr_packet *pkt)
{
    return CONTAINER_OF(cb, struct sk_buff, pkt);
}
#+end_src

They end-up consuming the packet via calling indirect function call vif_rx():
#+begin_src C
    ret = vif->vif_rx(vif, pkt, vlan_id);
    if (!ret)
        ret = RX_HANDLER_CONSUMED;
#+end_src

In the vif_rx function can e.g. be assigned to:
- (vr_interface.c) eth_rx
- (vr_interface.c) vhost_rx

End up calling: vr_fabric_input ([[https://github.com/Juniper/contrail-vrouter/blob/master/dp-core/vr_datapath.c][dp-core/vr_datapath.c]])


* Designing with BPF for XDP+TC                                      :export:
:PROPERTIES:
:reveal_extra_attr: class="mid-slide"
:END:

Examples of designing with BPF

** Slide: Design protocol parser with BPF for XDP/TC                :export:

@@html:<small>@@
Background: XDP/TC metadata area placed in-front packet headers (32 Bytes).
Works as communication channel between XDP-tail-calls, and into TC-BPF hook
@@html:</small>@@

Split BPF-prog /parser-step/ into standalone BPF-prog
- Output is *parse-info with header types and offsets*
- Parse-info is /stored in XDP/TC metadata area/ (in-front packet headers)

*Tail-call next BPF-prog*, have /access to metadata/ area
- Due to verifier, prog getting parse-info still need some bounds-check

Advantage: Parser prog can be replaced by hardware

** Slide: Design to load less-code                                  :export:

Generic netstack is also slow because
- Need to *handle every known protocol* (cannot fit in Instruction-Cache)

BPF give ability to /runtime change and load new code/
- The 'ebplane' design should take advantage of this

Specifically for: *Protocol parsing* "module"
- Don't create huge BPF-prog that can parse everything
- Idea: Domain Specific Language (maybe P4) for BPF-prog parsing step
  * Users /describe protocols/ relevant for them, and /parse-info struct/
  * *Result*: /smaller BPF-prog for parsing/ (less Instruction-Cache usage)
  * (Make sure this can also be compiled for HW targets)

* Notes

** Topic: XDP redirect into containers

Containers are in-many-use-case better handled via TC-BPF

** Topic: XDP redirect into Guest-VM

** Topic: How to test XDP

** Topic: IP-fragmentation

** Topic: Introduce TC-BPF hook

TC-BPF also (like XDP) have "direct access" mode to packet-data
- But access limited to memory-linear part of packet-data
- Thus, how much is accessible depending on how SKB were created

XDP also have direct access, but force drivers to use memory model
- Requiring packet-data to be delivered "memory-linear" (in physical mem)

The TC 'da' option stand for 'direct-action' mode
- Meaning, BPF-prog will directly return the TC-action code (TC_ACT_*) 

*** TC-BPF code notes

Follow entry code for TC-BPF 'ingress' hook:
- sch_handle_ingress() call tcf_classify()
- that finds and call cls_bpf_classify() in net/sched/cls_bpf.c

Return action codes (TC_ACT_*) avail/used in sch_handle_ingress()
#+begin_src C
switch (tcf_classify(skb, miniq->filter_list, &cl_res, false)) {
	case TC_ACT_OK:
	case TC_ACT_RECLASSIFY:
		skb->tc_index = TC_H_MIN(cl_res.classid);
		break;
	case TC_ACT_SHOT:
		mini_qdisc_qstats_cpu_drop(miniq);
		kfree_skb(skb);
		return NULL;
	case TC_ACT_STOLEN:
	case TC_ACT_QUEUED:
	case TC_ACT_TRAP:
		consume_skb(skb);
		return NULL;
	case TC_ACT_REDIRECT:
		/* skb_mac_header check was done by cls/act_bpf, so
		 * we can safely push the L2 header back before
		 * redirecting to another netdev
		 */
		__skb_push(skb, skb->mac_len);
		skb_do_redirect(skb);
		return NULL;
	case TC_ACT_CONSUMED:
		return NULL;
	default:
		break;
	}
#+end_src

Verify that/how TC-BPF gets DA (Direct Access) only to memory-linear part of
packet data from the SKB.

In cls_bpf_classify() the BPF-prog invocation happens via =BPF_PROG_RUN=, and
prior to each =BPF_PROG_RUN= the function bpf_compute_data_pointers() are
called.  It is responsible for computing the memory-linear part:

#+begin_src C
/* Compute the linear packet data range [data, data_end) which
 * will be accessed by various program types (cls_bpf, act_bpf,
 * lwt, ...). Subsystems allowing direct data access must (!)
 * ensure that cb[] area can be written to when BPF program is
 * invoked (otherwise cb[] save/restore is necessary).
 */
static inline void bpf_compute_data_pointers(struct sk_buff *skb)
{
	struct bpf_skb_data_end *cb = (struct bpf_skb_data_end *)skb->cb;

	BUILD_BUG_ON(sizeof(*cb) > FIELD_SIZEOF(struct sk_buff, cb));
	cb->data_meta = skb->data - skb_metadata_len(skb);
	cb->data_end  = skb->data + skb_headlen(skb);
}
#+end_src

The important part is =skb_headlen(skb)= that return the length of "main
buffer" that is pointed to by =skb->data=. That means it does not account for
data in fragmented parts, e.g. =frags= or =flag_list=.


** Topic: Broadcast and multicast

Cloning packets in XDP is not currently possible

XDP: Sending to multiple destination; not supported
- Simple idea: Broadcast via redirect "send" to ALL port in devmap
- Multicast via creating devmap for multicast groups

Alternative is to fallback
- let either: netstack or TC-BPF hook handle broadcast/multicast

** Topic: Missing XDP egress hook

In localhost/container delivery use-case: tunnel/overlay-network
- Encap headers need to be added by TC-BPF egress hook



* Emacs end-tricks                                                 :noexport:

# Local Variables:
# org-re-reveal-title-slide: "<h1 class=\"title\">%t</h1><h2
# class=\"author\">Jesper Dangaard Brouer<br/>Kernel Developer<br/>Red Hat</h2>
# <h3>ebplane hosted by Juniper<br/>USA, Sunnyvale, Oct 2019</h3>"
# org-export-filter-headline-functions: ((lambda (contents backend info) (replace-regexp-in-string "Slide: " "" contents)))
# End:
