\section{Evaluation}\label{sec:evaluation}
In this section we evaluate \textit{mq-cake} and show its capabilities of accurately enforcing rate limits up to the network card's capabilities of 25 Gbps and achieves excellent linear scaling with an increasing number of hardware queues.
%
Further, \textit{mq-cake} achieves 10--2500X lower tail-latencies as compared to EDT-BPF approach.

This section is organized as follows:
%
The first subsection describes the experimental setup. 
%
Next, we evaluate \textit{mq-cake}'s rate limiting capabilities and the corresponding accuracy as well as its scaling properties in comparison to HTB and single-queue CAKE.
%
In the third subsection we test and evaluate \textit{mq-cake} and EDT-BPF under TCP traffic as well as the corresponding latencies using the Flent~\cite{flent} network testing tool.
%
The fourth subsection considers the dynamic properties of \textit{mq-cake}, especially its behavior when the number of UDP flows and \textit{synctime} change.
%
The fifth subsection discusses the limitations of the current approach, especially focusing on imbalances in load between queues.
%
Lastly, we summarize the evaluation results and outline further directions for future works in this field.

\subsection{Experimental Setup}
The experimental setup consists of two identical servers, both of which are equipped with: 
(1) an Intel CPU (Intel(R) Xeon(R) Gold 6209U CPU@2.10GHz) with 20 physical cores and hyperthreading capabilities;
(2) 192GB RAM;
(3) two 25G NICs (Intel XXV710 for 25GbE SFP28 (rev 02)).
Both servers run an Ubuntu 22.04.4 LTS with a 6.5.0-35-generic kernel that contains \textit{mq-cake}. 
These machines are connected back-to-back, where one machine generates traffic and measures throughput either using MoonGen~\cite{moongen} or Flent~\cite{flent} and the other machine --- the Device under Test (DuT) --- receives the traffic, enforces a rate limit, and sends the traffic back to the traffic generating device.
%
The generated traffic using MoonGen consists only of UDP flows, with a transmission speed of 25 Gbps.
%
In case of the Flent tests, 1024 TCP streams are used to saturate the link.
%
If not mentioned otherwise, hyperthreading is enabled on the SuT.

The receiving interface of the SuT distributes the incoming flows in a round-robin fashion across its receive queues.
%
The interrupts of each receive queue are mapped to exactly one CPU core and irqbalance daemon~\cite{irqbalance} is disabled.
%
To avoid any side effects from existing firewall and routing rules, the ingress and egress interface of the SuT are attached to a separate network namespace~\cite{network-namespace}.
%
The intel\_iommu~\cite{iommu} feature is also explicitly disabled, since it is enabled by default in the 6.5 Linux kernel version and massively degrades the performance of network IO operations.
%
Further, NIC offloading capabilities like GRO, GSO and TSO~\cite{offloads} are disabled on each interface on the SuT.
%
The TIPSY framework~\cite{tipsy} is used to orchestrate tests.
%
For configuring and installing \textit{mq-cake}, the \textit{tc} command line tool is extended~\cite{mq-cake-iproute}.
%
Further, in case of the MoonGen UDP flood tests, the installed \textit{mq-cake} instances are configured with \textit{besteffort flows overhead 18 mpu 64 noatm}~\cite{cake-manual}.
%
The same options are used for single-queue CAKE.
%
The overhead compensation is configured so that the rate calculation of MoonGen and \textit{mq-cake}/single-queue CAKE are identical.
%
In order to enforce a global rate limit using HTB, we install a single class at the root-qdisc, which rate limits all network traffic.
%
As mentioned above, the EDT-BPF as presented in prior work is not suitable to enforce a global rate limit.
To enable a comparison with \textit{mq-cake}, we modify the BPF program implementation to enforce such a global rate limit and incorporate the aforementioned overhead compensation.
%
The drop-horizon is set to $2s$, following the value proposed by the authors~\cite{edt-ebpf}. The adjusted BPF program is detailed in Algorithm~\ref{alg:edt-ebpf}.

To align with the expected buffer occupation, the \lstinline{limit} and \lstinline{flow_limit} parameters of the FQ instances are configured based on the following formula:
\begin{align*}
    \text{Limit} = \frac{\text{Rate Limit}\cdot \text{Drop Horizon}}{\text{\# FQ instances}\cdot \text{Packet size} \cdot 8}
\end{align*}

If the FQ buffer sizes are not properly adjusted, FQ may drop packets unnoticed by the BPF program, ultimately impacting performance.

\begin{algorithm}[t]
    \caption{EDT-BPF implementation}\label{alg:edt-ebpf}
\begin{algorithmic}[1]
\Procedure{rate\_limit}{skb}
    \State pkt\_len = skb$\rightarrow$len + compensation
    \State {delay\_ns = pkt\_len*NS\_PER\_SEC/global\_rate\_limit}
    \State next\_tstamp = global\_next\_tstamp
    \\
    \If {next\_tstamp $\leq$ now}
        \State global\_next\_tstamp = now + delay\_ns
        \State \Return TC\_ACT\_OK
    \EndIf
    \\
    \If {next\_tstamp$-$now $\geq$ DROP\_HORIZON}
        \State \Return TC\_ACT\_SHOT
    \EndIf
    \\
    \State skb$\rightarrow$tstamp = next\_tstamp
    \State \_\_sync\_fetch\_and\_add(global\_next\_tstamp, delay\_ns)
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Accuracy and Scalability}
\begin{figure*}
    \begin{subfigure}{0.3\linewidth}
        \centering
        \includesvg[scale=0.5]{images/tp_rate_conformance_htb_cake_mq-cake}
        \caption{Achieved throughput}\label{fig:tp_rate_conformance}
    \end{subfigure}
    \hfill
    \hspace{0.5cm}
    \begin{subfigure}{0.3\linewidth}
        \centering
        \includesvg[scale=0.5]{images/tp_deviation_total_htb_cake_mq-cake}
        \caption{Total deviation}\label{fig:tp_deviation_total}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.3\linewidth}
        \centering
        \includesvg[scale=0.5]{images/tp_deviation_perc_htb_cake_mq-cake}
        \caption{Relative absolute deviation}\label{fig:tp_deviation_perc}
    \end{subfigure}
    \caption{Achieved throughput and deviation from the target rate at various rate limits under
    network traffic containing only full MTU-sized packets}\label{fig:accuracy}
\end{figure*}
The most pressing questions about the presented approach is: How accurately does it enforce the configured rate limit? And how does it scale with an increasing number of hardware queues?
In this section, we present an in-depth analysis of \textit{mq-cake}'s performance using an unresponsive UDP traffic flood and compare it to the single-queue sch\_cake and sch\_htb. 
We demonstrate that \textit{mq-cake} not only achieves excellent rate conformance but also exhibits near-perfect scaling properties.

Figure~\ref{fig:tp_rate_conformance} shows the achieved throughput for varying rate limits, ranging from 10 Mbps to 24 Gbps.
%
In this test run, the network traffic consists of 120 UDP flows containing only full MTU-sized packets.
%
The number of receive and transmission queues is set to 40, meaning that every available logical CPU is assigned one receive and transmission queue.
%
These settings maximize the achievable throughput and reduce concurrent access to the same qdisc by distributing packet handling across the per-transmission queue qdisc instances.
%
Figure~\ref{fig:tp_deviation_total} highlights the total deviation from the configured maximum rate limit as well as a relative percentage of the rate limit (Figure~\ref{fig:tp_deviation_perc}).
%
Together, these plots demonstrate that both \textit{mq-cake} is able to shape traffic up to 24 Gbps, with a maximum deviation of less than
0.25\%. HTB and single-queue CAKE, on the other hand, plateau at around 7--8 Gbps.

\begin{figure}[h]
    \centering
    \includesvg[scale=0.5]{images/txq_scaling_64_htb_cake_mq-cake}
    \caption{Achieved throughput in relation to the number of available hardware queues for 64 byte packets and a 20 Gbps rate limit}\label{fig:scaling}
\end{figure}
To show the scalability traits of \textit{mq-cake}, we configure the next experiment with a rate limit of 20 Gbps and reduce the UDP packet sizes to 64 bytes.
%
Further, we ensure that the number of receive queues always matches the number of transmission queues, preventing imbalances in the traffic load between qdiscs.
%
The effect of these imbalances are further explained in the Limitations section.
%

Figure~\ref{fig:scaling} reveals the throughput achieved by \textit{mq-cake}, HTB, and single-queue CAKE in relation to the number of available hardware queues.
%
The test shows that both HTB's and single-queue CAKE's performance degrades as more hardware queues become available.
This is due to lock contention, which increases as the number of receive queues grows. 
Under these conditions, an increasing number of CPUs attempt to access the qdisc, which then increases the overall wait time to acquire the root lock. 
\textit{mq-cake}, on the other hand, scales linearly â€” the achieved throughput increases at a quicker rate up to 20 transmission
queues, after which point the improvement reduces due the use of hyperthreading cores.
This effect is due to resource-sharing between the two logical cores residing in one physical core:
Thus, their performance is not completely independent from one another.
However, even with hyperthreading enabled, \textit{mq-cake} is still able to increase the throughput.

\subsection{Impact of Synctime}\label{sec:synchronization-time}
Up to this point, the traffic in the previous evaluations has been held static, meaning that the number of flows did not change.
%
With our next experiment, we show and evaluate the impact of the synctime on the rate limiter's accuracy, particularly when the number of active queues changes.
%
To gain insights in \textit{mq-cake}'s accuracy and responsiveness, we induce a change in the number of active queues by increasing the UDP traffic from initial 4 flows to 40 flows.
%

% In order to investigate the impact of the synchronization time on the rate limiter's accuracy, the number of flows in the following test configuration is changed by switching from the  initial 4 flows to 40 flows.
%
\begin{figure}
    \centering
    \includesvg[scale=0.5]{images/switching_200us}
    \caption{\textit{mq-cake}'s behavior when switching from 4 to 40 flows with a 200$\mu s$ synchronization time, full MTU-sized packets, and 40 transmission queues}\label{fig:switching_200us}
\end{figure}
%
\begin{figure*}
    \begin{subfigure}{0.3\linewidth}
        \centering
        \includesvg[scale=0.5]{images/qlen_qdel_200us_2}
        \caption{Rate limit 2 Gbps.}\label{fig:qlen_qdel_200us_2}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.3\linewidth}
        \centering
        \includesvg[scale=0.5]{images/qlen_qdel_200us_8}
        \caption{Rate limit 8 Gbps.}\label{fig:qlen_qdel_200us_8}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.3\linewidth}
        \centering
        \includesvg[scale=0.5]{images/qlen_qdel_200us_15}
        \caption{Rate limit 15 Gbps.}\label{fig:qlen_qdel_200us_15}
    \end{subfigure}
    \caption{Induced queue lengths and delays at varying \textit{synctimes} and at a configured global rate limit of 2, 8, and 15 Gbps}\label{fig:qlen_qdel}
\end{figure*}

Figure~\ref{fig:switching_200us} shows such a switching event between 4.94s and 4.95s.
%
During the switch, the throughput spikes due to \textit{mq-cake}'s inaccurate estimation of the number of active queues.
%
Before the switch, only 4 queues were active; during the switch, the remaining 36 inactive queues are activated and then scan all other qdiscs to estimate their local rate limit.
%
Since this scanning is not necessarily executed simultaneously, the active queue estimation per qdisc will likely be lower than 40 --- not all qdiscs will have already enqueued or transmitted a packet at the point of scanning.
%
Further, the 4 already-active qdiscs will not immediately update their estimated rate upon new flow arrivals, which can delay their local rate limit reduction.
%
These conditions result in the observed overshoot in Figure~\ref{fig:switching_200us}.
%

\begin{figure}[h]
    \centering
    \includesvg[scale=0.5]{images/sync_txq_64}
    \caption{Achieved packet rate based on the available transmission queues for varying \textit{synctimes}. The traffic consists of 4 flows containing only 64 byte packets. In case of a \textit{synctime} of $0us$, the rate estimation is done for every packet.}\label{fig:sync_txq_64}
\end{figure}
%
When evaluating accuracy, it is important to consider the induced queue length at the next bottleneck in the packet path, which is caused by the throughput spike, as well as the increased latencies it produces.
%
The width of the spike can be controlled by manipulating the \textit{synctime}.
%
Exceeding the global rate limit leads to buffering excess packets on the next bottleneck link.
%
This buffering increases latencies and may lead to packet drops.
%
To gain insights into the amount of induced latencies and to provision buffer sizes, the next step is to examine these metrics in relation to the synchronization time. 
%
Figure~\ref{fig:qlen_qdel} outlines the induced queue lengths as well as the corresponding induced queueing delays at three different global rate limits.
%
Synchronization times beyond 100$\mu s$ increase the spike's overshoot as well as its duration for the reasons described above.
%
The longer the \textit{synctime}, the longer queues will send an inordinately high number of bytes due to their inaccurate local rate estimation.
%
These plots also clearly show that reducing the \textit{syncime} also inhibits the spike intensity as well as the queueing delay.
%
However, if the \textit{synctime} is too greatly reduced (i.e.\ less than 50$\mu s$ in the conducted experiments), the overhead of the synchronization loop increases, lowering the achieved throughput.
%
Figure~\ref{fig:sync_txq_64} shows the relation between the achieved rate and the number of transmission queues for different \textit{synctimes}.
%
This plot clearly shows that when the \textit{synctime} is too low, the achieved packet rate decreases due to the synchronization overhead.
%
A greater number of transmission queues increases the \textit{mq-cake} instances' scanning time and may well lead to cache misses when accessing the other qdiscs' activity metrics.
%

\subsection{TCP and Latencies}
\begin{figure}[t]
    \centering
    \hspace{-0.25cm}
    \begin{subfigure}{\linewidth}
        \centering
        \includesvg[scale=0.5]{images/tp-edtbpf-mqcake-2s-horizon-20000.svg}
        \caption{Throughput}\label{fig:flent_tp_2s}
    \end{subfigure}
    \\
    \begin{subfigure}{\linewidth}
        \centering
        \includesvg[scale=0.5]{images/ping-edtbpf-mqcake-2s-horizon-20000-log.svg}
        \caption{Ping}\label{fig:flent_ping_2s}
    \end{subfigure}
    \caption{Flent tcp\_nup test with 1024 TCP streams, a configured rate limit of 20 Gbps, and a $2s$ drop horizon for EDT-BPF}\label{fig:flent_2s}
\end{figure}

The previous experiments are based on unresponsive UDP traffic.
To review how these approaches perform with a packet-loss sensitive transport protocol, we inspect the rate conformance under TCP traffic and the resulting latencies using the network testing tool Flent~\cite{flent} and the TCP Cubic~\cite{tcp-cubic} algorithm.
%
Flent's tcp\_nup test is executed using 1024 TCP upload streams.
%
Figure~\ref{fig:flent_2s} compares the performance of \textit{mq-cake} and EDT-BPF under a global rate limit of 20 Gbps.

\textit{mq-cake} maintains stable rate enforcement, remaining slightly below the configured limit, whereas EDT-BPF initially exceeds the rate limit before gradually reducing throughput in the latter half of the test (Figure~\ref{fig:flent_tp_2s}).
%
Figure~\ref{fig:flent_ping_2s} shows the latencies measured during the test execution. \textit{mq-cake} achieves $0.4 ms$ latencies at the 99th percentile, a 2500x improvement as compared to EDT-BPF.
%
The drop horizon of EDT-BPF directly corresponds with the expected latencies.
%

Figure~\ref{fig:flent_5ms} shows the same test execution as before but with a reduced drop horizon of $5ms$ for the EDT-BPF approach. Figure~\ref{fig:flent_tp_5ms} reveals that lowering the drop horizon to $5ms$ not only stabilizes EDT-BPF's rate conformance but also reduces the tail latencies to $5ms$ (Figure~\ref{fig:flent_ping_5ms}).
%
However, reducing the drop horizon only works effectively, if the RTT's of the TCP flows have similar values as the drop horizon, thus high RTT flows would suffer from such a low drop horizon.
Even with this configuration \textit{mq-cake} achieves 10x lower latencies as compared to EDT-BPF. 

These experiments highlight \textit{mq-cake}'s ability to maintain low latencies without the need of a backpressure mechanism, thus making it suitable not only for end hosts but also in packet forwarding use cases. 
\begin{figure}
    \centering
    \hspace{-0.25cm}
    \begin{subfigure}{\linewidth}
        \centering
        \includesvg[scale=0.5]{images/tp-edtbpf-mqcake-5ms-horizon-20000.svg}
        \caption{Throughput}\label{fig:flent_tp_5ms}
    \end{subfigure}
    \\
    \begin{subfigure}{\linewidth}
        \centering
        \includesvg[scale=0.5]{images/ping-edtbpf-mqcake-5ms-horizon-20000-log.svg}
        \caption{Ping}\label{fig:flent_ping_5ms}
    \end{subfigure}
    \caption{Flent tcp\_nup test with 1024 TCP streams, a configured rate limit of 20 Gbps, and a $5ms$ drop horizon for EDT-BPF}\label{fig:flent_5ms}
\end{figure}

\subsection{Limitations}
Over the course of the evaluation, we showed that \textit{mq-cake} scales excellent with increasing number of hardware queues as well as reducing tail latencies.
However, we identified that the current approach has a reduced accuracy when network traffic is suboptimal distributed across the \textit{mq-cake} instances.
So far, our experimental setup ensures that the qdisc layer of the Linux kernel is saturated with packets.
%
However, under real-world conditions, this might not always be the case, as not all flows are sent at full speed or evenly distributed across transmission queues.
%
In the worst case, this can lead to imbalances between the loads of different \textit{mq-cake} instances, where traffic enqueued in one qdisc cannot saturate the estimated local rate limit while another qdisc instance is heavily flooded with packets.
%
These imbalances taint the active queue estimation and lead to much lower throughput.
%

For example, consider a case where the global rate limit is set to 10 Gbps and there is an incoming traffic of 10 Gbps.
%
In this example, 80\% of the incoming traffic is enqueued in Qdisc A and the remaining 20\% of the traffic is steered to Qdisc B. In this case, both Qdiscs will estimate two active queues and lower their rate limit to 5 Gbps each.
However, since Qdisc B can only forward 2 Gbps and Qdisc A is capped at 5 Gbps, the resulting throughput is only 7 Gbps.
%
\begin{figure}[H]
    \centering
    \includesvg[scale=0.5]{images/txq_imbalance_1514}
    \caption{Achieved throughput in relation to the number of available hardware transmission queues with different \textit{synctimes} for flows with full MTU-sized packets, where the rate limit is set to 20 Gbps and the number of receive queues is held at 40}\label{fig:txq_imbalance_1514}
\end{figure}
Figure~\ref{fig:txq_imbalance_1514} shows such an imbalance scenario.
%
In this experiment, the number of receive queues is held at 40 as the number of transmission queues increases.
%
Concentrating first on a \textit{synctime} of 200$\mu s$, this plot shows that the achieved rate worsens when the number of transmission queues surpasses half the number of receive queues.
%
At this critical juncture, the receive queues no longer distribute traffic equally across the transmission queues, which leads to the imbalanced scenario described above.
%
For example, when there are 24 transmission queues, 16 transmission queues receive double the amount of packets compared to the remaining 8 transmission queues.
%
The estimated rate of the 8 transmission queues is higher than the traffic they receive, leading to unused bandwidth and a declining throughput.
%
However, as more transmission queues are added, the imbalance is reduced. At the same time, the estimated rate for each transmission queue also decreases, leading to less unused bandwidth. 
%
These imbalances in multi-queue networking environments are well known in the literature~\cite{titan, loom, silo}.
%

\subsection{Discussion and Future Work}
The presented experiments reveal that \textit{mq-cake} is able to shape traffic up to 25 Gbps while achieving high accuracy with a deviation around 0.25\%.
%
Further, \textit{mq-cake} increases throughput with a greater number of hardware queues, achieving 14x higher packet rates as compared to single-queue CAKE and HTB.
%
\textit{mq-cake} also improves tail-latencies while being as accurate as the EDT-BPF approach.

%
The analysis of the \textit{synctime} shows that setting its value too low increases the CPU overhead.
%
As a lower bound, the synchronization time should be higher than the time it takes to complete one scan over all qdiscs.
%
On the other hand, higher \textit{synctimes} lead to less CPU load but also increase the time it takes \textit{mq-cake} to converge.
%
To balance between CPU load and accuracy, the experiments show that a \textit{synctime} value between 100--200$\mu s$ is ideal.

Looking ahead, we aim to further explore solutions for addressing load imbalances between \textit{mq-cake} instances as well as approaches to mitigate overshooting above the configured rate limit during switching events.
%
Concerning imbalances, in initial tests, we could already observe that it is feasible to add a rebalance mechanism to \textit{mq-cake}, which can improve local rate limit estimations.
%
In addition, we seek to investigate automated approaches to adjust and configure the \textit{synctime} interval and other internal configurations such as for example the \textit{memlimit}.
%
In our experiments, we observed that reducing the \textit{memlimit} had positive impacts on accuracy.
%
Furthermore, to deepen our understanding of \textit{mq-cake}'s applicability, we plan to evaluate its performance with higher-speed network cards and test it under real-world traffic conditions.
%
There is also potential to investigate other applications that could benefit from our proposed synchronization mechanisms.
