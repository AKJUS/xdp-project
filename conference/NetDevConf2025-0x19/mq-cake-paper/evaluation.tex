\section{Evaluation}\label{sec:evaluation}
After examining the implementation, this chapter analyzes and evaluates the proposed approach.
%
This evaluation reveals that \textit{MQ\_CAKE} can accurately enforce rate limits up to the network card's capabilities of 25 Gbps and achieves excellent linear scaling with an increasing number of hardware queues.
%
Further, \textit{MQ\_CAKE} achieves 10--2500X lower tail-latencies as compared to EDT-eBPF approach.

This chapter is organized as follows:
%
The first section describes the experimental setup. 
%
Next, this work evaluates \textit{MQ\_CAKE}'s rate limiting capabilities and the corresponding accuracy as well as its scaling properties in comparison to EDT-eBPF.
%
The third section considers the dynamic properties of \textit{MQ\_CAKE}, especially its behavior when the number of UDP flows and \textit{synctime} change.
%
The fourth section tests and evaluates \textit{MQ\_CAKE} and EDT-eBPF under TCP traffic using the Flent~\cite{flent} network testing tool.
%
Lastly, this work discusses the limitations of the current approach as well as open questions and future works in this field.

\subsection{Experimental Setup}
The experimental setup consists of two identical servers, both of which are equipped with: 
(1) an Intel CPU (Intel(R) Xeon(R) Gold 6209U CPU@2.10GHz) with 20 physical cores and hyperthreading capabilities;
(2) 192GB RAM;
(3) two 25G NICs (Intel XXV710 for 25GbE SFP28 (rev 02)).
Both servers run an Ubuntu 22.04.4 LTS with a 6.5.0-35-generic kernel that contains \textit{MQ\_CAKE}. 
These machines are connected back-to-back, where one machine generates traffic and measures throughput either using MoonGen~\cite{moongen} or Flent~\cite{flent} and the other machine --- the Server under Test (SuT) --- receives the traffic, enforces a rate limit, and sends the traffic back to the traffic generating device.
%
The generated traffic using MoonGen consists only of UDP flows, with a transmission speed of 25Gbps.
%
In case of the Flent tests, 1024 TCP streams are used to saturate the link.
%
If not mentioned otherwise, hyperthreading is enabled on the SuT.

The receiving interface of the SuT distributes the incoming flows in a round-robin fashion across its receive queues.
%
The interrupts of each receive queue are mapped to exactly one CPU core and irqbalance daemon~\cite{irqbalance} is disabled.
%
To avoid any side effects from existing firewall and routing rules, the ingress and egress interface of the SuT are attached to a separate network namespace~\cite{network-namespace}.
%
The intel\_iommu~\cite{iommu} feature is also explicitly disabled, since it is enabled by default in the 6.5 Linux kernel version and massively degrades the performance of network IO operations.
%
Further, NIC offloading capabilities like GRO, GSO and TSO~\cite{offloads} are disabled on each interface on the SuT.
%
The TIPSY framework~\cite{tipsy} is used to orchestrate tests.
%
For configuring and installing \textit{MQ\_CAKE}, the \textit{tc} command line tool is extended~\cite{mq-cake-iproute}.
%
Further, in case of the MoonGen UDP flood tests, the installed \textit{MQ\_CAKE} instances are configured with \textit{besteffort flows overhead 18 mpu 64 noatm memlimit (default\_memlimit$\div$nr of queues)}~\cite{cake-manual}.
%
The overhead compensation is configured so that the rate calculation of MoonGen and \textit{MQ\_CAKE} are identical.
%
The EDT-eBPF program builds upon the pseudo-code presented at the Netdev Conference 0x14~\cite{edt-ebpf}, with modifications to enforce a global rate limit and incorporate the aforementioned overhead compensation.
%
The drop-horizon is set to $2s$, following the value proposed by the authors~\cite{edt-ebpf}. The adjusted eBPF program is detailed in Algorithm~\ref{alg:edt-ebpf}.

To align with the expected buffer occupation, the \lstinline{limit} and \lstinline{flow_limit} parameters of the FQ instances are configured based on the following formula:
\begin{align*}
    \text{Limit} = \frac{\text{Rate Limit}\cdot \text{Drop Horizon}}{\text{\# FQ instances}\cdot \text{Packet size} \cdot 8}
\end{align*}

If the FQ buffer sizes are not properly adjusted, FQ may drop packets unnoticed by the eBPF program, ultimately impacting performance.

\begin{algorithm}[h]
    \caption{EDT-eBPF implementation}\label{alg:edt-ebpf}
\begin{algorithmic}[1]
\Procedure{rate\_limit}{skb}
    \State pkt\_len = skb$\rightarrow$len + compensation
    \State {delay\_ns = pkt\_len*NS\_PER\_SEC/global\_rate\_limit}
    \State next\_tstamp = global\_next\_tstamp
    \\
    \If {next\_tstamp $\leq$ now}
        \State global\_next\_tstamp = now + delay\_ns
        \State \Return TC\_ACT\_OK
    \EndIf
    \\
    \If {next\_tstamp$-$now $\geq$ DROP\_HORIZON}
        \State \Return TC\_ACT\_SHOT
    \EndIf
    \\
    \State skb$\rightarrow$tstamp = next\_tstamp
    \State \_\_sync\_fetch\_and\_add(global\_next\_tstamp, delay\_ns)
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Accuracy and Scalability}
\begin{figure*}
    \begin{subfigure}{0.3\linewidth}
        \centering
        \includesvg[scale=0.5]{images/tp_rate_conformance}
        \caption{Achieved throughput}\label{fig:tp_rate_conformance}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.3\linewidth}
        \centering
        \includesvg[scale=0.5]{images/tp_deviation_total.svg}
        \caption{Total deviation}\label{fig:tp_deviation_total}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.3\linewidth}
        \centering
        \includesvg[scale=0.5]{images/tp_deviation_perc}
        \caption{Relative deviation}\label{fig:tp_deviation_perc}
    \end{subfigure}
    \caption{Achieved throughput and deviation from the target rate at various rate limits under
    network traffic containing only full MTU-sized packets}\label{fig:accuracy}
\end{figure*}
Figure~\ref{fig:tp_rate_conformance} shows the achieved throughput for varying rate limits, ranging from 10 Mbps to 24 Gbps.
%
In this test run, the network traffic consists of 120 UDP flows containing only full MTU-sized packets.
%
The number of receive and transmission queues is set to 40, meaning that every available logical CPU is assigned one receive and transmission queue.
%
These settings maximize the achievable throughput and reduce concurrent access to the same qdisc by distributing packet handling across the per-transmission queue qdisc instances.
%
Figure~\ref{fig:tp_deviation_total} highlights the total deviation from the configured maximum rate limit as well as a relative percentage of the rate limit (Figure~\ref{fig:tp_deviation_perc}).
%
Together, these plots demonstrate that both \textit{MQ\_CAKE} and EDT-eBPF can effectively shape traffic up to 24 Gbps, with a maximum deviation of less than 0.8\%. Notably, \textit{MQ\_CAKE} achieves an even lower deviation of approximately 0.2\%.

Figure~\ref{fig:tp_deviation_total} shows that  \textit{MQ\_CAKE}'s achieved rate is slightly above the configured rate limit.
%
This slight overshoot is important to keep in mind so as to avoid over-provisioning the link: thus, the configured rate should be set to 99.75\% of the desired rate.
%

\begin{figure}[h]
    \centering
    \includesvg[scale=0.5]{images/txq_scaling_64}
    \caption{Achieved throughput in relation to the number of available hardware queues for 64 byte packets and a 20 Gbps rate limit}\label{fig:scaling}
\end{figure}
To show the scalability traits of \textit{MQ\_CAKE}, the next test is configured with a rate limit of 20 Gbps and the UDP packet sizes are reduced to 64 bytes.
%
Further, the test setup ensures that the number of receive queues always equals the number of transmission queues.
%
This setting ensures that every transmission queue receives the same amount of the received packets, thus preventing imbalances in the load between qdiscs.
%
The effect of these imbalances are further explained in the Limitations section.
%

Figure~\ref{fig:scaling} reveals the throughput achieved by \textit{MQ\_CAKE} and EDT-eBPF in relation to the number of available hardware queues.
%
The test shows that both \textit{MQ\_CAKE} and EDT-eBPF scale linearly with an increasing number of transmission queues.
%
The achieved throughput increases at a quicker rate up to 20 transmission queues, after which point the improvement reduces due the use of hyperthreading cores.
%
This effect is due to resource-sharing between the two logical cores residing in one physical core:
%
Thus, their performance is not completely independent from one another.
%
However, even with hyperthreading enabled, \textit{MQ\_CAKE} and EDT-eBPF are still able to increase their throughput.

\subsection{Synctime}\label{sec:synchronization-time}
Up to this point, the traffic in the previous evaluations has been held static, meaning that the number of flows did not change.
%
In order to investigate the impact of the synchronization time on the rate limiter's accuracy, the number of flows in the following test configuration is changed by switching from the  initial 4 flows to 40 flows.
%
\begin{figure}
    \centering
    \includesvg[scale=0.5]{images/switching_200us}
    \caption{\textit{MQ\_CAKE}'s behavior when switching from 4 to 40 flows with a 200$\mu s$ synchronization time, full MTU-sized packets, and 40 transmission queues}\label{fig:switching_200us}
\end{figure}
%
\begin{figure*}
    \begin{subfigure}{0.3\linewidth}
        \centering
        \includesvg[scale=0.5]{images/qlen_qdel_200us_2}
        \caption{Rate limit 2 Gbps.}\label{fig:qlen_qdel_200us_2}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.3\linewidth}
        \centering
        \includesvg[scale=0.5]{images/qlen_qdel_200us}
        \caption{Rate limit 8 Gbps.}\label{fig:qlen_qdel_200us_8}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.3\linewidth}
        \centering
        \includesvg[scale=0.5]{images/qlen_qdel_200us_15}
        \caption{Rate limit 15 Gbps.}\label{fig:qlen_qdel_200us_15}
    \end{subfigure}
    \caption{Induced queue lengths and delays at varying \textit{synctimes} and at a configured global rate limit of 2, 8, and 15 Gbps}\label{fig:qlen_qdel}
\end{figure*}

Figure~\ref{fig:switching_200us} shows such a switching event between 4.94s and 4.95s.
%
During the switch, the throughput spikes due to \textit{MQ\_CAKE}'s inaccurate estimation of the number of active queues.
%
Before the switch, only 4 queues were active: During the switch, the remaining 36 inactive queues are activated and then scan all other qdiscs to estimate their local rate limit.
%
Since this scanning is not necessarily executed simultaneously, the active queue estimation per qdisc will likely be lower than 40 --- not all qdiscs will have already enqueued or transmitted a packet at the point of scanning.
%
Further, the 4 already-active qdiscs will not immediately update their estimated rate upon new flow arrivals, which can delay their local rate limit reduction.
%
These conditions result in the observed overshoot in Figure~\ref{fig:switching_200us}.
%

%
When evaluating accuracy, it is important to consider the induced queue length at the next bottleneck in the packet path, which is caused by the throughput spike, as well as the increased latencies it produces.
%
The width of the spike can be controlled by manipulating the \textit{synctime}.
%
Exceeding the global rate limit leads to buffering excess packets on the next bottleneck link.
%
This buffering increases latencies and may lead to packet drops.
%
To gain insights into the amount of induced latencies and to provision buffer sizes, the next step is to examine these metrics in relation to the synchronization time. 
%
Figure~\ref{fig:qlen_qdel} outlines the induced queue lengths as well as the corresponding induced queueing delays at three different global rate limits.
%
Synchronization times beyond 100$\mu s$ increase the spike's overshoot as well as its duration for the reasons described above.
%
The longer the \textit{synctime}, the longer queues will send an inordinately high number of bytes due to their inaccurate local rate estimation.
%
These plots also clearly show that reducing the \textit{syncime} also inhibits the spike intensity as well as the queueing delay.
%
However, if the \textit{synctime} is too greatly reduced (i.e.\ less than 50$\mu s$ in the conducted experiments), the overhead of the synchronization loop increases, lowering the achieved throughput.
%
\begin{figure}[H]
    \centering
    \includesvg[scale=0.5]{images/sync_txq_64}
    \caption{Achieved packet rate based on the available transmission queues for varying \textit{synctimes}. The traffic consists of 4 flows containing only 64 byte packets.}\label{fig:sync_txq_64}
\end{figure}
Figure~\ref{fig:sync_txq_64} shows the relation between the achieved rate and the number of transmission queues for different \textit{synctimes}.
%
This plot clearly shows that when the \textit{synctime} is too low, the achieved packet rate decreases due to the synchronization overhead.
%
A greater number of transmission queues increases the \textit{MQ\_CAKE} instances' scanning time and may well lead to cache misses when accessing the other qdiscs' activity metrics.
%

\subsection{TCP and Latencies}
\begin{figure}
    \centering
    \hspace{-0.25cm}
    \begin{subfigure}{\linewidth}
        \centering
        \includesvg[scale=0.5]{images/tp-edtbpf-mqcake-2s-horizon-20000.svg}
        \caption{Throughput}\label{fig:flent_tp_2s}
    \end{subfigure}
    \\
    \begin{subfigure}{\linewidth}
        \centering
        \includesvg[scale=0.5]{images/ping-edtbpf-mqcake-2s-horizon-20000-log.svg}
        \caption{Ping}\label{fig:flent_ping_2s}
    \end{subfigure}
    \caption{Flent tcp\_nup test with 1024 TCP streams, a configured rate limit of 20 Gbps, and a $2s$ drop horizon for EDT-eBPF}\label{fig:flent_2s}
\end{figure}

% The previous tests are based on unresponsive UDP traffic:
To review how this setup functions under a packet-loss sensitive transport protocol, this section inspects the initial results of TCP traffic and the resulting latencies using the network testing tool Flent~\cite{flent} and TCP cubic~\cite{tcp-cubic} algorithm.
%
Flent's tcp\_nup test is executed using 1024 TCP upload streams.
%
Figure~\ref{fig:flent_2s} compares the performance of \textit{MQ\_CAKE} and EDT-eBPF under a global rate limit of 20 Gbps.

\textit{MQ\_CAKE} maintains stable rate enforcement, remaining slightly below the configured limit, whereas EDT-eBPF initially exceeds the rate limit before gradually reducing throughput in the latter half of the test (Figure~\ref{fig:flent_tp_2s}).
%
Figure~\ref{fig:flent_ping_2s} shows the latencies measured during the test execution. \textit{MQ\_CAKE} achieves 0.4 ms latencies at the 99th percentile, a 2500X improvement as compared to EDT-eBPF.
%
The drop horizon of EDT-eBPF directly corresponds with the expected latencies.
%

Figure~\ref{fig:flent_5ms} shows the same test execution as before but with a reduced drop horizon of $5ms$ for the EDT-eBPF approach. Figure~\ref{fig:flent_tp_5ms} reveals that lowering the drop horizon to $5ms$ not only stabilizes EDT-eBPF's rate conformance but also reduces the tail latencies to $5ms$ (Figure~\ref{fig:flent_ping_5ms}).
%
However, reducing the drop horizon only works effectively, if the RTT's of the TCP flows have similar values as the drop horizon, thus high RTT flows would suffer from such a low drop horizon.
Even with this configuration \textit{MQ\_CAKE} achieves 10X lower latencies as compared to EDT-eBPF. 
\begin{figure}
    \centering
    \hspace{-0.25cm}
    \begin{subfigure}{\linewidth}
        \centering
        \includesvg[scale=0.5]{images/tp-edtbpf-mqcake-5ms-horizon-20000.svg}
        \caption{Throughput}\label{fig:flent_tp_5ms}
    \end{subfigure}
    \\
    \begin{subfigure}{\linewidth}
        \centering
        \includesvg[scale=0.5]{images/ping-edtbpf-mqcake-5ms-horizon-20000-log.svg}
        \caption{Ping}\label{fig:flent_ping_5ms}
    \end{subfigure}
    \caption{Flent tcp\_nup test with 1024 TCP streams, a configured rate limit of 20 Gbps, and a $5ms$ drop horizon for EDT-eBPF}\label{fig:flent_5ms}
\end{figure}

\subsection{Discussion and Limitations}
The presented experiments reveal that \textit{MQ\_CAKE} is able to shape traffic up to 25 Gbps while achieving high accuracy with a deviation around 0.2\%.
%
Further, \textit{MQ\_CAKE} increases throughput with a greater number of hardware queues, even achieving slightly better higher throughput as compared to EDT-eBPF beyond 20 transmission queues.
%
In addition, the analysis of the \textit{synctime} shows that setting its value too low increases the CPU overhead.
%
As a lower bound, the synchronization time should be higher than the time it takes to complete one scan over all qdiscs.
%
On the other hand, higher \textit{synctimes} lead to less CPU load but also increase the time it takes \textit{MQ\_CAKE} to converge.
%
To balance between CPU load and accuracy, the experiments show that a \textit{synctime} value between 100--200$\mu s$ is ideal.
\textit{MQ\_CAKE} also improves tail-latencies while being as accurate as the EDT-eBPF approach.

Overall, the aforementioned tests reveal that \textit{MQ\_CAKE} displays excellent scalability while maintaining accuracy in enforcing rate limits:
%
However, this approach is, of course, not without shortcomings.
%
The experiments conducted in this thesis ensure that the qdisc layer of the Linux kernel is saturated with packets.
%
However, under real-world conditions, this might not always be the case, as not all flows are sent at full speed or evenly distributed across transmission queues.
%
In the worst case, this can lead to imbalances between the loads of different \textit{MQ\_CAKE} instances, where traffic enqueued in one qdisc cannot saturate the estimated local rate limit while another qdisc instance is heavily flooded with packets.
%
These imbalances taint the active queue estimation and lead to much lower throughput.
%

For example, consider a case where the global rate limit is set to 10 Gbps and there is an incoming traffic of 10 Gbps.
%
In this example, 80\% of the incoming traffic is enqueued in Qdisc A and the remaining 20\% of the traffic is steered to Qdisc B. In this case, both Qdiscs will estimate two active queues and lower their rate limit to 5 Gbps each.
However, since Qdisc B can only forward 2 Gbps and Qdisc A is capped at 5 Gbps, the resulting throughput is only 7 Gbps.
%
\begin{figure}[H]
    \centering
    \includesvg[scale=0.5]{images/txq_imbalance_1514}
    \caption{Achieved throughput in relation to the number of available hardware transmission queues with different \textit{synctimes} for flows with full MTU-sized packets, where the rate limit is set to 20 Gbps and the number of receive queues is held at 40}\label{fig:txq_imbalance_1514}
\end{figure}
Figure~\ref{fig:txq_imbalance_1514} shows such an imbalance scenario.
%
In this experiment, the number of receive queues is held at 40 as the number of transmission queues increases.
%
Concentrating first on a \textit{synctime} of 200$\mu s$, this plot shows that the achieved rate worsens when the number of transmission queues surpasses half the number of receive queues.
%
At this critical juncture, the receive queues no longer distribute traffic equally across the transmission queues, which leads to the imbalanced scenario described above.
%
For example, when there are 24 transmission queues, 16 transmission queues receive double the amount of packets compared to the remaining 8 transmission queues.
%
The estimated rate of the 8 transmission queues is higher than the traffic they receive, leading to unused bandwidth and a declining throughput.
%
However, as more transmission queues are added, the imbalance is reduced. At the same time, the estimated rate for each transmission queue also decreases, leading to less unused bandwidth. 
%
It is important to note that the EDT-eBPF approach's performance is not affected by these imbalances.

These imbalances in multi-queue networking environments are well known in the literature~\cite{titan}~\cite{loom}~\cite{silo}.
%
To address this issue, Titan~\cite{titan} attempts to balance out traffic between queues when packets are enqueued.
%
It is likely that \textit{MQ\_CAKE} could benefit from such a balancing system.
%
Initial tests of a rebalancing mechanism within \textit{MQ\_CAKE} also show that this is possible: However, this mechanism is not further analyzed within this paper.
%
Another method to counteract imbalances is to reduce the \textit{synctime}, which can be seen in Figure~\ref{fig:txq_imbalance_1514} for synchronization times of 5--10$\mu s$. However, as described above, reducing the \textit{synctime} is only possible to a certain extent and depends on the number of transmission queues.
%
