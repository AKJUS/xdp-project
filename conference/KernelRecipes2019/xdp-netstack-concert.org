# -*- fill-column: 79; -*-
#+TITLE: XDP closer integration with network stack
#+AUTHOR: Jesper Dangaard Brouer <hawk@kernel.org>
#+EMAIL: brouer@redhat.com
#+REVEAL_THEME: redhat
#+REVEAL_TRANS: linear
#+REVEAL_MARGIN: 0
#+REVEAL_EXTRA_JS: { src: '../reveal.js/js/redhat.js'}
#+REVEAL_ROOT: ../reveal.js
#+OPTIONS: reveal_center:nil reveal_control:t reveal_history:nil
#+OPTIONS: reveal_width:1600 reveal_height:900
#+OPTIONS: ^:nil tags:nil toc:nil num:nil ':t

* For conference: Kernel Recipes 2019                              :noexport:

This presentation will be given at [[https://kernel-recipes.org/en/2019/][Kernel Recipes 2019]].

Links to talk:
 - https://kernel-recipes.org/en/2019/talks/
 - https://kernel-recipes.org/en/2019/xdp-closer-integration-with-network-stack/

** Abstract

XDP (eXpress Data Path) is the new programmable in-kernel fast-path, which is
placed as a layer before the existing Linux kernel network stack (netstack).

We claim XDP is not kernel-bypass, as it is a layer before and it can easily
fall-through to netstack. Reality is that it can easily be (ab)used to create a
kernel-bypass situation, where non of the kernel facilities are used (in form of
BPF-helpers and in-kernel tables). The main disadvantage with kernel-bypass, is
the need to re-implement everything, even basic building blocks, like routing
tables and ARP protocol handling.

It is part of the concept and speed gain, that XDP allows users to avoid calling
part of the kernel code. Users have the freedom to do kernel-bypass and
re-implement everything, but the kernel should provide access to more in-kernel
tables, via BPF-helpers, such that users can leverage other parts of the Open
Source ecosystem, like router daemons etc.

This talk is about how XDP can work in-concert with netstack, and proposal on
how we can take this even-further. Crazy ideas like using XDP frames to move SKB
allocation out of driver code, will also be proposed.

* Colors in slides                                                 :noexport:
Text colors on slides are chosen via org-mode italic/bold high-lighting:
 - /italic/ = /green/
 - *bold*   = *yellow*
 - */italic-bold/* = red

* Slides below                                                     :noexport:

Only sections with tag ":export:" will end-up in the presentation. The prefix
"Slide:" is only syntax-sugar for the reader (and it removed before export by
emacs).

* Slide: What will you learn?                                        :export:

Audience at Kernel Recipes: Other /kernel developers/
- But for *different kernel sub-systems*

Hopefully you already /heard about XDP/ (eXpress Data Path) */?/* *?* /?/
- I'll *explain why* network sub-system created this...
- And /future/ *crazy ideas* to extend XDP
  - (hopefully) in a way that cooperate more with kernel netstack

** Slide: Why was XDP needed?                                       :export:

This was about the *kernel networking stack staying relevant*
 - For emerging use-cases and areas

Linux networking stack assumes layer L4-L7 delivery
 - Obviously slow when compared to L2-L3 kernel-bypass solutions

XDP operate at layers L2-L3
 - Shows same performance as these L2-L3 kernel-bypass solutions

@@html:<small>@@

The networking OSI layer model:
 - L2=Ethernet
 - L3=IPv4/IPv6
 - L4=TCP/UDP
 - L7=Applications

@@html:</small>@@

** Slide: What is XDP?                                              :export:

@@html:<small>@@
What kind of monster did we create with XDP?!?
@@html:</small>@@

XDP (eXpress Data Path) is a Linux *in-kernel* fast-path
 - /New programmable layer in-front/ of traditional network stack
   - Read, modify, drop, redirect or pass
 - For L2-L3 use-cases: seeing x10 performance improvements!
 - Can accelerate *in-kernel* L2-L3 use-cases (e.g. forwarding)

What is /AF_XDP/? (the Address Family XDP socket)
 - /Hybrid/ *kernel-bypass* facility
 - Delivers raw L2 frames into userspace (in SPSC queue)

** Slide: AF_XDP: Used for kernel-bypass?!?                         :export:

@@html:<small>@@
Did you really say, this can be used for */bypassing the Linux kernel netstack/* ?
@@html:</small>@@

Sure, build in /freedom/ for *kernel-bypass via AF_XDP*
- DPDK already have a Poll-Mode driver for AF_XDP

Why is this /better/, than other (bypass) solutions?
- Flexible /sharing of NIC resources/, NIC still avail to netstack
- XDP/eBPF prog filters packets using XDP_REDIRECT into AF_XDP socket
  - Move *selective* frames out of kernel, no need to reinject
- /Leverages existing kernel infrastructure/, eco-system and market position

* Slide: How can XDP be (ab)used?                                    :export:
:PROPERTIES:
:reveal_extra_attr: class="mid-slide"
:END:

Used or abused?
- Freedom re-implement everything (when bypassing the kernel)
- Or freedom to shoot yourself in the foot?

** Slide: Simple view on how XDP gains speed                        :export:

XDP speed gains comes from
- *Avoiding* /memory allocations/
  - no SKB allocations and no-init (memset zero 4 cache-lines)
- /Bulk/ processing of frames
- Very /early access/ to frame (in driver code *after DMA sync*)
- Ability to /skip/ (large parts) of kernel /code/

** Slide: Skipping code: Efficient optimization                     :export:

Skipping code: *Imply skipping features* provided by /network stack/
  - Gave users freedom to e.g. skip netfilter or route-lookup
  - Users have to re-implement features they actually needed
    - Sometimes cumbersome via BPF-maps

/Avoid/ *re-implement features*:
- Evolve XDP via /BPF-helpers/

** Slide: Evolving XDP via BPF-helpers                               :export:

@@html:<small>@@
*We should encourage adding helpers instead of duplicating data in BPF maps*
@@html:</small>@@

Think of XDP as a /software offload layer for the kernel netstack/
 - Simply setup and use the Linux netstack, but accelerate parts of it with XDP

IP routing good example: /Access routing table from XDP via BPF helpers/ (v4.18)
 - Let Linux handle routing (daemons) and neighbour lookups
 - Talk at LPC-2018 (David Ahern): [[http://vger.kernel.org/lpc-networking2018.html#session-1][Leveraging Kernel Tables with XDP]]

Obvious *next target*: /Bridge lookup helper/
 - Like IP routing: transparent XDP acceleration of bridge forwarding
   - Fallback for ARP lookups, flooding etc.
 - Huge potential *performance boost for Linux bridge* use cases!

* Slide: Understand networking packet data structures                :export:
:PROPERTIES:
:reveal_extra_attr: class="mid-slide"
:END:

To understand next slides and (XDP) kernel networking
- Need to know difference between 3 struct's
- Used for describing and pointing to actual packet data

** Slide: Fundamental struct's                                      :export:

The struct's describing data-frame at different levels
- /=sk_buff=/ : Good old SKB, allocated from SLAB/kmem_cache (4 cachelines)
- /=xdp_buff=/ : Used by BPF XDP-prog, allocated on call stack
- /=xdp_frame=/: xdp_buff info state compressed, used by XDP-redirect
  - No allocation, placed in top of data-frame

At driver level
- HW specific /"descriptor"/ with info and point to (DMA) data buffer
  - *contains HW-offloads* (see later) that driver transfers to SKB

#+BEGIN_NOTES
Consider if describing xdp_rxq_info is needed?
#+END_NOTES

* Slide: Evolving XDP: Future ideas                                  :export:
:PROPERTIES:
:reveal_extra_attr: class="mid-slide"
:END:

*Warning*: Next slides about */crazy/* *future* /ideas/
- This stuff might never get implemented!

* Slide: Move SKB allocations out of NIC drivers                     :export:

Goal: Simplify driver, via creating SKB inside network-core code
- Happens today via *=xdp_frame=* in both /veth/ and /cpumap/
- (Slight hickup: Max frame size unknown, lies about skb->truesize)

The =xdp_frame= is placed in top of data-frame (=data_hard_start=)
- Currently 32-bytes

Issue: *SKB*'s created this way are /lacking HW-offloads/ like:
- HW /checksum/ info (for =skb->ip_summed= + =skb->csum=)
- HW /RX hash/ (=skb_set_hash(hash, type)=)
- (these are almost always needed... tempted to extend =xdp_frame=)

** Slide: Other HW-offloads                                         :export:

Other /existing/ offloads, used by SKBs, but *not always enabled*
 - /VLAN/ (=__vlan_hwaccel_put_tag()=)
 - RX /timestamp/
   - HW =skb_hwtstamps()= (stored in skb_shared_info)
   - Earlier XDP software timestamp (for =skb->tstamp=)
 - RX /mark/ (=skb->mark= supported by mlx5)

@@html:<br/><small>@@
Other *potential* offloads, which hardware can do (but not used by SKB):
 - Unique u64 /flow identifier/ key (mlx5 HW)
 - Higher-level protocol header offsets
   - RSS-hash can deduce e.g. IPv4/TCP (as frag not marked as TCP)
   - But NIC HW have full parse info avail
@@html:</small>@@

** Slide: Blocked by missing HW-offloads                            :export:

@@html:<small>@@
SKB alloc outside NIC driver, blocked by missing HW-offloads.
@@html:</br>@@
The GOAL is to come-up with a Generic Offload Abstraction Layer...
@@html:</small>@@

Generic and dynamic way to transfer HW-offload info
- Only enable info when needed
- Both /made available for SKB creation and XDP programs/

The big questions are:
- Where to *store this information?*
- How to make it /dynamic/?

** Slide: Storing generic offload-info                              :export:

Where to store generic offload-info?
- To avoid allocation use packet/frame data area
  - (1) Extend /xdp_frame/: imply top of frame head-room
  - (2) Use XDP /meta-data/ area: located in-front of payload start
  - (3) Use tail-room (frame-end): Already used for skb_shared_info GRO

No choice done yet...

** Slide: Dynamic generic offload-info

Next challenge: How to make this dynamic?
- Each driver have own format for HW descriptor
- Hopefully BTF can help here?

Drivers could export BTF description
- BPF prog wanting to use area, must have matching BTF
- But how can kernel-code use BTF desc and transfer to SKB fields?

** Slide: Dependency: Handling multi-frame packets

@@html:<small>@@
SKB alloc outside NIC driver, ALSO need XDP multi-frame handling
@@html:</small>@@

Multi-frame packets have several use-cases
- Jumbo-frames
- TSO (TCP Segmentation Offload)
- Header split, (L4) headers in first segment, (L7) payload in next

XDP need answer/solution for multi-frame packets
- To fully move SKB alloc+setup out of NIC drivers
- Design idea/proposal in XDP-project: [[https://github.com/xdp-project/xdp-project/blob/master/areas/core/xdp-multi-buffer01-design.org][xdp-multi-buffer01-design.org]]

* Slide: Fun with xdp_frame before SKB alloc                         :export:
:PROPERTIES:
:reveal_extra_attr: class="mid-slide"
:END:

After SKB alloc got moved out of drivers
- What can we now create of crazy stuff?!?

** Slide: New layer with xdp_frame?

Could we update netstack L2 RX-handler to handle xdp_frame packets?

- Bridging
- Macvlan

* Emacs end-tricks                                                 :noexport:

# Local Variables:
# org-re-reveal-title-slide: "<h1 class=\"title\">%t</h1><h2
# class=\"author\">Jesper Dangaard Brouer<br/>Kernel Developer<br/>Red Hat</h2>
# <h3>Kernel Recipes Conf<br/>Paris, Sep 2019</h3>"
# org-export-filter-headline-functions: ((lambda (contents backend info) (replace-regexp-in-string "Slide: " "" contents)))
# End:
