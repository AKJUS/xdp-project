# -*- fill-column: 79; -*-
#+TITLE: XDP - challenges and future work
#+AUTHOR: Jesper Dangaard Brouer and Toke Høiland-Jørgensen
#+EMAIL: toke@toke.dk
#+REVEAL_THEME: redhat
#+REVEAL_TRANS: linear
#+REVEAL_MARGIN: 0
#+REVEAL_EXTRA_JS: { src: './reveal.js/js/custom.js'}
#+OPTIONS: reveal_center:nil reveal_control:t reveal_history:nil
#+OPTIONS: reveal_width:1600 reveal_height:900
#+OPTIONS: ^:nil tags:nil toc:nil num:nil ':t

This presentation will be given at Linux Plumbers Conference 2018 Networking
Track.

 http://vger.kernel.org/lpc-networking.html

This emacs org-mode document contains both slides for the presentation and
other notes for the paper and project.  The slides are in reveal.js format and
are generated by exporting this document via ox-reveal emacs package.

* Export/generate presentation

** Setup for org export to reveal.js
First, install the ox-reveal emacs package.

Package: ox-reveal git-repo and install instructions:
https://github.com/yjwen/org-reveal

After this, move to the 'Topics and slides' subtree and hit =C-c C-e C-s R R=
to export just the subtree; then open .html file to view slideshow. The
variables at document end ("Local Variables") will set up the title slide and
filter the "Slide:" prefix from headings; Emacs will ask for permission to load
them, as they will execute code.

** Export to PDF

The conference requires presentations to be delivered in PDF format.  Usually
the reveal.js when run as a webserver under nodejs, have a printer option for
exporting to PDF vai print to file, but we choose not run this builtin
webserver.

Alternatively I found a tool called 'decktape', for exporting HTML pages to
PDF: https://github.com/astefanutti/decktape

The 'npm install' failed on my system:

 $ npm install decktape

But (after running npm update) I can start the decktape.js file direct via
the 'node' command.

 $ node ~/git/decktape/decktape.js presentation-lpc2018-xdp-future.html slides.pdf

And it worked.

* Homepage abstract for presentation

http://vger.kernel.org/lpc-networking.html#session-19

** Title: XDP challenges and future work

Speakers: Jesper Dangaard Brouer, Toke Høiland-Jørgensen

Duration (incl. QA): 35 min

Content: Slides, Paper

** Abstract:

XDP already offers rich facilities for high performance packet
processing, and has seen deployment in several production
systems. However, this does not mean that XDP is a finished system; on
the contrary, improvements are being added in every release of Linux,
and rough edges are constantly being filed down. The purpose of this
talk is to discuss some of these possibilities for future
improvements, including how to address some of the known limitations
of the system. We are especially interested in soliciting feedback and
ideas from the community on the best way forward.

The issues we are planning to discuss include, but are not limited to:

 - User experience and debugging tools: How do we make it easier for
   people who are not familiar with the kernel or XDP to get to grips
   with the system and be productive when writing XDP programs?

 - Driver support: How do we get to full support for XDP in all
   drivers? Is this even a goal we should be striving for?

 - Performance: At high packet rates, every micro-optimisation
   counts. Things like inlining function calls in drivers are
   important, but also batching to amortise fixed costs such as DMA
   mapping. What are the known bottlenecks, and how do we address
   them?

 - QoS and rate transitions: How should we do QoS in XDP? In
   particular, rate transitions (where a faster link feeds into a
   slower) are currently hard to deal with from XDP, and would benefit
   from, e.g., Active Queue Management (AQM). Can we adapt some of the
   AQM and QoS facilities in the regular networking stack to work with
   XDP? Or should we do something different?

 - Accelerating other parts of the stack: Tom Herbert started the
   discussion on accelerating transport protocols with XDP back
   in 2016. How do we make progress on this? Or should we be doing
   something different? Are there other areas where we can extend XDPs
   processing model to provide useful accelerations?

* Other XDP talks co-scheduled

We promised to introduce other XDP talks, and got scheduled as the first talk
for this reason.

Thus, part of the structure will be given by other XDP talks.

** Other XDP and eBPF related talks:

When below are marked DONE, this means a slide mention it.

We should take care to mention all talks that are directly related to XDP, and
can skip mentioning eBPF talks that does not affect XDP.

*** (Our-talk) XDP - challenges and future work
*** DONE Leveraging Kernel Tables with XDP
*** DONE XDP acceleration using NIC metadata, continued
*** DONE (AF_XDP) Bringing the Power of eBPF to Open vSwitch
*** DONE (AF_XDP) The Path to DPDK Speeds for AF_XDP
*** DONE (facebook) XDP 1.5 years in production. Evolution and lessons learned.
*** DONE (facebook) eBPF / XDP based firewall and packet filtering
*** DONE P4C-XDP: Programming the Linux Kernel Forwarding Plane using P4
*** Using eBPF as an abstraction for switching
*** Building socket-aware BPF programs
*** BPF Host Network Resource Management
*** Combining kTLS and BPF for Introspection and Policy Enforcement


*** XDP/eBPF unrelated talks:

Experiences Evaluating DC-TCP

Scaling Linux bridge forwarding database

ERSPAN Support for Linux

This talk is not about XDP: From Resource Limits to SKB Lists.

Optimizing UDP for content delivery with GSO, pacing and zerocopy.

Linux SCTP is catching up and going above!

What's happened to the world of networking hardware offloads?

TC SW datapath: a performance analysis

PHYlink and SFP: Going beyond 1G Copper PHYs


* Document organizing presentation

The presentation will be organized around a number of XDP-topics.  We cannot
bring-up all topics, but have selected some.  To introduce other talks, their
topics have to be included. Also have some of our own that to get feedback.

Only sections with tag ":export:" will end-up in the presentation.

The "Topic:" notation will be transition slide (or none), and the "Slide:"
notation will be actual slides.  The "Topic:" notes describe more about the
topic, this text could be used in the paper.

* Story baseline                                                   :noexport:

 - Introduce XDP-paper.

   We wrote XDP-paper, that doc XDP architecture, and do head-to-head comparison
   against DPDK.

 - This talk is focused on "Limitation and Future Work".

   We are fortunate that other people have already started to work on "future
   work" items, and are even being covered and presented at this conf.  We will
   mention these areas and defer the details and discussion to these talks.

 - Purpose soliciting feedback and ideas from the community

   Besides referencing the topics covered in other talks, we have also selected
   some XDP topics that we wish to soliciting feedback om from this community.

* Slide: Presentation overview                                      :export:
:PROPERTIES:
:reveal_extra_attr: class="mid-slide"
:END:

Introduce scientific XDP paper

Lots of XDP activity -- Especially at this conference

Evolving XDP -- without killing performance

** Slide: XDP-paper                                                 :export:

Scientific XDP paper accepted:
 - /"The eXpress Data Path: Fast Programmable Packet Processing in the Operating System Kernel"/
 - Conference: ACM CoNEXT 2018, Dec 4-7, Heraklion, Greece

Purpose
 - Describe the full XDP system design in one place
 - Make scientific research community notice XDP
 - First head-to-head comparison with DPDK

This talk came out of the "Limitations and Future Work" section
 - Purpose: *soliciting feedback and ideas* from the community


** Slide: What is XDP?                                              :export:

#+BEGIN_NOTES
Be inspirational: New programmable layer in network stack.
(Mostly for general-track LPC participants)
#+END_NOTES

XDP basically: New layer in the kernel network stack
 - Before allocating the SKB
 - Driver level hook at DMA level

Means: Competing at the same “layer” as DPDK / netmap
 - Super fast, due to
   - Take action/decision earlier (e.g. skip some network layers)
   - No memory allocations

Not kernel bypass; data-plane is kept inside the kernel
 - Via eBPF: makes early network stack run-time programmable
 - Cooperates with the kernel stack

** Slide: XDP is a huge success                                     :export:

XDP is maturing in upstream kernels
 - Still under active development

Popular topic at this LPC network track miniconf
 - 12 talks XDP and eBPF-related! (incl. this)
 - 4 related to non-XDP eBPF issues
 - 8 directly XDP-related (incl. this)
   - We will list these talks and /frame them in the bigger XDP picture/

** Slide: Production use-cases                                       :export:

#+BEGIN_NOTES
Pitch: Can people actually use this in production, even-though XDP is still
under much developement
#+END_NOTES

XDP has seen production use
 - [[https://netdevconf.org/2.1/session.html?bertin][CloudFlare]] publically say they use XDP
 - [[https://suricata.readthedocs.io/en/latest/capture-hardware/ebpf-xdp.html][Suricata]] have XDP [[https://github.com/OISF/suricata/blob/master/ebpf/xdp_filter.c][plugins]]
 - Facebook released the [[https://github.com/facebookincubator/katran][Katran load balancer]]

Other talks will cover this:
 - (Facebook)
   [[http://vger.kernel.org/lpc-networking.html#session-10][XDP 1.5 years in production. Evolution and lessons learned]]
 - (Facebook)
   [[http://vger.kernel.org/lpc-networking.html#session-15][eBPF / XDP based firewall and packet filtering]]


* Evolving XDP
:PROPERTIES:
:reveal_extra_attr: class="mid-slide"
:END:

While XDP is a disruptive and innovative technology
 - Still needs to follow kernel's evolutionary development style
 - Far from finished, improvements and features in every kernel release
 - Beneficial to cooperate with the kernel community
   - Yes, you actually need to describe and argue for your use-case!

** Topic: Performance

XDP is all about performance, don't screw it up!

Guiding principle: Adding features must not negatively affect baseline XDP
performance.  Use optimization technique of moving runtime checks to setup time
checks.

** Slide: New XDP features vs high performance                      :export:

XDP is all about processing many (M)PPS
 - *Watch out so feature creep doesn't kill performance!*

Guiding principle: /New features must not negatively affect baseline XDP perf/
 - Optimisation hint: move runtime checks to setup time

Issue: /*Who is monitoring XDP performance?*/
 - We need CI tests for this... but who will do this work?

** Topic: Evolving XDP

How we imagine XDP getting extended.

We see XDP as a software offloading layer in the kernel network stack.

IP-forwarding is a good example, as the Linux kernel and ecosystem have
everything to become a L3 IP-router, with control-plane software like router
daemons etc.  The forwarding performance is constrained due to software
overhead, which is where XDP can help.  We want XDP work in-concert with this
ecosystem.  This can best be achieved by eBPF helper functions that allow XDP
to perform lookup in kernel tables, e.g. like the FIB lookup that was recently
added (also covered in the XDP-paper).

The topic and the FIB lookup are covered in David Ahern's talk:
"Leveraging Kernel Tables with XDP"

We want to encourage people to add these kind of helpers to XDP, when use-case
arise.  It is possible to e.g. track the IP-route table and ARP table via
monitoring RT-netlink messages, and maintain an on-the-side BPF maps that can
influence XDP route decisions.  This is actually showed as an XDP samples/bpf
program called xdp_router_ipv4, which was done before the FIB-lookup helper was
added.  The performance advantage is very small, and only occurs when
xdp_router_ipv4 sample hit an "exact_match" cache.

** Slide: Evolving XDP via BPF-helpers                               :export:

Think of XDP as /a software offload layer for the kernel network stack/
 - Setup and use Linux kernel network stack
 - But accelerate parts of it with XDP

IP routing good example:
 - Let Linux handle routing (daemons) and neighbour lookups
 - Access routing table from XDP via BPF helpers
 - This is covered in David Ahern's talk: [[http://vger.kernel.org/lpc-networking.html#session-1][Leveraging Kernel Tables with XDP]]

*We should encourage adding helpers instead of duplicating data in BPF maps*



* Topic: XDP as a building block

   If it is not clear to people, explain that XDP is core kernel
   facility, that other Open Source projects need to pickup, use and
   innovate on-top of.

XDP will hopefully be used for faster delivery into Guest-OS.  The best and
fastest abstraction into KVM is still not determined.  The tuntap driver
already implemented ndo_xdp_xmit, which allows XDP_REDIRECT into XXX (TODO is
it virtio_net or vhost_net???), bypassing the Host-OS skb-alloc, while still
performing one copy.  Other options might be to leverage AF_XDP to register
Guest-OS memory with the XDP based NIC driver, which would allow zero-copy RX
into the Guest-OS.

The discussion of eBPF/XDP vs P4 often comes up. Our view is that of-cause you
should be able to write a data-plane in domain-specific language like P4. And
to use load this in a XDP hook, you need to write a new backend to your P4
compiler that generate the XDP/eBPF code.

* Slide: XDP as a building block                                   :export:

XDP is a core kernel layer building block
 - Open Source projects should build and innovate on top
 - Only time will tell what people use it for

** Slide: Building block for Guest OS delivery?

Directions we hope to see XDP go
 - Faster packet delivery into Guest OS
   - Already possible (with copy) via tuntap driver into virtio_net
   - The AF_XDP approach might offer zero-copy

** Slide: Building block for P4

P4 vs eBPF/XDP is the /wrong attitude/
 - Instead [[https://github.com/vmware/p4c-xdp/][compile your P4 code into eBPF]] and run it with XDP
 - Talk on this approach by William Tu (VMware):
   - [[http://vger.kernel.org/lpc-networking.html#session-16][P4C-XDP: Programming the Linux Kernel Forwarding Plane using P4]]

* Topic: Zero-copy to userspace AF_XDP

   Ref two AF_XDP Talks.

   Pitch: XDP have been focused on keeping packet handling and processing in
   kernel space, via leveraging eBPF.  To avoid the overhead of syscalls,
   context switch and packet copies.

Personal note: I always had plans for a fast-path from XDP into a userspace
socket. (Discussed this public in G+ post with DaveM) I had imagined that the
kernel would do the memory allocation, for RX-ring, and VMA map this into
userspace.  AF_XDP goes the other way, and let userspace (pre) alloc.

   AF_XDP is about moving packet handling into userspace.  The key point for
   integrating this with XDP redirect is flexibility.  We want to avoid NIC
   hardware being taken over by the zero-copy facility.  Want to avoid the
   all-or-nothing proposition like we have seen with DPDK.

   AF_XDP avoids the overhead by establishing SPSC queues as communication
   channels to userspace.  The copy is avoided by userspace allocating and
   gifting/providing kernel with this memory, which is used directly for RX
   DMA delivery.

   The AF_XDP socket is woken-up like a regular socket, but for
   high-performance userspace can poll the socket.

   On the TX-side AF_XDP does have a syscall, but userspace can fill
   several TX buffers into TX ring before calling the sendmsg syscall.

* Slide: Zero-copy to userspace AF_XDP                              :export:

XDP usual performance benefit comes from in-kernel processing via BPF

AF_XDP is for faster raw packet delivery into userspace
 - Unlike tcpdump, as it owns/steals the packet
   * (Might want to invent/allow a copy and XDP_PASS mode)
 - Hooking into XDP provides:
   - Packet filter flexibility (avoid netstack reinject tricks)
   - Copy-variant work on any driver with XDP_REDIRECT
 - Performance tricks:
   - Preallocated userspace memory, give it to kernel for NIC RX-rings
   - NetChannel like SPSC queues between kernel and userspace

** Slide: AF_XDP talks

Two talks about AF_XDP on schedule:
 - [[http://vger.kernel.org/lpc-networking.html#session-11][The Path to DPDK Speeds for AF_XDP]]
   - By Björn Töpel and Magnus Karlsson (Intel)
 - [[http://vger.kernel.org/lpc-networking.html#session-7][Bringing the Power of eBPF to Open vSwitch]]
   - By William Tu (VMware)

* Topic: Move SKB allocation out of driver

   The long term goal is moving SKB allocations out of driver code.

   This is already supported by all drivers implementing XDP_REDIRECT, as
   CPU-map and redirects into tun-driver, create the SKB later based on the
   xdp_frame.  Working towards generalizing this.

   Missing part are howto transfer the different driver offloads
   (e.g. csum-info, rxhash, HW-mark) in a vendor neutral and generic way.  This
   depends/waits on metadata talk, for this to be generic enough.

* Slide: Move SKB allocation out of driver                         :export:

Long term goal:
 - Write NIC driver(s) without SKB allocations

Not alloc SKB in driver at RX: Is supported today via XDP_REDIRECT
 - Via redirect into CPU-map or tun-driver

Missing part: driver offloads (e.g. csum-info, rxhash, HW-mark)
 - Depends/waits on metadata, in a vendor neutral way
 - We (also) need core-netstack to understand metadata BTF struct
   - Not just BPF progs

Hope this will be resolved covered in talk:
 - Talk: "XDP acceleration using NIC metadata, continued"


* Slide/topic (maybe skip): Jumbo-frames

(Topic text) People in XDP-newbies list request jumbo-frames. They don't seem
to understand to support high-speed packet processing, we cannot support this
feature.  Besides killing performance, jumbo-frames will also make it more
difficult to move-SKB-alloc out of drivers.  That said, if it is possible to
support Jumbo-Frames without hurting performance, then it could be accepted,
but that will be very difficult.

Understand: RX-ring cannot know packet length, must have room for MTU
 - XDP requires packet mapped in physical continuous memory
   (Due to eBPF mem direct-access, safety via verifier)
 - Alloc above 4K, more expensive (higher-order pages)

Hybrid solution: Only grant XDP access to top 4K
 - Above 4K can be in other pages
 - This will kill, moving SKB allocations out of driver plan
   (makes it complicated and thus affect performance)


* Topic: Controlling resources for ndo_xdp_xmit

(Usability related) XDP_REDIRECT entangled with ndo_xdp_xmit.

Decouple XDP_REDIRECT from ndo_xdp_xmit.
This mostly technical, but also related to usability.

When XDP redirecting a frame out another net_device, then the drivers
ndo_xdp_xmit function is called.  But device drivers don't enable ndo_xdp_xmit
by default, because it costs many resource (TX queue per CPU core).  There is
no interface to enable ndo_xdp_xmit.  The current solution, to enable
ndo_xdp_xmit, is to load an dummy XDP program on the device, you want to
redirect to.

The implicit notion of loading an XDP program, also implies allocating
resources to XDP-TX queues is flawed.  As the XDP user, might not want to use
any redirect feature.  And even if using XDP_REDIRECT this could be CPU-map
redirect, which does not need XDP-TX queues.

The reason for only enabling XDP-TX queues when really needed is that this
consumes HW resources.  Given the TX queue per CPU core assumption, this makes
it problematic on systems with many CPU cores. E.g. it was discovered the ixgbe
driver cannot load XDP on systems with more than 96 CPU cores.

* Slide: Resources for ndo_xdp_xmit                                  :export:

Redirecting XDP frame out another net_device, call ndo_xdp_xmit
 - But egress net_device don't default alloc XDP-TX resouces
   - Reason#1: consumes HW resources, 1 TX-queue per CPU core
   - Reason#2: cannot know it will be used for XDP traffic

We need proper API for enabling XDP-TX resources on egress net_device
 - Current hack, load dummy XDP prog on egress dev
 - Would be natural to, alloc TX resources when dev added to DEVMAP
   - But how to handle non-map redirect variant?

This implicit XDP-TX resources allocation when loading XDP
 - is wasting HW resources when XDP doesn't need ndo_xdp_xmit

* Topic: What does XDP driver support?

(Usability related)

   Issue that not all drivers support all features, but userspace cannot query
   what a given driver supports.  If a driver e.g. doesn't support XDP_REDIRECT,
   then it can only be detected "runtime" by observing a WARN_ONCE kernel log
   message and afterwards packets are silently dropped (can be debugged via
   tracepoints).

   The issue was seen with Suricata, and they want some kind of way to
   know what XDP features are avail.  This is needed to reject or
   change behavior when parsing the Suricata setup file.

Original goal of not exposing feature-bits to userspace, was to force full
support for XDP in all drivers.  Is this goal still realistic, after X kernel
releases, where only 3 HW-drivers have implemented XDP_REDIRECT.

* Slide: What does XDP driver support?                             :export:

Today: Userspace cannot query what XDP features NIC driver support
 - Original goal of not exposing feature-bits to userspace:
   was to force full support for XDP in all drivers.
 - Is this still realistic goal?
   E.g. only 3 HW-drivers have implemented XDP_REDIRECT.
   Some users are happy with (only) XDP_DROP and XDP_TX.

Userspace need to know if XDP_REDIRECT is supported:
 - Suricata use-case:
   - If XDP_REDIRECT is not supported, reject config at startup
   - Or alternative TC solution can be used

* Topic: XDP egress hook (just before TX)

Can be used for:

- Reacting to TX ring status
- Implementing QoS / AQM


* Topic: NIC memory models and DMA mapping

XDP recently (v4.18) got support for different memory models per driver
RX-queue, via the xdp_return_frame() and xdp_rxq_info_reg_mem_model() APIs.

This allow drivers to innovate new memory models, but also gives the
opportunity to generalize and share common code to handle memory recycle
schemes for drivers.  The page_pool is one example common code.

We want to see more drivers need to use page_pool, and work on page_pool is
needed, especially in the area of keeping frames DMA mapped.

We plan to extend the xdp_return_frame API with a bulking option, because it
can naturally do bulking at DMA-TX completion, and the page_pool need this to
handle a known weakness (of concurrent CPUs returning frames to the same
RX-queue).

On Intel machines the DMA map/unmap/sync operation are very lightweight, as due
the coherency model, this might not be true for other architectures.  As XDP
have been very Intel focused, the DMA overhead have seen much attention. The
Spectre-V2 mitigation changed the picture. and will force us to address the DMA
overhead issues due to the indirect call API.

* Slide: Memory models                                             :export:

Recent (4.18): XDP memory models per driver RX-queue
 - New flexibility to innovate
 - Also opportunity to generalize and share common code between drivers
   - page_pool is generalization example, need more drivers using it

Planned changes
 - Extend xdp_return_frame API with bulking, performance reasons
 - Keep pages DMA mapped in page_pool (almost supported)

* Slide: DMA mapping                                               :export:

More optimizations for DMA mapping needed
 - Was low priority, due to almost zero cost on Intel CPUs
 - Spectre-V2 mitigation makes DMA calls more expensive

* Topic: Spectre-V2: mitigation killed XDP performance

   (This is related to DMA mapping)

   This is primary due to DMA-API, what uses indirect call to abstract
   different DMA-engines.

   For mlx5 driver, there is also issues with indirect functions pointer calls,
   inside the driver itself.

   For XDP there are a number of workarounds and performance optimizations for
   the DMA slowdown.  The easiest solution is to amortized via bulking DMA API
   calls.  The DMA APIs already have the scatter-gather API, which in-effect is
   bulking.

   Today most drivers already avoid repeated DMA map/unmap calls, by keeping
   the DMA mapping intact, while processing the frame (e.g by normal netstack)
   and if recycling works they don't need to unmap the frame. They instead does
   the DMA-sync in appropriate places.

   The page_pool API, want to generalize keeping the page DMA-mapped, such that
   the driver doesn't have to do this.  No driver currently takes advantage of
   this DMA page_pool feature.

   During ndo_xdp_xmit(), today, individual frames need to be DMA-mapped for
   the TX device.  Recently this changed to bulk API (curr bulk 16), which
   would easily do bulk DMA mapping.

   Would it make sense to move redirect DMA mapping into redirect-core code?
   Do drivers need fine control over the exact DMA-map call?  If not, the
   DMA-TX addr could be stored in xdp_frame, then xdp_return_frame API could
   also handle the DMA-unmap call.

* Slide: Performance issue: Spectre (variant 2)                    :export:

CONFIG_RETPOLINE and newer GCC compiler
 - for stopping Spectre (variant 2) CPU side-channel attacks

Hey, you killed my XDP performance! (Retpoline tricks for indirect calls)
- Still processing 6 Mpps per CPU core
- But could do approx 13 Mpps before!

Initial through it was net_device->ndo_xdp_xmit call
 - Implemented redirect bulking, but only helped a little

Real pitfall: DMA API use indirect function call pointers
 - Christoph Hellwig PoC patch show perf return to approx 10 Mpps

One XDP indirect call we cannot remove: Invoking BPF program


* Skipped topics                                                   :noexport:
** Topic (maybe skip): Usability: User Experience and Debugging

Lots of small unexpected behavior, seen from normal users of XDP.

E.g. The sample xdp_redirect_map show RX packets per sec, but people think this
is TX packet per sec.  In case the redirect TX device does not support XDP,
there is not immediate feedback, the packets are silently dropped (can be
caught via tracepoint).  Thus, users observe increased PPS counter, when
misconfig happens.

This RX counting, seems natural as a XDP-core developer, as we know the eBPF
program cannot know or measure was happens _after_ it have run (as it simply
returns a verdict/action return code). For a user, it will be more natural to
"see" TX-pps. So, we could monitor interface TX-stats, but that is also not
possible, as some XDP-drivers don't account XDP-TX packets in the regular
ifstat counters, which in itself is problematic.

** Topic (maybe skip): eBPF verifier

   All the discussions about extending the eBPF verifier, should be
   move to the eBPF-mini-conf.

** Topic (maybe skip): ARM and XDP support

We need to make sure, our XDP optimizations does not become too Intel specific.

I have bought a MacchiatoBin ARM64-board, that I plan to developed XDP for.
(Got cross-compiler and upstream kernel working, found DMA-bounce buffer
issues, that I need to upstream fix for)

** Topic (maybe skip): Accelerating Transport Protocols

   E.g. it should be possible to do delivery into TCP sockets, and
   hand-over the packet-page (without first allocating an SKB).

   Ref-Talk: "Building socket-aware BPF programs" is part of this work.


* Notes

** Org-mode hints

https://orgmode.org/manual/Quoting-HTML-tags.html#Quoting-HTML-tags

** Colors from Red Hat guide lines

Red Hat Colors:

 - Red Hat Red #cc0000
 - Medium Red #a30000
 - Dark Red #820000

None of these red colors fit with baggrond color:
 - Using red 65% #ff4d4d
 - Found via: https://www.w3schools.com/colors/colors_picker.asp

Secondary Palette:

 - Dark Blue #004153
 - Medium Blue #4e9fdd
 - Light Blue #5bc6e8
 - Lighter Blue #a3dbe8

Accent Palette:

 - Purple #3b0083
 - Orange #ec7a08
 - Green #7ab800
 - Turquoise #007a87
 - Yellow #fecb00

# Local Variables:
# org-reveal-title-slide: "<h1 class=\"title\">%t</h1><h2
# class=\"author\">Jesper Dangaard Brouer (Red Hat)<br/>Toke Høiland-Jørgensen
# (Karlstad University)</h2><h3>LPC Networking Track<br/>Vancouver, Nov
# 2018</h3>"
# org-export-filter-headline-functions: ((lambda (contents backend info) (replace-regexp-in-string "Slide: " "" contents)))
# End:
