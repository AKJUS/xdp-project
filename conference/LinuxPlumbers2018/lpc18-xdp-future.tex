% -*- TeX-engine: default; -*-
\documentclass[sigconf]{acmart}
\usepackage[font=footnotesize]{subcaption}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{booktabs}
\usepackage{xcolor}

\setcopyright{none}
\acmConference[LPC '18 Networking Track]{Linux Plumbers Conference 2018 Networking Track}{Nov 13--14,
2018}{Vancouver, British Columbia}
\acmISBN{}
\acmDOI{}
\settopmatter{printccs=false, printacmref=false}
%\widowpenalty=100
%\clubpenalty=100
%\brokenpenalty=100

\begin{document}
\title{XDP -- challenges and future work}
\author{Jesper Dangaard Brouer}
\affiliation{%
  \institution{Red Hat}}
\email{brouer@redhat.com}

\author{Toke Høiland-Jørgensen}
\affiliation{%
  \institution{Karlstad University}}
\email{toke@toke.dk}


\captionsetup{font+=small}


\begin{abstract}
XDP already offers rich facilities for high performance packet
processing, and has seen deployment in several production
systems. However, this does not mean that XDP is a finished system; on
the contrary, improvements are being added in every release of Linux,
and rough edges are constantly being filed down. The purpose of this
talk is to discuss some of these possibilities for future
improvements, including how to address some of the known limitations
of the system. We are especially interested in soliciting feedback and
ideas from the community on the best way forward.

The issues we are planning to discuss include, but are not limited to:

\begin{itemize}
\item User experience and debugging tools: How do we make it easier for people
  who are not familiar with the kernel or XDP to get to grips with the system
  and be productive when writing XDP programs?

\item Driver support: How do we get to full support for XDP in all drivers? Is
  this even a goal we should be striving for?

\item Performance: At high packet rates, every micro-optimisation counts. Things
  like inlining function calls in drivers are important, but also batching to
  amortise fixed costs such as DMA mapping. What are the known bottlenecks, and
  how do we address them?

\item QoS and rate transitions: How should we do QoS in XDP? In particular, rate
  transitions (where a faster link feeds into a slower) are currently hard to
  deal with from XDP, and would benefit from, e.g., Active Queue Management
  (AQM). Can we adapt some of the AQM and QoS facilities in the regular
  networking stack to work with XDP? Or should we do something different?

\item Accelerating other parts of the stack: Tom Herbert started the discussion
  on accelerating transport protocols with XDP back in 2016. How do we make
  progress on this? Or should we be doing something different? Are there other
  areas where we can extend XDPs processing model to provide useful
  accelerations?
\end{itemize}

\end{abstract}


\maketitle

\section{Introduction}%
\label{sec:introduction}

We have recently finished writing an academic paper on XDP~\cite{xdp-paper},
which includes a description of the architecture, and performance comparison
with DPDK. In the course of that work, we have identified and discussed several
areas of potential improvements. The purpose of this paper, and the associated
talk, is to broaden that discussion to the Linux community to solicit feedback,
and to give a broad overview of ideas and directions of work that is in
progress, as well as some things that are still at the idea stage.

To do this, we include an overview of the other XDP-related talk and topics that
will be covered at the networking track at LPC '18; those topics will not be
covered in detail in this work. In addition, we include a couple of ideas that
are not covered elsewhere, which we will discuss in more detail.

\section{XDP in production use}
\label{sec:xdp-production-use}

Even though XDP is still being very actively developed (as the large number of
talks at LPC shows), we have already seen examples of it being successfully
deployed in production environments. High-profile examples include Cloudflare's
use for DDOS protection~\cite{cloudflare-ddos}, and Facebook's Katran load
balancer~\cite{katran}. There are also XDP plugins for Suricata~\cite{suricata},
Open vSwitch~\cite{ovs-xdp}, and even DPDK~\cite{dpdk-xdp}.

There are two other talks at LPC that cover Facebook's production use of XDP in
more detail~\cite{facebook-lessons,facebook-firewall}.

 \section{The state of XDP performance}
\label{sec:state-xdp-perf}

XDP has been deliberately designed to achieve maximum packet processing
performance. This is achieved by combining several techniques, such as avoiding
memory allocations in the processing path, and running the processing at the
earliest possible time after packets are received from the hardware. The results
of these efforts is the impressive performance shown in
Figures~\ref{fig:drop-test} and \ref{fig:redirect-test} (both from the XDP
paper~\cite{xdp-paper}).

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{images/drop-test.pdf}
\caption{\label{fig:drop-test} Packet drop performance. DPDK uses one core for
  control tasks, so only 5 are available for packet processing.}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{images/redirect-test.pdf}
\caption{\label{fig:redirect-test} Packet forwarding throughput. Sending and
  receiving on the same interface takes up more bandwidth on the same PCI port,
  which means we hit the PCI bus limit at 70 Mpps.}
\end{figure}

However, attaining this level of performance is not trivial, and the smallest
optimisations (or conversely, small additions of overhead) can have a large
impact. As an example, the Mellanox driver we used for our tests
(\texttt{mlx5}), performs 10 non-inlined function calls for every packet. Our
tests show that the overhead \emph{of just the function calls} corresponds to an
additional 9\,Mpps of performance on a single core.

Because of this sensitivity to overhead, it is imperative that thorough
performance evaluations of new features are performed to avoid regressions, and
a guiding principle should be that new features must not negatively affect
baseline XDP performance. One optimisation technique that can be used to achieve
this is to move as many checks as possible to setup time rather than execution
time.





\section{Evolving XDP}
\label{sec:evolving-xdp}

One way to view XDP is as a ``software offload'', which can accelerate critical
parts of the packet processing path, while allowing the regular network stack to
handle the rest. This is possible because of the ability to mix custom
high-speed packet processing with the features already implemented in the
kernel. The networking stack already contains high-quality implementations of
features such as routing and bridging, which an XDP program can cherry-pick
among to perform its tasks without incurring the overhead of the full networking
stack. An example of this approach is the routing lookup helper added by David
Ahern, which he covers in a separate talk~\cite{ahern-routing}.

We foresee that the addition of additional kernel helpers as an important avenue
for extending the functionality of XDP. In many cases, functionality can be
implemented by a custom eBPF program using maps; however, exposing existing
kernel functionality has the advantage of retaining the existing configuration
and management interface of the kernel. In addition, this makes it possible to
let the regular networking stack handle tricky edge cases, allowing the XDP
program to focus on accelerating the fast path. We believe this is a killer
feature of XDP, and we wish to encourage people to think about adding (or
requesting!) such helpers where it makes sense for their use case.

\section{XDP as a building block}
\label{sec:xdp-as-building}


\section{Zero-copy to userspace with AF\_XDP}
\label{sec:zero-copy-userspace}

\section{Moving SKB allocation out of device drivers}
\label{sec:moving-skb-alloc}

\section{Usability: Decoupling XDP\_REDIRECT with ndo\_xdp\_xmit resource
  control}
\label{sec:usab-deco-xdp_r}

\section{Partial XDP support in drivers}
\label{sec:partial-xdp-support}

\section{XDP egress hook}
\label{sec:xdp-egress-hook}

A limitation of the current design of XDP is that programs get no feedback if a
redirect to another device fails. Instead, the packet is just silently dropped,
and the only way to see why is by attaching to the right tracepoint. This is
especially problematic when forwarding packets from a fast device to a slower
one. And the way \texttt{XDP\_REDIRECT} is implemented, there is no way for the
XDP program to gain insight into the state of the device being forwarded
\emph{to}.

We believe that a possible fix for this is to add another eBPF hook at packet
egress from a device, i.e., at the latest possible time before a packet is put
into the device TX ring. At this point, it is possible for the driver to supply
information about the current state of the TX ring buffer (such as free space),
which the eBPF program can react appropriately to, for example by signalling
ingress XDP programs to send traffic another way if the TX ring is full, or by
implementing AQM-like reactions when TX ring pressure increases.

\section{NIC memory models and DMA mapping}
\label{sec:nic-memory-models}



\bibliographystyle{ACM-Reference-Format}
\bibliography{xdp-future}

\end{document}
