% -*- TeX-engine: default; -*-
\documentclass[sigconf]{acmart}
\usepackage[font=footnotesize]{subcaption}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{booktabs}
\usepackage{xcolor}

\setcopyright{none}
\acmConference[LPC '18 Networking Track]{Linux Plumbers Conference 2018 Networking Track}{Nov 13--14,
2018}{Vancouver, British Columbia}
\acmISBN{}
\acmDOI{}
\settopmatter{printccs=false, printacmref=false}
%\widowpenalty=100
%\clubpenalty=100
%\brokenpenalty=100

\begin{document}
\title{XDP -- challenges and future work}
\author{Jesper Dangaard Brouer}
\affiliation{%
  \institution{Red Hat}}
\email{brouer@redhat.com}

\author{Toke Høiland-Jørgensen}
\affiliation{%
  \institution{Karlstad University}}
\email{toke@toke.dk}


\captionsetup{font+=small}


\begin{abstract}
XDP already offers rich facilities for high performance packet
processing, and has seen deployment in several production
systems. However, this does not mean that XDP is a finished system; on
the contrary, improvements are being added in every release of Linux,
and rough edges are constantly being filed down. The purpose of this
talk is to discuss some of these possibilities for future
improvements, including how to address some of the known limitations
of the system. We are especially interested in soliciting feedback and
ideas from the community on the best way forward.

The issues we are planning to discuss include, but are not limited to:

\begin{itemize}
\item User experience and debugging tools: How do we make it easier for people
  who are not familiar with the kernel or XDP to get to grips with the system
  and be productive when writing XDP programs?

\item Driver support: How do we get to full support for XDP in all drivers? Is
  this even a goal we should be striving for?

\item Performance: At high packet rates, every micro-optimisation counts. Things
  like inlining function calls in drivers are important, but also batching to
  amortise fixed costs such as DMA mapping. What are the known bottlenecks, and
  how do we address them?

\item QoS and rate transitions: How should we do QoS in XDP? In particular, rate
  transitions (where a faster link feeds into a slower) are currently hard to
  deal with from XDP, and would benefit from, e.g., Active Queue Management
  (AQM). Can we adapt some of the AQM and QoS facilities in the regular
  networking stack to work with XDP? Or should we do something different?

\item Accelerating other parts of the stack: Tom Herbert started the discussion
  on accelerating transport protocols with XDP back in 2016. How do we make
  progress on this? Or should we be doing something different? Are there other
  areas where we can extend XDPs processing model to provide useful
  accelerations?
\end{itemize}

\end{abstract}


\maketitle

\section{Introduction}%
\label{sec:introduction}

We have recently finished writing an academic paper on XDP~\cite{xdp-paper},
which includes a description of the architecture, and performance comparison
with DPDK. In the course of that work, we have identified and discussed several
areas of potential improvements. The purpose of this paper, and the associated
talk, is to broaden that discussion to the Linux community to solicit feedback,
and to give a broad overview of ideas and directions of work that is in
progress, as well as some things that are still at the idea stage.

To do this, we include an overview of the other XDP-related talk and topics that
will be covered at the networking track at LPC '18; those topics will not be
covered in detail in this work. In addition, we include a couple of ideas that
are not covered elsewhere, which we will discuss in more detail.

\section{XDP in production use}
\label{sec:xdp-production-use}

Even though XDP is still being very actively developed (as the large number of
talks at LPC shows), we have already seen examples of it being successfully
deployed in production environments. High-profile examples include Cloudflare's
use for DDOS protection~\cite{cloudflare-ddos}, and Facebook's Katran load
balancer~\cite{katran}. There are also XDP plugins for Suricata~\cite{suricata},
Open vSwitch~\cite{ovs-xdp}, and even DPDK~\cite{dpdk-xdp}.

There are two other talks at LPC that cover Facebook's production use of XDP in
more detail~\cite{facebook-lessons,facebook-firewall}.

 \section{The state of XDP performance}
\label{sec:state-xdp-perf}

XDP has been deliberately designed to achieve maximum packet processing
performance. This is achieved by combining several techniques, such as avoiding
memory allocations in the processing path, and running the processing at the
earliest possible time after packets are received from the hardware. The results
of these efforts is the impressive performance shown in
Figures~\ref{fig:drop-test} and \ref{fig:redirect-test} (both from the XDP
paper~\cite{xdp-paper}).

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{images/drop-test.pdf}
\caption{\label{fig:drop-test} Packet drop performance. DPDK uses one core for
  control tasks, so only 5 are available for packet processing.}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{images/redirect-test.pdf}
\caption{\label{fig:redirect-test} Packet forwarding throughput. Sending and
  receiving on the same interface takes up more bandwidth on the same PCI port,
  which means we hit the PCI bus limit at 70 Mpps.}
\end{figure}

However, attaining this level of performance is not trivial, and the smallest
optimisations (or conversely, small additions of overhead) can have a large
impact. As an example, the Mellanox driver we used for our tests
(\texttt{mlx5}), performs 10 non-inlined function calls for every packet. Our
tests show that the overhead \emph{of just the function calls} corresponds to an
additional 9\,Mpps of performance on a single core.

Because of this sensitivity to overhead, it is imperative that thorough
performance evaluations of new features are performed to avoid regressions, and
a guiding principle should be that new features must not negatively affect
baseline XDP performance. One optimisation technique that can be used to achieve
this is to move as many checks as possible to setup time rather than execution
time.





\section{Evolving XDP}
\label{sec:evolving-xdp}

\section{XDP as a building block}
\label{sec:xdp-as-building}

\section{Zero-copy to userspace with AF\_XDP}
\label{sec:zero-copy-userspace}

\section{Moving SKB allocation out of device drivers}
\label{sec:moving-skb-alloc}

\section{Usability: Decoupling XDP\_REDIRECT with ndo\_xdp\_xmit resource
  control}
\label{sec:usab-deco-xdp_r}

\section{Partial XDP support in drivers}
\label{sec:partial-xdp-support}

\section{XDP egress hook}
\label{sec:xdp-egress-hook}

\section{NIC memory models and DMA mapping}
\label{sec:nic-memory-models}



\bibliographystyle{ACM-Reference-Format}
\bibliography{xdp-future}

\end{document}
