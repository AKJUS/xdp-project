# -*- fill-column: 79; -*-
#+TITLE: Notes for netdev-track at LinuxPlumbers 2018

Trying to organize and structure the presentation for Linux Plumbers Conference
2018 Networking Track.

 http://vger.kernel.org/lpc-networking.html

* Homepage abstract for presentation

http://vger.kernel.org/lpc-networking.html#session-19

** Title: XDP challenges and future work

Speakers: Jesper Dangaard Brouer, Toke Høiland-Jørgensen

Duration (incl. QA): 35 min

Content: Slides, Paper

** Abstract:

XDP already offers rich facilities for high performance packet
processing, and has seen deployment in several production
systems. However, this does not mean that XDP is a finished system; on
the contrary, improvements are being added in every release of Linux,
and rough edges are constantly being filed down. The purpose of this
talk is to discuss some of these possibilities for future
improvements, including how to address some of the known limitations
of the system. We are especially interested in soliciting feedback and
ideas from the community on the best way forward.

The issues we are planning to discuss include, but are not limited to:

 - User experience and debugging tools: How do we make it easier for
   people who are not familiar with the kernel or XDP to get to grips
   with the system and be productive when writing XDP programs?

 - Driver support: How do we get to full support for XDP in all
   drivers? Is this even a goal we should be striving for?

 - Performance: At high packet rates, every micro-optimisation
   counts. Things like inlining function calls in drivers are
   important, but also batching to amortise fixed costs such as DMA
   mapping. What are the known bottlenecks, and how do we address
   them?

 - QoS and rate transitions: How should we do QoS in XDP? In
   particular, rate transitions (where a faster link feeds into a
   slower) are currently hard to deal with from XDP, and would benefit
   from, e.g., Active Queue Management (AQM). Can we adapt some of the
   AQM and QoS facilities in the regular networking stack to work with
   XDP? Or should we do something different?

 - Accelerating other parts of the stack: Tom Herbert started the
   discussion on accelerating transport protocols with XDP back
   in 2016. How do we make progress on this? Or should we be doing
   something different? Are there other areas where we can extend XDPs
   processing model to provide useful accelerations?


* Other XDP talks co-scheduled

I promised we will introduce other XDP talks.

Thus, part of the structure will be given by other XDP talks.

** Other XDP and eBPF related talks:

When below are marked DONE, this means a slide mention it.

*** (Our-talk) XDP - challenges and future work
*** DONE Leveraging Kernel Tables with XDP
*** DONE XDP acceleration using NIC metadata, continued
*** DONE (AF_XDP) Bringing the Power of eBPF to Open vSwitch
*** DONE (AF_XDP) The Path to DPDK Speeds for AF_XDP
*** DONE (facebook) XDP 1.5 years in production. Evolution and lessons learned.
*** DONE (facebook) eBPF / XDP based firewall and packet filtering
*** DONE P4C-XDP: Programming the Linux Kernel Forwarding Plane using P4
*** Using eBPF as an abstraction for switching
*** Building socket-aware BPF programs
*** BPF Host Network Resource Management
*** Combining kTLS and BPF for Introspection and Policy Enforcement


*** XDP/eBPF unrelated talks:

Experiences Evaluating DC-TCP

Scaling Linux bridge forwarding database

ERSPAN Support for Linux

This talk is not about XDP: From Resource Limits to SKB Lists.

Optimizing UDP for content delivery with GSO, pacing and zerocopy.

Linux SCTP is catching up and going above!

What's happened to the world of networking hardware offloads?

TC SW datapath: a performance analysis

PHYlink and SFP: Going beyond 1G Copper PHYs


* Topics and slides

*IDEA* for organizing presentation:

The presentation will be organized around a number of XDP-topics.  We cannot
bring-up all topics, but have selected some.  To introduce other talks, their
topics have to be included. Also have some of our own that to get feedback.

The "Topic:" notation will be transition slide (or none), and the "Slide:"
notation will be actual slides.  The "Topic:" notes describe more about the
topic, this text could be used in the paper.

** Topic: Story baseline

 - Introduce XDP-paper.

   We wrote XDP-paper, that doc XDP architecture, and do head-to-head comparison
   against DPDK.

 - This talk is focused on "Limitation and Future Work".

   We are fortunate that other people have already started to work on "future
   work" items, and are even being covered and presented at this conf.  We will
   mention these areas and defer the details and discussion to these talks.

 - Purpose soliciting feedback and ideas from the community

   Besides referencing the topics covered in other talks, we have also selected
   some XDP topics that we wish to soliciting feedback om from this community.

** Slide: XDP-paper

Scientific XDP paper accepted
 - Title: "The eXpress Data Path: Fast Programmable Packet Processing in the Operating System Kernel"
 - Conference: ACM CoNEXT 2018 Greece

This talk focused: "Limitation and Future Work"
 - Purpose soliciting feedback and ideas from the community

** Slide: What is XDP

Frame what is XDP.

Be inspirational: New programmable layer in network stack

Keep it very short, if possible 1-slide.  This is mostly for people finding this
slide deck, or LPC people that don't know that XDP is.

** Slide: XDP is a huge success

XDP is maturing in upstream kernels
 - Still under active development

Very popular topic at this LPC network-track miniconf
 - 11 talks XDP (or BPF) related!

** Slide: Production use-cases

(Can people actually use this in production?)

XDP have seen production use
 - CloudFlare publically say they use XDP
 - Suricata have XDP plugins

Other talks will cover this:
 - (facebook) XDP 1.5 years in production. Evolution and lessons learned
 - (facebook) eBPF / XDP based firewall and packet filtering

** Topic: Performance

XDP is all about performance, don't screw it up!

Guiding principle: Adding features must not negatively affect baseline XDP
performance.  Use optimization technique of moving runtime checks to setup time
checks.

** Slide: New XDP features and performance requirement

XDP is all about performance
 - Watch-out that feature creep doesn't kill performance!

Guiding principle: New features must not negatively affect baseline XDP perf
 - Optimization hint: move runtime checks to setup time

** Slide/topic (maybe skip): Jumbo-frames

(Topic text) People in XDP-newbies list request jumbo-frames. They don't seem
to understand to support high-speed packet processing, we cannot support this
feature.  Besides killing performance, jumbo-frames will also make it more
difficult to move-SKB-alloc out of drivers.

Understand: RX-ring cannot know packet length, must have room for MTU
 - XDP requires packet mapped in physical continuous memory
   (Due to eBPF mem direct-access, safety via verifier)
 - Alloc above 4K, more expensive (higher-order pages)

Hybrid solution: Only grant XDP access to top 4K
 - Above 4K can be in other pages
 - This will kill, moving SKB allocations out of driver plan
   (makes it complicated and thus affect performance)

** Topic: Evolving XDP

How we imagine XDP getting extended.

We see XDP as a software offloading layer in the kernel network stack.

IP-forwarding is a good example, as the Linux kernel and ecosystem have
everything to become a L3 IP-router, with control-plane software like router
daemons etc.  The forwarding performance is constrained due to software
overhead, which is where XDP can help.  We want XDP work in-concert with this
ecosystem.  This can best be achieved by eBPF helper functions that allow XDP
to perform lookup in kernel tables, e.g. like the FIB lookup that was recently
added (also covered in the XDP-paper).

The topic and the FIB lookup is covered in David Ahern's talk:
"Leveraging Kernel Tables with XDP"

We want to encourage people to add these kind of helpers to XDP, when use-case
arise.  It is possible to e.g. track the IP-route table and ARP table via
monitoring RT-netlink messages, and maintain an on-the-side BPF maps that can
influence XDP route decisions.  This is actually showed as an XDP samples/bpf
program called xdp_router_ipv4, which was done before the FIB-lookup helper was
added.  The performance advantage is very small, and only occurs when
xdp_router_ipv4 sample hit an "exact_match" cache.

** Slide: Evolving XDP via BPF-helpers

XDP software offloading layer for kernel network stack
 - Setup and use Linux kernel network stack
 - But accelerate parts of it with XDP

IP-routing good example of XDP acceleration
 - Let Linux handle routing (deamons) and ARP-tables
 - Access routing FIB table lookups from XDP via BPF helpers
 - Encourage add helpers, instead of on-the-side BPF maps

The topic and the FIB lookup is covered in David Ahern's talk:
 "Leveraging Kernel Tables with XDP"

** Topic: XDP as a building block

   If it is not clear to people, explain that XDP is core kernel
   facility, that other Open Source projects need to pickup, use and
   innovate on-top of.

XDP will hopefully be used for faster delivery into Guest-OS.  The best and
fastest abstraction into KVM is still not determined.  The tuntap driver
already implemented ndo_xdp_xmit, which allows XDP_REDIRECT into XXX (TODO is
it virtio_net or vhost_net???), bypassing the Host-OS skb-alloc, while still
performing one copy.  Other options might be to leverage AF_XDP to register
Guest-OS memory with the XDP based NIC driver, which would allow zero-copy RX
into the Guest-OS.

The discussion of eBPF/XDP vs P4 often comes up. Our view is that of-cause you
should be able to write a data-plane in domain-specific language like P4. And
to use load this in a XDP hook, you need to write a new backend to your P4
compiler that generate the XDP/eBPF code.

** Slide: XDP as a building block

XDP is a core kernel layer building block
 - Open Source projects should build and innovate on top
 - Only time will tell what people use it for

Directions we hope to see XDP used for
 - Faster delivery into Guest-OS
   - Already possible via 1-copy via tuntap driver into virtio_net
   - The AF_XDP approach might offer zero-copy

P4 vs eBPF/XDP is a wrong attitude
 - Instead translate/compile your P4 code into eBPF/XDP compatible code
 - Talk on this approach:
   "P4C-XDP: Programming the Linux Kernel Forwarding Plane using P4"

** Topic: Zero-copy to userspace AF_XDP

   Ref two AF_XDP Talks.

   Pitch: XDP have been focused on keeping packet handling and processing in
   kernel space, via leveraging eBPF.  To avoid the overhead of syscalls,
   context switch and packet copies.

Personal note: I always had plans for a fast-path from XDP into a userspace
socket. (Discussed this public in G+ post with DaveM) I had imagined that the
kernel would do the memory allocation, for RX-ring, and VMA map this into
userspace.  AF_XDP goes the other way, and let userspace (pre) alloc.

   AF_XDP is about moving packet handling into userspace.  The key point for
   integrating this with XDP redirect is flexibility.  We want to avoid NIC
   hardware being taken over by the zero-copy facility.  Want to avoid the
   all-or-nothing proposition like we have seen with DPDK.

   AF_XDP avoids the overhead by establishing SPSC queues as communication
   channels to userspace.  The copy is avoided by userspace alloacting and
   gifting/providing kernel with this memory, which is used directly for RX
   DMA delivery.

   The AF_XDP socket is woken-up like a regular socket, but for
   high-performance userspace can poll the socket.

   On the TX-side AF_XDP does have a syscall, but userspace can fill
   several TX buffers into TX ring before calling the sendmsg syscall.

** Slide: Zero-copy to userspace AF_XDP

XDP usual performance benefit comes from in-kernel processing via BPF
 - (avoiding context switch to userspace)

AF_XDP is for faster raw packet delivery into userspace
 - Hooking into XDP, provides packet filter flexibility
 - Performance tricks:
   - Preallocated userspace memory, getting fill by kernel
   - NetChannel like SPSC queues between kernel and userspace
 - Unlike tcpdump, as it owns/steals the packet
   * Might want to invent/allow a copy and XDP_PASS mode.

Two talks about AF_XDP on schedule:
 - (AF_XDP) The Path to DPDK Speeds for AF_XDP
 - (AF_XDP) Bringing the Power of eBPF to Open vSwitch

** Topic: Move SKB allocation out of driver

   The long term goal is moving SKB allocations out of driver code.

   This is already supported by all drivers implementing XDP_REDIRECT, as
   CPU-map and redirects into tun-driver, create the SKB later based on the
   xdp_frame.  Working towards generalizing this.

   Missing part are howto transfer the different driver offloads
   (e.g. csum-info, rxhash, HW-mark) in a vendor neutral and generic way.  This
   depends/waits on metadata talk, for this to be generic enough.

** Slide: Move SKB allocation out of driver

Long term goal:
 - Write NIC driver(s) without SKB allocations

Not alloc SKB in driver at RX: Is supported today via XDP_REDIRECT
 - Via redirect into CPU-map or tun-driver

Missing part: driver offloads (e.g. csum-info, rxhash, HW-mark)
 - Depends/waits on metadata, in a vendor neutral way
 - We (also) need core-netstack need to understand metadata BTF struct
   - Not just BPF progs

Hope this will be resolved covered in talk:
 - Talk: "XDP acceleration using NIC metadata, continued"

** Topic: Controlling resources for ndo_xdp_xmit

(Usability related) XDP_REDIRECT entangled with ndo_xdp_xmit.

Decouple XDP_REDIRECT from ndo_xdp_xmit.
This mostly technical, but also related to usability.

When XDP redirecting a frame out another net_device, then the drivers
ndo_xdp_xmit function is called.  But device drivers don't enable ndo_xdp_xmit
by default, because it costs many resource (TX queue per CPU core).  There is
no interface to enable ndo_xdp_xmit.  The current solution, to enable
ndo_xdp_xmit, is to load an dummy XDP program on the device, you want to
redirect to.

The implicit notion of loading an XDP program, also implies allocating
resources to XDP-TX queues is flawed.  As the XDP user, might not want to use
any redirect feature.  And even if using XDP_REDIRECT this could be CPU-map
redirect, which does not need XDP-TX queues.

The reason for only enabling XDP-TX queues when really needed is that this
consumes HW resources.  Given the TX queue per CPU core assumption, this makes
it problematic on systems with many CPU cores. E.g. it was discovered the ixgbe
driver cannot load XDP on systems with more than 96 CPU cores.

** Slide: Controlling resources for ndo_xdp_xmit

Redirecting XDP frame out another net_device, call ndo_xdp_xmit
 - But egress net_device don't default alloc XDP-TX resouces
   - Reason#1: consumes HW resources, 1 TX-queue per CPU core
   - Reason#2: cannot know it will be used for XDP traffic

We need proper API for enabling XDP-TX resources on egress net_device
 - Current hack, load dummy XDP prog on egress dev
 - Would be natural to, alloc TX resources when dev added to DEVMAP
 - But how to handle non-map redirect variant?

This implicit XDP-TX resources allocation when loading XDP
 - is wasting HW resources when XDP doesn't need ndo_xdp_xmit

** Topic: (Usability related) Driver Support

   Issue that not all drivers support all features, but userspace cannot query
   what a given driver supports.  If a driver e.g. doesn't support XDP_REDIRECT,
   then it can only be detected "runtime" by observing a WARN_ONCE kernel log
   message and afterwards packets are silently dropped (can be debugged via
   tracepoints).

   The issue was seen with Suricata, and they want some kind of way to
   know what XDP features are avail.  This is needed to reject or
   change behavior when parsing the Suricata setup file.

Original goal of not exposing feature-bits to userspace, was to force full
support for XDP in all drivers.  Is this goal still realistic, after X kernel
releases, where only 3 HW-drivers have implemented XDP_REDIRECT.

** Topic: XDP egress hook (just before TX)

Can be used for:

- Reacting to TX ring status
- Implementing QoS / AQM

** Topic: NIC memory models and DMA mapping

XDP recently (v4.18) got support for different memory models per driver
RX-queue, via the xdp_return_frame() and xdp_rxq_info_reg_mem_model() APIs.

This allow drivers to innovate new memory models, but also gives the
opportunity to generalize and share common code to handle memory recycle
schemes for drivers.  The page_pool is one example common code.

We want to see more drivers need to use page_pool, and work on page_pool is
needed, especially in the area of keeping frames DMA mapped.

We plan to extend the xdp_return_frame API with a bulking option, because it
can naturally do bulking at DMA-TX completion, and the page_pool need this to
handle a known weakness (of concurrent CPUs returning frames to the same
RX-queue).


On Intel machines the DMA map/unmap/sync operation are very lightweight, as due
the coherency model, this might not be true for other architectures.  As XDP
have been very Intel focused, the DMA overhead have seen much attention. The
Spectre-V2 mitigation changed the picture. and will force us to address the DMA
overhead issues due to the indirect call API.

** Topic: Spectre-V2: mitigation killed XDP performance

   (This is related to DMA mapping)

   This is primary due to DMA-API, what uses indirect call to abstract
   different DMA-engines.

   For mlx5 driver, there is also issues with indirect functions pointer calls,
   inside the driver itself.

   For XDP there are a number of workarounds and performance optimizations for
   the DMA slowdown.  The easiest solution is to amortized via bulking DMA API
   calls.  The DMA APIs already have the scatter-gather API, which in-effect is
   bulking.

   Today most drivers already avoid repeated DMA map/unmap calls, by keeping
   the DMA mapping intact, while processing the frame (e.g by normal netstack)
   and if recycling works they don't need to unmap the frame. They instead does
   the DMA-sync in appropriate places.

   The page_pool API, want to generalize keeping the page DMA-mapped, such that
   the driver doesn't have to do this.  No driver currently takes advantage of
   this DMA page_pool feature.

   During ndo_xdp_xmit(), today, individual frames need to be DMA-mapped for
   the TX device.  Recently this changed to bulk API (curr bulk 16), which
   would easily do bulk DMA mapping.

   Would it make sense to move redirect DMA mapping into redirect-core code?
   Do drivers need fine control over the exact DMA-map call?  If not, the
   DMA-TX addr could be stored in xdp_frame, then xdp_return_frame API could
   also handle the DMA-unmap call.

** Topic (maybe skip): Usability: User Experience and Debugging

Lots of small unexpected behavior, seen from normal users of XDP.

E.g. The sample xdp_redirect_map show RX packets per sec, but people think this
is TX packet per sec.  In case the redirect TX device does not support XDP,
there is not immediate feedback, the packets are silently dropped (can be
caught via tracepoint).  Thus, users observe increased PPS counter, when
misconfig happens.

This RX counting, seems natural as a XDP-core developer, as we know the eBPF
program cannot know or measure was happens _after_ it have run (as it simply
returns a verdict/action return code). For a user, it will be more natural to
"see" TX-pps. So, we could monitor interface TX-stats, but that is also not
possible, as some XDP-drivers don't account XDP-TX packets in the regular
ifstat counters, which in itself is problematic.

** Topic (maybe skip): eBPF verifier

   All the discussions about extending the eBPF verifier, should be
   move to the eBPF-mini-conf.

** Topic (maybe skip): ARM and XDP support

We need to make sure, our XDP optimizations does not become too Intel specific.

I have bought a MacchiatoBin ARM64-board, that I plan to developed XDP for.
(Got cross-compiler and upstream kernel working, found DMA-bounce buffer
issues, that I need to upstream fix for)

** Topic (maybe skip) : Accelerating Transport Protocols

   E.g. it should be possible to do delivery into TCP sockets, and
   hand-over the packet-page (without first allocating an SKB).

   Ref-Talk: "Building socket-aware BPF programs" is part of this work.


* Notes
