# -*- fill-column: 79; -*-
#+TITLE: Notes for netdev-track at LinuxPlumbers 2018

Trying to organize and structure the presentation for Linux Plumbers Conference
2018 Networking Track.

 http://vger.kernel.org/lpc-networking.html

* Homepage abstract for presentation

http://vger.kernel.org/lpc-networking.html#session-19

** Title: XDP challenges and future work

Speakers: Jesper Dangaard Brouer, Toke Høiland-Jørgensen

Duration (incl. QA): 35 min

Content: Slides, Paper

** Abstract:

XDP already offers rich facilities for high performance packet
processing, and has seen deployment in several production
systems. However, this does not mean that XDP is a finished system; on
the contrary, improvements are being added in every release of Linux,
and rough edges are constantly being filed down. The purpose of this
talk is to discuss some of these possibilities for future
improvements, including how to address some of the known limitations
of the system. We are especially interested in soliciting feedback and
ideas from the community on the best way forward.

The issues we are planning to discuss include, but are not limited to:

 - User experience and debugging tools: How do we make it easier for
   people who are not familiar with the kernel or XDP to get to grips
   with the system and be productive when writing XDP programs?

 - Driver support: How do we get to full support for XDP in all
   drivers? Is this even a goal we should be striving for?

 - Performance: At high packet rates, every micro-optimisation
   counts. Things like inlining function calls in drivers are
   important, but also batching to amortise fixed costs such as DMA
   mapping. What are the known bottlenecks, and how do we address
   them?

 - QoS and rate transitions: How should we do QoS in XDP? In
   particular, rate transitions (where a faster link feeds into a
   slower) are currently hard to deal with from XDP, and would benefit
   from, e.g., Active Queue Management (AQM). Can we adapt some of the
   AQM and QoS facilities in the regular networking stack to work with
   XDP? Or should we do something different?

 - Accelerating other parts of the stack: Tom Herbert started the
   discussion on accelerating transport protocols with XDP back
   in 2016. How do we make progress on this? Or should we be doing
   something different? Are there other areas where we can extend XDPs
   processing model to provide useful accelerations?


* Other XDP talks co-scheduled

I promised we will introduce other XDP talks.

Thus, part of the structure will be given by other XDP talks.

** Other XDP and eBPF related talks:

*** (Our-talk) XDP - challenges and future work
*** Leveraging Kernel Tables with XDP
*** XDP acceleration using NIC metadata, continued
*** (AF_XDP) Bringing the Power of eBPF to Open vSwitch
*** (AF_XDP) The Path to DPDK Speeds for AF_XDP
*** Combining kTLS and BPF for Introspection and Policy Enforcement
*** (facebook) XDP 1.5 years in production. Evolution and lessons learned.
*** (facebook) eBPF / XDP based firewall and packet filtering
*** Using eBPF as an abstraction for switching
*** P4C-XDP: Programming the Linux Kernel Forwarding Plane using P4
*** Building socket-aware BPF programs
*** BPF Host Network Resource Management


*** XDP/eBPF unrelated talks:

Experiences Evaluating DC-TCP

Scaling Linux bridge forwarding database

ERSPAN Support for Linux

This talk is not about XDP: From Resource Limits to SKB Lists.

Optimizing UDP for content delivery with GSO, pacing and zerocopy.

Linux SCTP is catching up and going above!

What's happened to the world of networking hardware offloads?

TC SW datapath: a performance analysis

PHYlink and SFP: Going beyond 1G Copper PHYs


* Topics (also need to covering other talks)

*IDEA* for organizing presentation:

The presentation will be organized around a number of XDP-topics.  We cannot
bring-up all topics, but have selected some.  To introduce other talks, their
topics have to be included. Also have some of our own that to get feedback.

** Topic: Zero-copy to userspace AF_XDP

   Ref two AF_XDP Talks.

   Pitch: XDP have been focused on keeping packet handling and processing in
   kernel space, via leveraging eBPF.  To avoid the overhead of syscalls,
   context switch and packet copies.

   AF_XDP is about moving packet handling into userspace.  The key point for
   integrating this with XDP redirect is flexibility.  We want to avoid NIC
   hardware being taken over by the zero-copy facility.  Want to avoid the
   all-or-nothing proposition like we have seen with DPDK.

   AF_XDP avoids the overhead by establishing SPSC queues as communication
   channels to userspace.  The copy is avoided by userspace alloacting and
   gifting/providing kernel with this memory, which is used directly for RX
   DMA delivery.

   The AF_XDP socket is woken-up like a regular socket, but for
   high-performance userspace can poll the socket.

   On the TX-side AF_XDP does have a syscall, but userspace can fill
   several TX buffers into TX ring before calling the sendmsg syscall.

** Topic: Usability: User Experience and Debugging

** Topic: (Usability related) Driver Support

   Issue that not all drivers support all features, but userspace cannot query
   what a given driver supports.  If a driver e.g. doesn't support XDP_REDIRECT,
   then it can only be detected "runtime" by observing a WARN_ONCE kernel log
   message and afterwards packets are silently dropped (can be debugged via
   tracepoints).

   The issue was seen with Suricata, and they want some kind of way to
   know what XDP features are avail.  This is needed to reject or
   change behavior when parsing the suricata setup file.

** Topic: (Usability related) XDP_REDIRECT entangled with ndo_xdp_xmit.

   Decouple XDP_REDIRECT from ndo_xdp_xmit.
   This mostly technical, but also related to usability.

   Upstream sees XDP_REDIRECT driver support related to also supporting
   ndo_xdp_xmit.  IHMO these two functions are actually separate.  The
   XDP_REDIRECT action can be used completely without ndo_xdp_xmit, e.g for
   CPU-map redirect.

   The problem of enabling ndo_xdp_xmit on a driver is the HW resources it
   consumes, and the user might not even need the feature.  XDP-core assumes
   NIC driver allocates one dedicated TX-queue per CPU core in the system.
   This is wasting HW resources, and might not even be possible on systems with
   many CPU cores.

   E.g. it was discovered the ixgbe cannot be used on systems with more than 96
   CPU cores.

** Topic: Move SKB allocation out of driver

   The long term goal is moving SKB allocations out of driver code.

   This is already supported by all drivers implementing XDP_REDIRECT, as
   CPU-map and redirects into tun-driver, create the SKB later based on the
   xdp_frame.  Working towards generalizing this.

   Missing part are howto transfer the different driver offloads
   (e.g. csum-info, rxhash, HW-mark) in a vendor neutral and generic way.  This
   depends/waits on metadata talk, for this to be generic enough.

** Topic: QoS and Rate Transitions

** Topic: Accelerating Transport Protocols

   E.g. it should be possible to do delivery into TCP sockets, and
   hand-over the packet-page (without first allocating an SKB).

   Ref-Talk: "Building socket-aware BPF programs" is part of this
   work.

** Topic (maybe skip): eBPF verifier

   All the discussions about extending the eBPF verifier, should be
   move to the eBPF-mini-conf.

** Topic (maybe skip): XDP as a building block

   If it is not clear to people, explain that XDP is core kernel
   facility, that other Open Source projects need to pickup, use and
   innovate on-top of.

** Topic: NIC memory models and DMA mapping

   Does Bulk xdp_frame_return fit here?

** Topic: ARM and XDP support



* Slides

** (Story baseline) Slide: XDP-paper

 - Introduce XDP-paper.

   We wrote XDP-paper, that doc XDP architecture, and do head-to-head comparison
   against DPDK.

 - This talk is focused on "Limitation and Future Work".

   We are fortunate that other people have already started to work on "future
   work" items, and are even being covered and presented at this conf.  We will
   mention these areas and defer the details and discussion to these talks.

 - Purpose soliciting feedback and ideas from the community

   Besides referencing the topics covered in other talks, we have also selected
   some XDP topics that we wish to soliciting feedback om from this community.

** Slide: What is XDP

Frame what is XDP.

Be inspirational: New programmable layer in network stack

Keep it very short, if possible 1-slide.  This is mostly for people finding this
slide deck, or LPC people that don't know that XDP is.

** Slide: Production use-cases



Have seen real prod use-cases

Mention: 2x facebook talks

Mention: CloudFlare publically say they use XDP


** Slide: New XDP features and performance requirement

Slide with guideing principle: Adding features must not negativly affect
baseline XDP performance.  Use optimization technique of moving runtime checks
to setup time checks.
