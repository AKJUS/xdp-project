# -*- fill-column: 76; -*-
#+TITLE: Top-level XDP project management
#+CATEGORY: XDP
#+OPTIONS: ^:nil

This the top-level *XDP project management* file that contains *tasks* via
org-mode =TODO=, =NEXT= and =DONE=. The [[file:areas][areas directory]] also contains
*tasks*. It is recommended to use emacs when viewing and editing these
=.org= files, as the github rendering view removes the =TODO= and =DONE=
marking on the tasks.

Together with the emacs setup in [[file:../org-setup.el]], the extended
=org-agenda= view can be used for project management, and the setup
automatically detect [[http://doc.norang.ca/org-mode.html#TodoKeywordProjectTaskStates][Projects and Stuck Projects]]. For =org-agenda= view to
pickup these tasks, make sure to [[http://doc.norang.ca/org-mode.html#AgendaSetup][configure]] =org-agenda-files= to include
this directory and =areas/= directory.


* TODO Consistency for statistics with XDP

The short of it is that we need consistency in the counters across NIC
drivers and virtual devices. Right now stats are specific to a driver with
no clear accounting for the packets and bytes handled in XDP.

Progress: @dsahern (David Ahern) have started an email thread on the
subject: [[https://www.spinics.net/lists/netdev/msg535239.html][consistency for statistics with XDP mode]]

** TODO Missing update of ifconfig counters

Some drivers are not updating the "ifconfig" stats counters,
when in XDP mode.  This makes receive or send via XDP invisible to
sysadm/management tools.  This for-sure is going to cause confusion.

Closer look at other drivers.

 - ixgbe driver is doing the right thing.

 - i40e had a bug, where RX/TX stats are swapped (fixed in
   commit [[https://git.kernel.org/torvalds/c/cdec2141c24e][cdec2141c24e(v4.20-rc1)]]
   ("i40e: report correct statistics when XDP is enabled")).

 - mlx5 driver is not updating the regular RX/TX counters, but A LOT
   of other ethtool stats counters (which are the ones I usually
   monitor when testing).

** NEXT Figure out the counter semantics upstream
Need to have an upstream discussion, on what is the semantic.  IHMO
the regular RX/TX counters must be updated even for XDP frames.

** TODO Statistics per XDP-action

Accounting per XDP-action is also inconsistent across drivers. Some driver
account nothing, while others have elaborate counters exposed as ethtool
stats.

The common pattern is that XDP programs do their own accounting of action as
they see fit, and export this as BPF maps to their associated userspace
application, which a very per application specific approach.

David Ahern (@dsahern) argues that sysadm's need something more generic and
consistent, as they need a way to diagnose the system, regardless of the XDP
program that some developer asked to get installed. This argument that a 3rd
person should be able to diagnose a running XDP program was generally
accepted upstream.

There were a lot of performance concerns around introducing XDP-action
counters. Generally upstream voices wanted this to be opt-in, e.g. something
that the sysadm explicitly enable when needed, and not default on.

*** NEXT Implement simple XDP-actions counter and measure

As Saeed suggested do something really simple and central like:

#+begin_src diff
+++ b/include/linux/filter.h
@@ -651,7 +651,9 @@ static __always_inline u32 bpf_prog_run_xdp(const
struct bpf_prog *prog,
         * already takes rcu_read_lock() when fetching the program, so
         * it's not necessary here anymore.
         */
-       return BPF_PROG_RUN(prog, xdp);
+       u32 ret = BPF_PROG_RUN(prog, xdp);
+       xdp->xdp_rxq_info.stats[ret]++
+       return ret;
 }
#+end_src

WARNING: Realised above code is subject to speculative execution
side-channel leaking.

And measure if the performance concerns are real or not. Ilias or Jesper can
measure this on a slow ARM64 platform (espressobin), as only testing this on
Intel might lead to the wrong conclusion.

*** TODO Exporting XDP-actions counter to userspace

Collecting XDP-actions counter is only the first step.  We need to figure
out the best solution for exporting this to userspace.

One option is to piggyback on ethtool stats, which the drivers already use,
and it would also standardise the driver related XDP ethool stats.


* TODO Better ndo_xdp_xmit resource management
:PROPERTIES:
:OWNER:    tohojo
:END:

Driver resources needed to handle a ndo_xdp_xmit() is currently tied
to the driver having loaded an RX XDP program. This is strange, as
allocating these Driver TX HW resources is independent.

This can quickly lead to exhausting HW resources, like IRQs lines or
NIC TX HW queues, given it is assumed a TX queue is alloc/dedicated
for each CPU core.

** NEXT Change non-map xdp_redirect helper to use a hidden map
:LOGBOOK:
- State "NEXT"       from "WAIT"       [2019-02-25 Mon 15:09]
- State "WAIT"       from "NEXT"       [2019-02-21 Thu 13:00] \\
  Patch submitted, waiting for feedback
:END:

To be able to tie resource allocation to the interface maps (=devmap=), we
first need to change the non-map redirect variant so it uses a map under the
hood. Since xdp_redirect_map() is also significantly faster than the non-map
variant, this change should be a win in itself.

v1 comments and discussion: [[https://patchwork.ozlabs.org/patch/1046099/][Patch 1]] [[https://patchwork.ozlabs.org/patch/1046100/][Patch 2]]

[[https://patchwork.ozlabs.org/cover/1050219/][v3 patchwork link]]

** TODO Ethtool interface for enabling TX resources
Turns out the initial idea of using insertion into devmap as a trigger for
resource allocation doesn't work because of generic XDP. So we'll need an
ethtool interface; look into the existing channel configuration interface on
the kernel side and figure out how to express XDP resource allocation in a
good way.

** TODO Add automatic TX resource allocation to libbpf
Because we can't tie resource allocation to map insertion on the kernel
side, we need to solve the UI interface in userspace. So add a hook/wrapper
to libbpf that will automatically allocate TX resources when inserting into
a map.


* TODO Usability of programs in samples/bpf

The samples/bpf programs xdp_redirect + xdp_redirect_map are very user
unfriendly. #1 they use raw ifindex'es as input + output. #2 the pkt/s
number count RX packets, not TX'ed packets which can be dropped silently.
Red Hat QA, got very confused by #2.

** NEXT Change sample programs to accept ifnames as well as indexes

** NEXT Add TX counters to redirect samples/bpf programs

Simply include/sample the net_device TX stats.

** TODO Fix unloading wrong XDP on xdp-sample exit

The XDP sample programs unconditionally unload the current running XDP
program (via -1) on exit. If users are not careful with the order in-which
they start and stop XDP programs, then they get confused.

Almost done, but followup to make sure this gets merged upstream:
Upstream [[https://patchwork.ozlabs.org/project/netdev/list/?series=86597&state=%2a][proposal V1]] (by [[https://patchwork.ozlabs.org/project/netdev/list/?submitter=75761][Maciej Fijalkowski]]) is to check if the BPF-prog-ID
numbers match, before removing the current XDP-prog.

** TODO Change XDP-samples to enforce native-XDP and report if not avail

The default behaviour when attaching an XDP program on a driver that doesn't
have native-XDP is to fallback to generic-XDP, without notifying the user of
the end-state.

This behaviour is also used by xdp-samples, which unfortunately have lead
end-users to falsely think a given driver supports native-XDP. (QA are using
these xdp-samples and create cases due to this confusion).

Proposal is to change xdp-samples to enforce native-XDP, and report if this
was not possible, together with help text that display cmdline option for
enabling generic-XDP/SKB-mode.

** TODO Add xdpsock option to allow XDP_PASS for AF_XDP zero-copy mode

In AF_XDP zero-copy mode, sending frame to the network stack via XDP_PASS
results in an expense code path, e.g new page_alloc for copy of payload and
SKB alloc. We need this test how slow this code path is.

Also consider testing XDP-level redirect out another net_device with
AF_XDP-ZC enabled. (I think this will just drop the packets due to
mem_type).

** TODO xdp_monitor: record and show errno

It would be a big help diagnosing XDP issues if the xdp_monitor program also
reported the errno.

** TODO xdp_monitor: convert to use raw-tracepoints

The raw-tracepoints are suppose to be much faster, and XDP monitor want to
have as little impact on the system as possible. Thus, convert to use
raw-tracepoints.

* TODO BPF-selftests - top-level TODOs

The kernel git-tree contains a lot of selftests for BPF located in:
=tools/testing/selftests/bpf/=.

XDP (and its performance gain) is tied closely to NIC driver code, which
makes it hard to implement selftests for (including benchmark selftests).
Still we should have a goal of doing functional testing of the XDP core-code
components (via selftests).

Since driver =veth= got native-XDP support, we have an opportunity for
writing selftests that cover both generic-XDP and native-XDP.

** TODO bpf-selftest: improve XDP VLAN selftests

*Assignment* is to improve the selftest shell-script to test both
generic-XDP and native-XDP (for veth driver).

XDP add/remove VLAN headers have a selftest in =tools/testing/selftests/bpf/=
in files =test_xdp_vlan.c= and =test_xdp_vlan.sh=. This test was developed
in conjunction with fixing a bug in generic-XDP (see kernel commit
[[https://git.kernel.org/torvalds/c/297249569932][297249569932]] ("net: fix generic XDP to handle if eth header was mangled")).

Since driver =veth= got native-XDP support, the selftest no-longer tests
generic-XDP code path.

The ip utility (from iproute2) already support specifying, that an XDP prog
must use generic XDP when loading an XDP prog (option =xdpgeneric=).

** TODO bpf-selftest: find XDP-selftests affected by veth native-XDP

When driver =veth= got native-XDP support, then the XDP-selftests that were
based on =veth= changed from testing generic-XDP into testing native-XDP.

*Assignments:*
1. Determine how many and which veth based XDP-selftests are affected
2. Convert these selftests to test both generic-XDP and native-XDP

** TODO Make more XDP tests using BPF_PROG_TEST_RUN

[[https://twitter.com/bjorntopel/status/1098563282884014080?s=03][Tweet]] by Björn Töpel (@bjorntopel):

Many people aren't aware of the BPF_PROG_TEST_RUN command. It's really neat
being able to test your XDP programs "offline". The selftests use this a
lot. Docs: https://t.co/GDd7SfNYng and examples in tools/testing/selftests/bpf/.


* TODO Adding AF_XDP support to relevant userspace programs

There are several high-profile userspace programs that might benefit from
AF_XDP support. Adding this (or coordinating it with the program authors)
could be a way to show the benefits of XDP.

* TODO Busy-poll support for AF_XDP

Adding BUSY_POLL support to AF_XDP sockets was presented at the Linux
Plumbers Conference 2018 in Vancouver, BC. With this feature, the NAPI
context of the driver is executed from the process context by the
application calling the poll() syscall, instead of being driven by the
softirq mechanism. This has a number of benefits, for example,
being able to efficiently use a single core for application, driver
and other kernel infra that the application might need. With softirq,
we would need two cores to maintain any performance. Another benefit
is that the cachelines containing the descriptors do not have to
bounce between two caches, since this is now a core local operation as
the driver and the application is on the same core. The drawback is
that we now have to use a syscall (poll) in the data path and this
will slow things down.

There is already a busy_poll mechanisms in the kernel:
/proc/sys/net/core/busy_poll. When writing a non zero value in this
file, the busy poll support will be enabled for ALL sockets in the
system. There are a number of issues with this existing code when
applied to AF_XDP sockets.

 - The batch size is hardcoded to 8, a value that is too small for the
fast processing of XDP.

 - The semantics of poll() in busy_poll mode is that if you provide more
than one file descriptor, it will drive the napi context of the first
one supplied and if it has a packet, then it will NOT drive any of the
other. In other words, it will quit once it has found an fd with a
packet. This will not work for us, since we need all fd's napis to be
called since it is very likely that a packet will be found in each of
them. One could argue that this can be solved in user-space by
manipulating the array of fds supplied to poll() before every singel
call, but this would really complicate multi socket handling in
user-space.

 - The option is global across all sockets. Enough said.

My suggestion for addressing these issues is to introduce a new
busy_poll option that is only for AF_XDP called
XDP_BUSY_POLL_BATCH_SIZE (or something like it). This is a setsockopt
that can be supplied to individual AF_XDP sockets and the batch size
can thus also be set individually by suppling a value > 0. The
semantics of this mode is that both Rx and Tx have to be driven by
calling poll(). There is no guarantee that your packets will arrive or
be sent unless you call poll() (a sendto() will still work for the Tx
case, though, but it is not necessary). In this first patch set, we
can still get interrupts and processing from NAPI in this mode, but we
have some ideas on how to disable this so that NAPI is only driven
from poll(). But that is for a later patch set. Note that the sematics
would not change when we introduce this as we already today say that
you must call poll(), since there is no guarantee otherwise that you
will receive or send packets.

When suppling multiple busy_poll AF_XDP sockets to poll() all of them
will get theire napi contexts executed, so it is guaranteed that all of
them will be driven. It is also possible to mix regular sockets,
global busy_poll sockets and the new AF_XDP sockets in the same poll()
call. The semantics for each type will be maintained, as expected.

From an implementation point of view, I believe this can be
implemented with minimal changes to the net and fs code. We can get
this new behavior by using the standar fd (non-busy poll path) and
then drive the napi from the xsk specific poll callback. We do need to
change one internal interface in order to be able to have a variable
batch size. And Jesper's xdp_rxq_info struct need to be enlarged with a
napi_id field that the drivers need to populate. This can then be used
by the xsk poll code to drive the correct NAPI.

* TODO XDP feature flags

We are probably going to need feature flags for XDP after all. There are use
cases (e.g. Surricata, VM migration) that will want to know what to expect
from the system before committing to loading an XDP program.

** NEXT Propose a driver API to communicate feature flags

** TODO Add a userspace API to query features

* WAIT BTF-based metadata for XDP                                   :WAITING:

Waiting for tracing people to work out the details of BTF.
* WAIT XDP latency jit-vs-no jit, tuning etc                        :WAITING:
[2019-01-18 Fri 13:55]
How do we ensure consistently low latency packet processing is possible with
XDP?

This paper: [[https://www.net.in.tum.de/fileadmin/bibtex/publications/papers/ITC30-Packet-Filtering-eBPF-XDP.pdf][Performance Implications of Packet Filtering with Linux eBPF]]
conclude that turning on the jit *increases* the number of outliers (though
not quite clear if this is actually supported by their data). This should be
investigated.

Maybe write a tuning doc as well?

WAIT status as this is low priority for now.
