# -*- fill-column: 76; -*-
#+Title: Benchmark page_pool on AMD CPU
#+OPTIONS: ^:nil

* Document index (autogenerated)  :toc:
- [[#intel-results][Intel results]]
- [[#amd-results][AMD results]]
  - [[#amd-cpu---devel-kernel-pp01-dma-fix-v7][AMD CPU - devel kernel pp01-DMA-fix-v7]]
  - [[#change-srso-mode-ibpb][Change SRSO mode: IBPB]]
- [[#code][Code]]
  - [[#page_pool-code-changes-made-it-worse][page_pool code changes made it worse]]
  - [[#mitigation-patch-attempt-1][Mitigation patch attempt #1]]

* Intel results

Intel CPU E5-1650 was benchmarked in [[file:page_pool07_bench_DMA_fix.org]].

Table summary for baseline, with bench_page_pool_simple results:

| Test name   | Cycles | Nanosec |
| (tasklet_*) |        |         |
|-------------+--------+---------|
| fast_path   |     19 |   5.399 |
| ptr_ring    |     54 |  15.090 |
| slow        |    238 |  66.134 |

* AMD results

CPU: AMD EPYC 9684X 96-Core Processor (CF Gen 12)

** AMD CPU - devel kernel pp01-DMA-fix-v7

Kernel: 6.13.0-rc6-pp01-DMA-fix-v7+

Compare table: Intel-baseline vs AMD-patched:

| Test name   | Cycles |     |      | Nanosec |         |        |      % |
| (tasklet_*) |  Intel | AMD | diff |   Intel |     AMD |   diff | change |
|-------------+--------+-----+------+---------+---------+--------+--------|
| fast_path   |     19 |  54 |   35 |   5.399 |  21.256 | 15.857 |  293.7 |
| ptr_ring    |     54 | 118 |   64 |  15.090 |  46.668 | 31.578 |  209.3 |
| slow        |    238 | 354 |  116 |  66.134 | 139.400 | 73.266 |  110.8 |
#+TBLFM: $4=$3-$2::$7=$6-$5::$8=(($7/$5)*100);%.1f

Raw data:
#+begin_example
 bench_page_pool_simple: Loaded
 time_bench: Type:for_loop Per elem: 1 cycles(tsc) 0.543 ns (step:0) - (measurement period time:0.054322042 sec time_interval:54322042) - (invoke count:100000000 tsc_interval:138312637)
 time_bench: Type:atomic_inc Per elem: 4 cycles(tsc) 1.732 ns (step:0) - (measurement period time:0.173226385 sec time_interval:173226385) - (invoke count:100000000 tsc_interval:441064065)
 time_bench: Type:lock Per elem: 37 cycles(tsc) 14.659 ns (step:0) - (measurement period time:0.146598674 sec time_interval:146598674) - (invoke count:10000000 tsc_interval:373264716)
 bench_page_pool_simple: time_bench_page_pool01_fast_path(): Cannot use page_pool fast-path
 time_bench: Type:no-softirq-page_pool01 Per elem: 53 cycles(tsc) 21.197 ns (step:0) - (measurement period time:0.211978152 sec time_interval:211978152) - (invoke count:10000000 tsc_interval:539731955)
 bench_page_pool_simple: time_bench_page_pool02_ptr_ring(): Cannot use page_pool fast-path
 time_bench: Type:no-softirq-page_pool02 Per elem: 89 cycles(tsc) 35.316 ns (step:0) - (measurement period time:0.353166180 sec time_interval:353166180) - (invoke count:10000000 tsc_interval:899222004)
 bench_page_pool_simple: time_bench_page_pool03_slow(): Cannot use page_pool fast-path
 time_bench: Type:no-softirq-page_pool03 Per elem: 317 cycles(tsc) 124.732 ns (step:0) - (measurement period time:1.247323944 sec time_interval:1247323944) - (invoke count:10000000 tsc_interval:3175903951)
 bench_page_pool_simple: pp_tasklet_handler(): in_serving_softirq fast-path
 bench_page_pool_simple: time_bench_page_pool01_fast_path(): in_serving_softirq fast-path
 time_bench: Type:tasklet_page_pool01_fast_path Per elem: 54 cycles(tsc) 21.256 ns (step:0) - (measurement period time:0.212565703 sec time_interval:212565703) - (invoke count:10000000 tsc_interval:541227708)
 bench_page_pool_simple: time_bench_page_pool02_ptr_ring(): in_serving_softirq fast-path
 time_bench: Type:tasklet_page_pool02_ptr_ring Per elem: 118 cycles(tsc) 46.668 ns (step:0) - (measurement period time:0.466686939 sec time_interval:466686939) - (invoke count:10000000 tsc_interval:1188265830)
 bench_page_pool_simple: time_bench_page_pool03_slow(): in_serving_softirq fast-path
 time_bench: Type:tasklet_page_pool03_slow Per elem: 354 cycles(tsc) 139.400 ns (step:0) - (measurement period time:1.394000956 sec time_interval:1394000956) - (invoke count:10000000 tsc_interval:3549369301)
#+end_example

** Change SRSO mode: IBPB

Same kernel: 6.13.0-rc6-pp01-DMA-fix-v7+
 - booted with SRSO cmdline: spec_rstack_overflow=ibpb

#+begin_src sh
sudo kexec -l --initrd=/boot/initrd.img-6.13.0-rc6-pp01-DMA-fix-v7+  '--append=root=UUID=ab25bbdd-c315-497d-b12e-ea9ccf76a1e1 ro spec_rstack_overflow=ibpb net.ifnames=0 console=tty0 console=ttyS0,115200n8 '  -- /boot/vmlinuz-6.13.0-rc6-pp01-DMA-fix-v7+
#+end_src

Compare table: Intel-baseline vs AMD-with-SRSO-mode-IBPB:

| Test name   | Cycles |     |      | Nanosec |        |         |      % |
| (tasklet_*) |  Intel | AMD | diff |   Intel |    AMD |    diff | change |
|-------------+--------+-----+------+---------+--------+---------+--------|
| fast_path   |     19 |  14 |   -5 |   5.399 |  5.555 |   0.156 |    2.9 |
| ptr_ring    |     54 |  31 |  -23 |  15.090 | 12.389 |  -2.701 |  -17.9 |
| slow        |    238 |  95 | -143 |  66.134 | 37.607 | -28.527 |  -43.1 |
#+TBLFM: $4=$3-$2::$7=$6-$5::$8=(($7/$5)*100);%.1f

It looks like performance regression is *caused by side-channel mitigation SRSO*
 - https://docs.kernel.org/admin-guide/hw-vuln/srso.html

Raw data:
#+begin_example
[   63.470944] bench_page_pool_simple: Loaded
[   63.529856] time_bench: Type:for_loop Per elem: 1 cycles(tsc) 0.543 ns (step:0) - (measurement period time:0.054322298 sec time_interval:54322298) - (invoke count:100000000 tsc_interval:138312357)
[   63.739017] time_bench: Type:atomic_inc Per elem: 4 cycles(tsc) 1.896 ns (step:0) - (measurement period time:0.189646034 sec time_interval:189646034) - (invoke count:100000000 tsc_interval:482870371)
[   63.818542] time_bench: Type:lock Per elem: 15 cycles(tsc) 5.971 ns (step:0) - (measurement period time:0.059719740 sec time_interval:59719740) - (invoke count:10000000 tsc_interval:152055429)
[   63.837774] bench_page_pool_simple: time_bench_page_pool01_fast_path(): Cannot use page_pool fast-path
[   63.903016] time_bench: Type:no-softirq-page_pool01 Per elem: 13 cycles(tsc) 5.482 ns (step:0) - (measurement period time:0.054820667 sec time_interval:54820667) - (invoke count:10000000 tsc_interval:139581645)
[   63.923912] bench_page_pool_simple: time_bench_page_pool02_ptr_ring(): Cannot use page_pool fast-path
[   64.025630] time_bench: Type:no-softirq-page_pool02 Per elem: 23 cycles(tsc) 9.141 ns (step:0) - (measurement period time:0.091415141 sec time_interval:91415141) - (invoke count:10000000 tsc_interval:232757447)
[   64.046530] bench_page_pool_simple: time_bench_page_pool03_slow(): Cannot use page_pool fast-path
[   64.420369] time_bench: Type:no-softirq-page_pool03 Per elem: 92 cycles(tsc) 36.383 ns (step:0) - (measurement period time:0.363832351 sec time_interval:363832351) - (invoke count:10000000 tsc_interval:926378204)
[   64.441499] bench_page_pool_simple: pp_tasklet_handler(): in_serving_softirq fast-path
[   64.450372] bench_page_pool_simple: time_bench_page_pool01_fast_path(): in_serving_softirq fast-path
[   64.516143] time_bench: Type:tasklet_page_pool01_fast_path Per elem: 14 cycles(tsc) 5.555 ns (step:0) - (measurement period time:0.055554086 sec time_interval:55554086) - (invoke count:10000000 tsc_interval:141449469)
[   64.537713] bench_page_pool_simple: time_bench_page_pool02_ptr_ring(): in_serving_softirq fast-path
[   64.671718] time_bench: Type:tasklet_page_pool02_ptr_ring Per elem: 31 cycles(tsc) 12.389 ns (step:0) - (measurement period time:0.123894439 sec time_interval:123894439) - (invoke count:10000000 tsc_interval:315455068)
[   64.693384] bench_page_pool_simple: time_bench_page_pool03_slow(): in_serving_softirq fast-path
[   65.079181] time_bench: Type:tasklet_page_pool03_slow Per elem: 95 cycles(tsc) 37.607 ns (step:0) - (measurement period time:0.376078904 sec time_interval:376078904) - (invoke count:10000000 tsc_interval:957559603)
#+end_example

* Code

** page_pool code changes made it worse

Some recent page_pool code changes is making this worse.

The (exported) function call =page_pool_alloc_pages()= was changed to call
=page_pool_alloc_netmems()=, which cannot be inlined as it is also exported.
This extra function call cause SRSO overhead for a very small wrapper function,
which is a critical fast-path function call.

See [[https://elixir.bootlin.com/linux/v6.14-rc3/source/net/core/page_pool.c#L580-L603][code section]] from v6.14-rc3 :

#+begin_src C
netmem_ref page_pool_alloc_netmems(struct page_pool *pool, gfp_t gfp)
{
	netmem_ref netmem;

	/* Fast-path: Get a page from cache */
	netmem = __page_pool_get_cached(pool);
	if (netmem)
		return netmem;

	/* Slow-path: cache empty, do real allocation */
	if (static_branch_unlikely(&page_pool_mem_providers) && pool->mp_ops)
		netmem = pool->mp_ops->alloc_netmems(pool, gfp);
	else
		netmem = __page_pool_alloc_pages_slow(pool, gfp);
	return netmem;
}
EXPORT_SYMBOL(page_pool_alloc_netmems);
ALLOW_ERROR_INJECTION(page_pool_alloc_netmems, NULL);

struct page *page_pool_alloc_pages(struct page_pool *pool, gfp_t gfp)
{
	return netmem_to_page(page_pool_alloc_netmems(pool, gfp));
}
EXPORT_SYMBOL(page_pool_alloc_pages);
#+end_src

This was changed in commit:
 - https://git.kernel.org/torvalds/c/4dec64c52e24 ("page_pool: convert to use netmem")

** Mitigation patch attempt #1

Changing code to allow for inlining.

PoC code change:
#+begin_src diff
diff --git a/net/core/page_pool.c b/net/core/page_pool.c
index acef1fcd8ddc..54bdb231c2a1 100644
--- a/net/core/page_pool.c
+++ b/net/core/page_pool.c
@@ -585,7 +585,8 @@ static noinline netmem_ref __page_pool_alloc_pages_slow(struct page_pool *pool,
 /* For using page_pool replace: alloc_pages() API calls, but provide
  * synchronization guarantee for allocation side.
  */
-netmem_ref page_pool_alloc_netmems(struct page_pool *pool, gfp_t gfp)
+static
+netmem_ref __page_pool_alloc_netmems(struct page_pool *pool, gfp_t gfp)
 {
        netmem_ref netmem;
 
@@ -601,12 +602,17 @@ netmem_ref page_pool_alloc_netmems(struct page_pool *pool, gfp_t gfp)
                netmem = __page_pool_alloc_pages_slow(pool, gfp);
        return netmem;
 }
+
+netmem_ref page_pool_alloc_netmems(struct page_pool *pool, gfp_t gfp)
+{
+       return __page_pool_alloc_netmems(pool, gfp);
+}
 EXPORT_SYMBOL(page_pool_alloc_netmems);
 ALLOW_ERROR_INJECTION(page_pool_alloc_netmems, NULL);
 
 struct page *page_pool_alloc_pages(struct page_pool *pool, gfp_t gfp)
 {
-       return netmem_to_page(page_pool_alloc_netmems(pool, gfp));
+       return netmem_to_page(__page_pool_alloc_netmems(pool, gfp));
 }
 EXPORT_SYMBOL(page_pool_alloc_pages);
#+end_src
