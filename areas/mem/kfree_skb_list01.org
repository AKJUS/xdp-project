# -*- fill-column: 76; -*-
#+Title: Use kmem_cache_free_bulk in kfree_skb_list
#+Options: ^:nil

* Intro

Upstream proposal for using kmem_cache_free_bulk in kfree_skb_list.

Patchset V1:
 - Subject: [PATCH net-next 0/2] [[https://lore.kernel.org/all/167293333469.249536.14941306539034136264.stgit@firesoul/#r][net: use kmem_cache_free_bulk in kfree_skb_list]]
 - Message-ID: <167293333469.249536.14941306539034136264.stgit@firesoul>
 - https://lore.kernel.org/all/167293333469.249536.14941306539034136264.stgit@firesoul/

* Cover letter

#+begin_quote
net: use kmem_cache_free_bulk in kfree_skb_list

The kfree_skb_list function walks SKB (via skb->next) and frees them
individually to the SLUB/SLAB allocator (kmem_cache). It is more
efficient to bulk free them via the kmem_cache_free_bulk API.

Netstack NAPI fastpath already uses kmem_cache bulk alloc and free
APIs for SKBs.

The kfree_skb_list call got an interesting optimization in commit
520ac30f4551 ("net_sched: drop packets after root qdisc lock is
released") that can create a list of SKBs "to_free" e.g. when qdisc
enqueue fails or deliberately chooses to drop . It isn't a normal data
fastpath, but the situation will likely occur when system/qdisc are
under heavy workloads, thus it makes sense to use a faster API for
freeing the SKBs.

E.g. the (often distro default) qdisc fq_codel will drop batches of
packets from fattest elephant flow, default capped at 64 packets (but
adjustable via tc argument drop_batch).
#+end_quote

* Expected gain

The expected gain from using kmem_cache bulk alloc and free API can be
assessed via the microbencmark kernel modules in [[https://github.com/netoptimizer/prototype-kernel/tree/master/kernel/mm][prototype kernel]] git repo.

The module 'slab_bulk_test01' results at bulk 16 element:
#+begin_example
kmem-in-loop Per elem: 109 cycles(tsc) 30.532 ns (step:16) - (measurement period time:0.305327778 sec time_interval:305327778) - (invoke count:10000000 tsc_interval:1099193748)
kmem-bulk    Per elem: 64 cycles(tsc) 17.905 ns (step:16) - (measurement period time:0.179053491 sec time_interval:179053491) - (invoke count:10000000 tsc_interval:644598225)
#+end_example

Thus, best-case expected gain is: 45 cycles(tsc) 12.627 ns.
 - With usual microbenchmarks caveats
 - Notice this is both bulk alloc and free

* Qdisc pktgen synthetic benchmark

I have been using pktgen script [[https://github.com/torvalds/linux/blob/master/samples/pktgen/pktgen_bench_xmit_mode_queue_xmit.sh][pktgen_bench_xmit_mode_queue_xmit.sh]]
which can inject packets at the qdisc layer (invoking __dev_queue_xmit).

I'm testing this on a 10Gbit/s interface (driver ixgbe). The challenge is
that I need to overload the qdisc enqueue layer as that is triggering the
call to kfree_skb_list().

Linux with SKBs and qdisc injecting with pktgen is limited to producing
packets at (measured) 2,205,588 pps with a single TX-queue (and scaling up
1,951,771 pps per queue or 512 ns per pkt). Reminder 10Gbit/s at 64 bytes
packets is 14.8 Mpps (or 67.2 ns per pkt).

The trick to trigger the qdisc push-back way earlier is Ethernet
flow-control (which is on by default).

Commands for disable and enable of Ethernet flow-control:
#+begin_src sh
 ethtool -A ixgbe1 rx off tx off
 ethtool -A ixgbe1 rx on tx on
#+end_src

I was a bit surprised to see, but using pktgen_bench_xmit_mode_queue_xmit.sh
on my testlab the remote host was pushing back a lot, resulting in only
256Kpps being actually sent on wire. Monitored with ethtool stats [[https://github.com/netoptimizer/network-testing/blob/master/bin/ethtool_stats.pl][script]].

#+begin_example
ethtool_stats.pl --dev ixgbe1 --sec 3
Show adapter(s) (ixgbe1) statistics (ONLY that changed!)
Ethtool(ixgbe1  ) stat:            1 (              1) <= fdir_miss /sec
Ethtool(ixgbe1  ) stat:           88 (             88) <= rx_bytes /sec
Ethtool(ixgbe1  ) stat:           92 (             92) <= rx_bytes_nic /sec
Ethtool(ixgbe1  ) stat:          601 (            601) <= rx_flow_control_xoff /sec
Ethtool(ixgbe1  ) stat:           55 (             55) <= rx_flow_control_xon /sec
Ethtool(ixgbe1  ) stat:            1 (              1) <= rx_packets /sec
Ethtool(ixgbe1  ) stat:            1 (              1) <= rx_pkts_nic /sec
Ethtool(ixgbe1  ) stat:           88 (             88) <= rx_queue_5_bytes /sec
Ethtool(ixgbe1  ) stat:            1 (              1) <= rx_queue_5_packets /sec
Ethtool(ixgbe1  ) stat:     15414708 (     15,414,708) <= tx_bytes /sec
Ethtool(ixgbe1  ) stat:     16442355 (     16,442,355) <= tx_bytes_nic /sec
Ethtool(ixgbe1  ) stat:       256912 (        256,912) <= tx_packets /sec
Ethtool(ixgbe1  ) stat:       256912 (        256,912) <= tx_pkts_nic /sec
Ethtool(ixgbe1  ) stat:     15414708 (     15,414,708) <= tx_queue_0_bytes /sec
Ethtool(ixgbe1  ) stat:       256912 (        256,912) <= tx_queue_0_packets /sec
Ethtool(ixgbe1  ) stat:         1043 (          1,043) <= tx_restart_queue /sec
#+end_example

