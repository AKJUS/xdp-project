# -*- fill-column: 76; -*-
#+Title: Use kmem_cache_free_bulk in kfree_skb_list
#+Options: ^:nil

* Intro

Upstream proposal for using kmem_cache_free_bulk in kfree_skb_list.

Patchset V1:
 - Subject: [PATCH net-next 0/2] [[https://lore.kernel.org/all/167293333469.249536.14941306539034136264.stgit@firesoul/#r][net: use kmem_cache_free_bulk in kfree_skb_list]]
 - Message-ID: <167293333469.249536.14941306539034136264.stgit@firesoul>
 - https://lore.kernel.org/all/167293333469.249536.14941306539034136264.stgit@firesoul/

Patchset V2:
 - Subject: [PATCH net-next V2 0/2] net: use kmem_cache_free_bulk in kfree_skb_list
 - Message-ID: <167361788585.531803.686364041841425360.stgit@firesoul>
 - https://lore.kernel.org/all/167361788585.531803.686364041841425360.stgit@firesoul

Patches that got applied:
 - [[https://git.kernel.org/netdev/net-next/c/05cb8b39ca59][05cb8b39ca59]] ("Merge branch 'net-use-kmem_cache_free_bulk-in-kfree_skb_list'")
 - [[https://git.kernel.org/netdev/net-next/c/a4650da2a2d6][a4650da2a2d6]] ("net: fix call location in kfree_skb_list_reason")
 - [[https://git.kernel.org/netdev/net-next/c/eedade12f4cb][eedade12f4cb]] ("net: kfree_skb_list use kmem_cache_free_bulk")

Fixes for patches that got applied
 - [[https://git.kernel.org/netdev/net-next/c/f72ff8b81ebc][f72ff8b81ebc]] ("net: fix kfree_skb_list use of skb_mark_not_on_list")

Avoid future bugs in this area
 - [[https://git.kernel.org/netdev/net-next/c/9dde0cd3b10f][9dde0cd3b10f]] ("net: introduce skb_poison_list and use in kfree_skb_list")

* Auto-generated document table of contents                             :toc:
- [[#intro][Intro]]
- [[#cover-letter][Cover letter]]
  - [[#stacked-git-mail][stacked git mail]]
- [[#expected-gain][Expected gain]]
- [[#qdisc-pktgen-synthetic-benchmark][Qdisc pktgen synthetic benchmark]]
  - [[#results1-unpatched][Results#1 unpatched]]
  - [[#results2-patched-kernel][Results#2 patched kernel]]
  - [[#results-summary][Results summary:]]

* Cover letter

#+begin_quote
net: use kmem_cache_free_bulk in kfree_skb_list

The kfree_skb_list function walks SKB (via skb->next) and frees them
individually to the SLUB/SLAB allocator (kmem_cache). It is more
efficient to bulk free them via the kmem_cache_free_bulk API.

Netstack NAPI fastpath already uses kmem_cache bulk alloc and free
APIs for SKBs.

The kfree_skb_list call got an interesting optimization in commit
520ac30f4551 ("net_sched: drop packets after root qdisc lock is
released") that can create a list of SKBs "to_free" e.g. when qdisc
enqueue fails or deliberately chooses to drop . It isn't a normal data
fastpath, but the situation will likely occur when system/qdisc are
under heavy workloads, thus it makes sense to use a faster API for
freeing the SKBs.

E.g. the (often distro default) qdisc fq_codel will drop batches of
packets from fattest elephant flow, default capped at 64 packets (but
adjustable via tc argument drop_batch).

Performance measurements done in [1]:
 [1] https://github.com/xdp-project/xdp-project/blob/master/areas/mem/kfree_skb_list01.org
#+end_quote

** stacked git mail

#+begin_src sh
stg mail --version "net-next V2" --edit-cover --cc meup \
  --cc jakub --cc davem --cc edumazet@google.com --cc pabeni@redhat.com \
  --to netdev \
  refactor..kfree_skb_list-bulk
#+end_src

* Expected gain

The expected gain from using kmem_cache bulk alloc and free API can be
assessed via the microbencmark kernel modules in [[https://github.com/netoptimizer/prototype-kernel/tree/master/kernel/mm][prototype kernel]] git repo.

The module 'slab_bulk_test01' results at bulk 16 element:
#+begin_example
kmem-in-loop Per elem: 109 cycles(tsc) 30.532 ns (step:16) - (measurement period time:0.305327778 sec time_interval:305327778) - (invoke count:10000000 tsc_interval:1099193748)
kmem-bulk    Per elem: 64 cycles(tsc) 17.905 ns (step:16) - (measurement period time:0.179053491 sec time_interval:179053491) - (invoke count:10000000 tsc_interval:644598225)
#+end_example

Thus, best-case expected gain is: 45 cycles(tsc) 12.627 ns.
 - With usual microbenchmarks caveats
 - Notice this is both bulk alloc and free

* Qdisc pktgen synthetic benchmark

I have been using pktgen script [[https://github.com/torvalds/linux/blob/master/samples/pktgen/pktgen_bench_xmit_mode_queue_xmit.sh][pktgen_bench_xmit_mode_queue_xmit.sh]]
which can inject packets at the qdisc layer (invoking __dev_queue_xmit).

I'm testing this on a 10Gbit/s interface (driver ixgbe). The challenge is
that I need to overload the qdisc enqueue layer as that is triggering the
call to kfree_skb_list().

Linux with SKBs and qdisc injecting with pktgen is limited to producing
packets at (measured) 2,205,588 pps with a single TX-queue (and scaling up
1,951,771 pps per queue or 512 ns per pkt). Reminder 10Gbit/s at 64 bytes
packets is 14.8 Mpps (or 67.2 ns per pkt).

The trick to trigger the qdisc push-back way earlier is Ethernet
flow-control (which is on by default).

Commands for disable and enable of Ethernet flow-control:
#+begin_src sh
 ethtool -A ixgbe1 rx off tx off
 ethtool -A ixgbe1 rx on tx on
#+end_src

I was a bit surprised to see, but using pktgen_bench_xmit_mode_queue_xmit.sh
on my testlab the remote host was pushing back a lot, resulting in only
256Kpps being actually sent on wire. Monitored with ethtool stats [[https://github.com/netoptimizer/network-testing/blob/master/bin/ethtool_stats.pl][script]].

#+begin_example
ethtool_stats.pl --dev ixgbe1 --sec 3
Show adapter(s) (ixgbe1) statistics (ONLY that changed!)
Ethtool(ixgbe1  ) stat:            1 (              1) <= fdir_miss /sec
Ethtool(ixgbe1  ) stat:           88 (             88) <= rx_bytes /sec
Ethtool(ixgbe1  ) stat:           92 (             92) <= rx_bytes_nic /sec
Ethtool(ixgbe1  ) stat:          601 (            601) <= rx_flow_control_xoff /sec
Ethtool(ixgbe1  ) stat:           55 (             55) <= rx_flow_control_xon /sec
Ethtool(ixgbe1  ) stat:            1 (              1) <= rx_packets /sec
Ethtool(ixgbe1  ) stat:            1 (              1) <= rx_pkts_nic /sec
Ethtool(ixgbe1  ) stat:           88 (             88) <= rx_queue_5_bytes /sec
Ethtool(ixgbe1  ) stat:            1 (              1) <= rx_queue_5_packets /sec
Ethtool(ixgbe1  ) stat:     15414708 (     15,414,708) <= tx_bytes /sec
Ethtool(ixgbe1  ) stat:     16442355 (     16,442,355) <= tx_bytes_nic /sec
Ethtool(ixgbe1  ) stat:       256912 (        256,912) <= tx_packets /sec
Ethtool(ixgbe1  ) stat:       256912 (        256,912) <= tx_pkts_nic /sec
Ethtool(ixgbe1  ) stat:     15414708 (     15,414,708) <= tx_queue_0_bytes /sec
Ethtool(ixgbe1  ) stat:       256912 (        256,912) <= tx_queue_0_packets /sec
Ethtool(ixgbe1  ) stat:         1043 (          1,043) <= tx_restart_queue /sec
#+end_example

The pktgen script is still generating around 2Mpps, but now most of these
are getting dropped due to "rx_flow_control_xoff".

** Results#1 unpatched

Default pktgen script count is 10 million packets increase this to 100
million for a longer test.

#+begin_example
[jbrouer@broadwell pktgen]$ ./pktgen_bench_xmit_mode_queue_xmit.sh -i ixgbe1 \
 -d 192.168.10.1 -m 00:1b:21:bb:9a:80 -t 1 -n $((100*1000000))
Running... ctrl^C to stop
Done
Device: ixgbe1@0
Result: OK: 41725871(c41679853+d46018) usec, 100000000 (60byte,0frags)
  2396594pps 1150Mb/sec (1150365120bps) errors: 1417469
#+end_example

** Results#2 patched kernel

Default pktgen script count is 10 million packets increase this to 100
million for a longer test.

#+begin_example
[jbrouer@broadwell pktgen]$ ./pktgen_bench_xmit_mode_queue_xmit.sh -i ixgbe1 \
 -d 192.168.10.1 -m 00:1b:21:bb:9a:80 -t 1 -n $((100*1000000))
Running... ctrl^C to stop
Done
Device: ixgbe1@0
Result: OK: 40323062(c40276577+d46485) usec, 100000000 (60byte,0frags)
  2479970pps 1190Mb/sec (1190385600bps) errors: 1422753
#+end_example

** Results summary:

Result line from pktgen script: count 100000000 (60byte,0frags)
 - Unpatched kernel: 2396594pps 1150Mb/sec (1150365120bps) errors: 1417469
 - Patched kernel  : 2479970pps 1190Mb/sec (1190385600bps) errors: 1422753

Difference:
 * +83376 pps faster (2479970-2396594)
 * -14 nanosec faster (1/2479970-1/2396594)*10^9

The patched kernel is faster. Around the expected gain from using the
kmem_cache bulking API.

These results are from a fairly freshly booted kernel, which make it more
likely that the kmem_cache isn't fragmented yet, thus the bulking is more
likely find objects from the same slab.
