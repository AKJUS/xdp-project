# -*- fill-column: 79; -*-
#+Title: Evolving the page_pool API

The page_pool API was accepted into kernel 4.18, and only used by one driver
mlx5.  This doc is about evolving the page_pool API.

* The page_pool is NOT finished

Lets be clear upfront: *The page_pool API is not finished*.

It is not ready to be used by all drivers, in the current state.
The mlx5 driver is _also_ not using the API optimally.

We both need to evolve the API, but also need to take some fundamental design
choices for the direction of the page_pool API.

* History

The page_pool was adapted from my proposals at MM-summit 2016+2017:

 http://people.netfilter.org/hawk/presentations/MM-summit2016/generic_page_pool_mm_summit2016.pdf
 http://people.netfilter.org/hawk/presentations/MM-summit2017/MM-summit2017-JesperBrouer.pdf

Jesper posted some RFC page_pool patches around that time.  That code was
adapted and is now part of the network stack, under =net/core/page_pool.c= and
=include/net/page_pool.h=.

The original code was designed to be part of kernel the MM-layer.  It hooked
into the =put_page()= call, and also used parts of struct-page for storing info
that allows for returning the page back to the originating device (to allow
keeping the page DMA mapped for the device).

Basic idea:
 - Pages stays DMA mapped as long as they are in the pool.
 - Driver use DMA-sync API to control when data is writable by CPU.
 - No need to keep track of pages traveling system, they are returned by
   =put_page= API.

* The XDP dream scenario

The XDP dream scenario is about returning frame-pages, with no refcnt changes
to the struct-page.  The optimal XDP case is 1-page per frame/packet with page
refcnt==1, and a put_page() call that catch refcnt==1, and recycle the page for
reuse.

This is very hard to acheive, as (see later) some kind of sync operation is
needed when recycling pages, back to originating device, but that can be
amortized through bulking.

When no XDP program is configured on the device, it might be better for memory
usage to split the page into two packets, and use a refcnt based model, as the
overhead of other part of network stack might not dwarf the benefit of this
memory vs speed optimization.


* Status today: lost reliable return hook

The big difference today is the page_pool is not hooked into =put_page= and the
MM-layer.  This means we *lost the reliable return hook point*, which made it
easy to keep the page DMA mapped.

When XDP redirecting (XDP_REDIRECT) as an xdp_frame, the XDP return API gives
us a reliable return hook. (We don't do it today, but it should be fairly easy
to keep the frame DMA mapped for redirect+return path).

When the frame travel as an SKB into the network stack, then we lost the
reliable return hook, as the netstack returns the pages based refcnt.


* Regaining the return hook

What are the options for reestablishing a return hook point?  There are 3
options, two options for creating a return hook or instead keeping track of
outstanding frames traveling the system.

The two return hooks are:
 - Page alloactor layer via put_page.
 - Network stack layer via __kfree_skb().

The alternative is to keep track of outstanding frames at the driver layer via
elevating refcnt.

** Return-hook: put_page

The original idea was to modify put_page() to catch pages with refcnt==1 and
recycle those.  This was rejected upstream, but meanwhile a hook have been
created at the exact spot we need.

The function calls used is called: put_devmap_managed_page().

It is used by HMM (Heterogeneous Memory Management), which is used by device
memory like GPU on board memory.  The DAX system also leverage this via type
MEMORY_DEVICE_FS_DAX.

The question is
 (1) can page_pool also leverage this,
 (2) is the performance good enough.

TODO investigate: The page "zonenum" must be ZONE_DEVICE, which semantic is
unclear, more info needed.  Can this type of page be used for "normal" network
stack delivery?

The code that endup being called is: __put_devmap_managed_page(page); The
callback in __put_devmap_managed_page(), is implemented by calling:
page->pgmap->page_free(page, page->pgmap->data);

From struct-page the part containing this area looks like:

#+BEGIN_SRC C
	struct {	/* ZONE_DEVICE pages */
		/** @pgmap: Points to the hosting device page map. */
		struct dev_pagemap *pgmap;
		unsigned long hmm_data;
		unsigned long _zd_pad_1;	/* uses mapping */
	};
#+END_SRC

TODO: Read =include/linux/memremap.h= and figure out, (a) that struct
dev_pagemap is used for, and (b) what it means and what requirements are
associated with being a MEMORY_DEVICE_PRIVATE type using ZONE_DEVICE.


** Return-hook: via SKB

** Keep track of pages





* Understanding page_pool details

Some details about the page_pool API that might not be obvious.

** Extremely fast alloc page

The page_pool leverage the knowledge/requirement, that allocations MUST happen
from NAPI context. (During driver init of RX ring, not in NAPI context, it is
known that no concurrent users of this page_pool exist, thus it is still safe).

A NIC driver creates a page_pool per RX-queue.  Combined with the protection
provide by NAPI context (per RX-queue), allow page_pool to get pages from a
completely unlocked array-style stack-queue (see struct pp_alloc_cache).  It is
difficult to get any faster than this.

Code from: __page_pool_get_cached()

#+BEGIN_SRC C
	if (likely(pool->alloc.count)) {
		/* Fast-path */
		page = pool->alloc.cache[--pool->alloc.count];
		return page;
	}
#+END_SRC

** Extremely fast recycle direct

Another optimization leveraged by page_pool is that, for frames that needs to
be dropped while still running under the RX NAPI context, either for error
cases or explicit drop due to XDP_DROP action.  The API call
page_pool_recycle_direct() can be used, which as described above, simply
returns the page to an array stack (code in __page_pool_recycle_direct()).

One advantage is that even with XDP_REDIRECT, the redirect core-code can choose
to drop frames and see almost the same drop performance as driver level code
(via calling xdp_return_frame_rx_napi).


* Warning: missing pieces in page_pool

** Currently: fully-locked page-recycle call

When the RFC page_pool got ripped out and converted it to be used in network
stack, the ALF (Array-based Lock Free) queue was dropped.  Instead the ptr_ring
was used and replaced the internal page_pool queue. The ptr_ring actually do
have some performance advantages over ALF-queue, e.g.  reduces the cross-CPU
cache-coherency talk, and is faster cross CPU.

One disadvantage is that ptr_ring_produce (or ptr_ring_produce_bh) call takes a
lock.  And it is currently called per returned page, see
__page_pool_recycle_into_ring(). This obviously is a scalability issue waiting
to happen, when/if multiple CPUs want to return packet originating from the
same RX-queue.

Thus, this need to be fixed/improved. The basic idea to address this is through
bulking.  But there are two ways to introduce (1) expose an explicit bulk
return API, or (2) hide it in the page_pool API via clever lockless per CPU
store (that return pages in a bulk).

Jesper have a lot of details for option (2), as a significant performance gain
can be acheived by having knowledge about (and separating) what context the
kernel is running in (softirq/bh, hardirq, process-context).



* Notes

How do we evolve the page_pool API?

What does the explict return (point) API give us?

Remember: the keep DMA mapped, is also a feature that needs to be leveraged
used by the XDP APIs.  Right now, the ndo_xdp_xmit does TX DMA map and DMA
unmap in completion.
