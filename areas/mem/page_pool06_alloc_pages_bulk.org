# -*- fill-column: 76; -*-
#+Title: Testing page-allocator API proposal for alloc_pages_bulk
#+OPTIONS: ^:nil

Testing experimental patch from Mel Gorman that add alloc_pages_bulk() API.

* Document index (autogenerated)  :toc:
- [[#use-case-page_pool][Use-case: page_pool]]
  - [[#test-setup-for-veth][Test setup for veth]]
- [[#baseline-test][Baseline test]]
  - [[#baseline01-veth-redirect][baseline01: veth redirect]]
- [[#using-alloc_pages_bulk][Using alloc_pages_bulk]]
  - [[#test01-veth-redirect-page_pool-bulk-16][test01: veth redirect (page_pool bulk 16)]]
  - [[#test02-veth-redirect-page_pool-bulk-64][test02: veth redirect (page_pool bulk 64)]]
- [[#test-with-rfc-patchset][Test with RFC patchset]]
  - [[#initial-test][initial test]]
- [[#patch-notes][patch notes]]
  - [[#follow-up-to-mels-patchset][Follow up to Mel's patchset]]
  - [[#bench-test-veth][bench test veth]]
  - [[#desc-net-page_pool-use-alloc_pages_bulk-in-refill-code-path][desc: net: page_pool: use alloc_pages_bulk in refill code path]]
- [[#test-on-mel-git-tree-mm-bulk-rebase-v4r2][Test on Mel git-tree: mm-bulk-rebase-v4r2]]
  - [[#stg-mail][stg mail]]
  - [[#updated-patch][Updated patch]]
  - [[#pop-patch-using-__alloc_pages_bulk][Pop patch using __alloc_pages_bulk]]
- [[#test-on-mel-git-tree-mm-bulk-rebase-v5r9][Test on Mel git-tree: mm-bulk-rebase-v5r9]]
  - [[#micro-benchmark-page_bench04_bulk][micro-benchmark: page_bench04_bulk]]
  - [[#adjust-inline-__rmqueue_pcplist][Adjust: inline __rmqueue_pcplist]]
  - [[#page_pool-use-alloc_pages_bulk_list][page_pool: use alloc_pages_bulk_list]]
  - [[#page_pool-use-alloc_pages_bulk_array][page_pool: use alloc_pages_bulk_array]]
  - [[#i-cache-layout-for-__alloc_pages_bulk][I-cache layout for __alloc_pages_bulk]]
- [[#test-on-mel-git-tree-mm-bulk-rebase-v6r5][Test on Mel git-tree: mm-bulk-rebase-v6r5]]
  - [[#baseline-kernel][baseline kernel]]
  - [[#use-list-variant][Use list variant]]
- [[#micro-optimisations][Micro optimisations]]
  - [[#observations][Observations]]
  - [[#why-happening][Why happening]]
  - [[#explaining-patch-with-fix][Explaining patch with fix]]

* Use-case: page_pool

The =alloc_pages_bulk()= use-case for page_pool is in
=__page_pool_alloc_pages_slow()=, for then the pool goes empty.

The mlx5 driver can easily empty the page_pool "cache", when redirected
xdp_frame's are converted into SKBs.

This happens both for veth and cpumap, as SKBs (created from xdp_frame) is
missing callback to page_pool.

** Test setup for veth

It requires some setup and two XDP programs to trigger the case, where
page_pool "cache" goes empty. And use of driver mlx5.

First: Create veth pair and enabled link up:

#+begin_src
ip link add veth1 type veth peer name veth2
ip link set veth1 up
ip link set veth2 up
#+end_src

Disable GRO/GSO/TSO on the veth devices
#+begin_example
ethtool -K veth1 gso off gro off tso off
ethtool -K veth2 gso off gro off tso off
#+end_example

When XDP-redirecting into a veth, we must remember to attach an XDP prog to
the peer device.

Redirect frame from mlx5p1 into veth1 (peer veth2)
- sudo ./xdp_redirect_map mlx5p1 veth1

Create SKBs from xdp_frame via XDP_PASS on veth2:
- sudo ./xdp_rxq_info --dev veth2 --act XDP_PASS

As the MAC-addr doesn't match the SKB packets are dropped very early, but it
suites our micro-benchmark test case.

* Baseline test

** baseline01: veth redirect

Kernel: Linux broadwell 5.11.0-net-next

#+begin_example
Running XDP on dev:veth2 (ifindex:12) action:XDP_PASS options:no_touch
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      0       3,677,958   0          
XDP-RX CPU      total   3,677,958  

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    0:0   3,677,966   0          
rx_queue_index    0:sum 3,677,966  

#+end_example

#+begin_example
Samples: 81K of event 'cycles', Event count (approx.): 73929158590
  Overhead  Symbol                                               IPC   [IPC Coverage]
+    4,87%  [k] __netif_receive_skb_core                         -      -
+    3,50%  [k] kmem_cache_free                                  -      -
+    3,26%  [k] dev_gro_receive                                  -      -
+    3,24%  [k] rmqueue                                          -      -
+    3,15%  [k] bpf_prog_943df0a1ce7ea5c2_xdp_prognum0           -      -
+    3,09%  [k] __xdp_release_frame                              -      -
+    2,99%  [k] memset_erms                                      -      -
+    2,89%  [k] get_page_from_freelist                           -      -
+    2,82%  [k] __alloc_pages_nodemask                           -      -
+    2,61%  [k] veth_xdp_rcv_one                                 -      -
+    2,54%  [k] free_unref_page_commit                           -      -
+    2,35%  [k] __list_del_entry_valid                           -      -
+    2,29%  [k] __netif_receive_skb_list_core                    -      -
+    2,13%  [k] dev_map_enqueue                                  -      -
+    2,07%  [k] mlx5e_xdp_handle                                 -      -
+    2,04%  [k] __xdp_build_skb_from_frame                       -      -
+    1,95%  [k] napi_gro_receive                                 -      -
+    1,94%  [k] xdp_do_redirect                                  -      -
+    1,85%  [k] netif_receive_skb_list_internal                  -      -
+    1,85%  [k] mlx5e_skb_from_cqe_mpwrq_linear                  -      -
+    1,84%  [k] mlx5e_handle_rx_cqe_mpwrq                        -      -
+    1,82%  [k] bpf_xdp_redirect_map                             -      -
+    1,74%  [k] kmem_cache_alloc_bulk                            -      -
+    1,71%  [k] ip_list_rcv                                      -      -
+    1,69%  [k] bpf_prog_a55118bafe28d557_xdp_redirect_map_prog  -      -
+    1,56%  [k] veth_xdp_rcv                                     -      -
+    1,48%  [k] __slab_free                                      -      -
+    1,45%  [k] free_unref_page_prepare.part.0                   -      -
+    1,43%  [k] eth_type_trans                                   -      -
+    1,40%  [k] dma_map_page_attrs                               -      -
+    1,30%  [k] mlx5e_poll_rx_cq                                 -      -
+    1,21%  [k] __list_add_valid                                 -      -
+    1,19%  [k] veth_xdp_xmit                                    -      -
+    1,11%  [k] free_unref_page                                  -      -
+    1,07%  [k] mlx5e_alloc_rx_mpwqe                             -      -
+    1,05%  [k] dma_unmap_page_attrs                             -      -
+    1,03%  [k] page_pool_release_page                           -      -
+    1,00%  [k] gro_normal_one                                   -      -
+    0,99%  [k] build_skb_around                                 -      -
+    0,94%  [k] __page_pool_alloc_pages_slow                     -      -
+    0,92%  [k] ip_rcv_core.isra.0                               -      -
+    0,90%  [k] prep_new_page                                    -      -
+    0,89%  [k] __build_skb_around                               -      -
+    0,81%  [k] free_pcp_prepare                                 -      -
#+end_example


* Using alloc_pages_bulk

Kernel: Linux broadwell 5.11.0-net-next-alloc_pages_bulk+
- With this fix [[https://patchwork.kernel.org/project/netdevbpf/patch/161402344429.1980160.4798557236979159924.stgit@firesoul/][to mlx5 driver]]

This patch: https://lore.kernel.org/linux-mm/20210210130705.GC3629@suse.de/
With this fix: https://lore.kernel.org/linux-mm/20210211091235.GC3697@techsingularity.net/

** test01: veth redirect (page_pool bulk 16)

Same veth setup as above: [[#test-setup-for-veth][setup]]

Results below with page_pool using bulk=16 for alloc_pages_bulk().

#+begin_example
Running XDP on dev:veth2 (ifindex:12) action:XDP_PASS options:no_touch
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      5       4,066,028   0          
XDP-RX CPU      total   4,066,028  

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    0:5   4,066,023   0          
rx_queue_index    0:sum 4,066,023  
#+end_example

#+begin_example
Samples: 51K of event 'cycles', Event count (approx.): 46934149161
  Overhead  Symbol                                               IPC   [IPC Coverage]
+    4,94%  [k] __netif_receive_skb_core                         -      -
+    4,02%  [k] kmem_cache_free                                  -      -
+    3,78%  [k] bpf_prog_943df0a1ce7ea5c2_xdp_prognum0           -      -
+    3,30%  [k] memset_erms                                      -      -
+    3,26%  [k] __xdp_release_frame                              -      -
+    3,23%  [k] dev_gro_receive                                  -      -
+    3,10%  [k] free_unref_page_commit                           -      -
+    3,06%  [k] veth_xdp_rcv_one                                 -      -
+    2,82%  [k] __list_del_entry_valid                           -      -
+    2,42%  [k] bpf_xdp_redirect_map                             -      -
+    2,40%  [k] __netif_receive_skb_list_core                    -      -
+    2,40%  [k] napi_gro_receive                                 -      -
+    2,21%  [k] __xdp_build_skb_from_frame                       -      -
+    2,16%  [k] mlx5e_skb_from_cqe_mpwrq_linear                  -      -
+    2,07%  [k] dev_map_enqueue                                  -      -
+    2,06%  [k] mlx5e_handle_rx_cqe_mpwrq                        -      -
+    2,01%  [k] ip_list_rcv                                      -      -
+    1,94%  [k] netif_receive_skb_list_internal                  -      -
+    1,92%  [k] kmem_cache_alloc_bulk                            -      -
+    1,91%  [k] bpf_prog_a55118bafe28d557_xdp_redirect_map_prog  -      -
+    1,90%  [k] veth_xdp_rcv                                     -      -
+    1,74%  [k] __list_add_valid                                 -      -
+    1,73%  [k] xdp_do_redirect                                  -      -
+    1,70%  [k] mlx5e_xdp_handle                                 -      -
+    1,63%  [k] free_unref_page_prepare.part.0                   -      -
+    1,61%  [k] dma_map_page_attrs                               -      -
+    1,53%  [k] __alloc_pages_bulk_nodemask                      -      -
+    1,43%  [k] __slab_free                                      -      -
+    1,42%  [k] mlx5e_poll_rx_cq                                 -      -
+    1,36%  [k] eth_type_trans                                   -      -
+    1,32%  [k] __page_pool_alloc_pages_slow                     -      -
+    1,22%  [k] free_unref_page                                  -      -
+    1,18%  [k] mlx5e_alloc_rx_mpwqe                             -      -
+    1,16%  [k] veth_xdp_xmit                                    -      -
+    1,09%  [k] build_skb_around                                 -      -
+    1,03%  [k] gro_normal_one                                   -      -
+    1,02%  [k] page_pool_release_page                           -      -
+    1,00%  [k] ip_rcv_core.isra.0                               -      -
+    1,00%  [k] dma_unmap_page_attrs                             -      -
+    0,99%  [k] __rmqueue_pcplist                                -      -
+    0,95%  [k] free_pcp_prepare                                 -      -
+    0,91%  [k] __build_skb_around                               -      -
     0,84%  [k] kfree_skb                                        -      -
#+end_example

** test02: veth redirect (page_pool bulk 64)

Same veth setup as above: [[#test-setup-for-veth][setup]]

Results below with page_pool using bulk=64 for alloc_pages_bulk().

#+begin_example
Running XDP on dev:veth2 (ifindex:12) action:XDP_PASS options:no_touch
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      1       4,297,656   0          
XDP-RX CPU      total   4,297,656  

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    0:1   4,297,649   0          
rx_queue_index    0:sum 4,297,649  
#+end_example

* Test with RFC patchset

Test with new patchset from Mel
 - Message-Id: <20210224102603.19524-1-mgorman@techsingularity.net>
 - https://lore.kernel.org/netdev/20210224102603.19524-1-mgorman@techsingularity.net/

** initial test

bulk=64

#+begin_example
Running XDP on dev:veth2 (ifindex:12) action:XDP_PASS options:no_touch
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      3       4,368,926   0          
XDP-RX CPU      total   4,368,926  

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    0:3   4,368,917   0          
rx_queue_index    0:sum 4,368,917  
#+end_example

* patch notes

** Follow up to Mel's patchset

Patchset V1:
#+begin_example
stg mail --version='RFC net-next' --edit-cover --cc meup \
 --to mel --cc chuck.lever@oracle.com \
 --to mm --cc netdev --cc linux-nfs@vger.kernel.org --cc lkml \
 --in-reply-to='20210224102603.19524-1-mgorman@techsingularity.net' \
 03-reorder-add-page_pool_dma_map..mm-make-zone-free_area-order
#+end_example
Message-ID: <161419296941.2718959.12575257358107256094.stgit@firesoul>

V2 with minor changes and dropping micro-optimisation:
#+begin_example
stg mail --version='RFC V2 net-next' --edit-cover --cc meup \
 --to mel --cc chuck.lever@oracle.com \
 --to mm --cc netdev --cc linux-nfs@vger.kernel.org --cc lkml \
 --in-reply-to='20210224102603.19524-1-mgorman@techsingularity.net' \
 05-03-reorder-add-page_pool_dma_map..06-04-page_pool-use-alloc_pages_bulk
#+end_example
Message-ID: <161460522573.3031322.15721946341157092594.stgit@firesoul>

#+begin_quote
Use bulk order-0 page allocator API for page_pool

This is a followup to Mel Gorman's patchset:
 - Message-Id: <20210224102603.19524-1-mgorman@techsingularity.net>
 - https://lore.kernel.org/netdev/20210224102603.19524-1-mgorman@techsingularity.net/

Showing page_pool usage of the API for alloc_pages_bulk().

Maybe Mel Gorman will/can carry these patches?
(to keep it together with the alloc_pages_bulk API)
#+end_quote

** bench test veth

Test again:
#+begin_example
Running XDP on dev:veth2 (ifindex:12) action:XDP_PASS options:no_touch
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      0       4,302,291   0          
XDP-RX CPU      total   4,302,291  

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    0:0   4,302,285   0          
rx_queue_index    0:sum 4,302,285  
#+end_example

** desc: net: page_pool: use alloc_pages_bulk in refill code path

#+begin_quote
There are cases where the page_pool need to refill with pages from the
page allocator. Some workloads cause the page_pool to release pages
instead of recycling these pages.

For these workload it can improve performance to bulk alloc pages from
the page-allocator to refill the alloc cache.

For XDP-redirect workload with 100G mlx5 driver (that use page_pool)
redirecting xdp_frame packets into a veth, that does XDP_PASS to create
an SKB from the xdp_frame, which then cannot return the page to the
page_pool. In this case, we saw[1] an improvement of 18.8% from using
the alloc_pages_bulk API (3,677,958 pps -> 4,368,926 pps).

[1] https://github.com/xdp-project/xdp-project/blob/master/areas/mem/page_pool06_alloc_pages_bulk.org
#+end_quote


* Test on Mel git-tree: mm-bulk-rebase-v4r2

Tests based on Mel Gorman's git tree:
 - git://git.kernel.org/pub/scm/linux/kernel/git/mel/linux.git

Branch: mm-bulk-rebase-v4r2
 - Changed the last patch with page_pool changes

** stg mail

Promised to followup in Message-ID: <20210315094038.22d6d79a@carbon>
- Below stg [[https://lore.kernel.org/netdev/161583677541.3715498.6118778324185171839.stgit@firesoul/][Message-ID]]

#+begin_example
stg mail --version='mel-git' --edit-cover --cc meup \
 --to mel --cc chuck.lever@oracle.com --cc alex \
 --to mm --cc netdev --cc linux-nfs@vger.kernel.org --cc lkml \
 --in-reply-to='20210315094038.22d6d79a@carbon' \
 net-page_pool-use
#+end_example

#+begin_quote
Subj: Followup: Update [PATCH 7/7] in Mel's series

This patch is against Mel's git-tree:
 git://git.kernel.org/pub/scm/linux/kernel/git/mel/linux.git

Using branch: mm-bulk-rebase-v4r2 but replacing the last patch related to
the page_pool using __alloc_pages_bulk().

 https://git.kernel.org/pub/scm/linux/kernel/git/mel/linux.git/log/?h=mm-bulk-rebase-v4r2

While implementing suggestions by Alexander Duyck, I realised that I could
simplify the code further, and simply take the last page from the
pool->alloc.cache given this avoids special casing the last page.

I re-ran performance tests and the improvement have been reduced to 13% from
18% before, but I don't think the rewrite of the specific patch have
anything to do with this.

Notes on tests:
 https://github.com/xdp-project/xdp-project/blob/master/areas/mem/page_pool06_alloc_pages_bulk.org#test-on-mel-git-tree
#+end_quote

Performance summary: +13% faster
 - (3,810,013 pps -> 4,308,208 pps)
 - ((4308208/3810013)-1)*100 = 13.07%

Previous: 18.8% (3,677,958 pps -> 4,368,926 pps).
 - Thus, slower than before.
 - Mostly look like better baseline

** Updated patch

Alexander Duyck point out there was a cleaner way to implement
changes in function =__page_pool_alloc_pages_slow()=.

#+begin_example
Running XDP on dev:veth2 (ifindex:12) action:XDP_PASS options:no_touch
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      3       4,308,208   0          
XDP-RX CPU      total   4,308,208  

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    0:3   4,308,208   0          
rx_queue_index    0:sum 4,308,208  
#+end_example

** Pop patch using __alloc_pages_bulk

#+begin_example
Running XDP on dev:veth2 (ifindex:12) action:XDP_PASS options:no_touch
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      1       3,810,013   0          
XDP-RX CPU      total   3,810,013  

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    0:1   3,810,013   0          
rx_queue_index    0:sum 3,810,013  
#+end_example

* Test on Mel git-tree: mm-bulk-rebase-v5r9

Tests based on Mel Gorman's git tree:
 - git://git.kernel.org/pub/scm/linux/kernel/git/mel/linux.git
 - Branch: mm-bulk-rebase-v5r9

** micro-benchmark: page_bench04_bulk

Notice these "per elem" measurements are alloc+free cost for order-0 pages

page_bench04_bulk micro-benchmark on branch: mm-bulk-rebase-v5r9
 - https://lore.kernel.org/netdev/20210322130446.0a505db0@carbon/

CPU: Intel(R) Xeon(R) CPU E5-1650 v4 @ 3.60GHz

#+begin_example
BASELINE
 single_page alloc+put: Per elem: 199 cycles(tsc) 55.472 ns

LIST variant: time_bulk_page_alloc_free_list: step=bulk size

 Per elem: 206 cycles(tsc) 57.478 ns (step:1)
 Per elem: 154 cycles(tsc) 42.861 ns (step:2)
 Per elem: 145 cycles(tsc) 40.536 ns (step:3)
 Per elem: 142 cycles(tsc) 39.477 ns (step:4)
 Per elem: 142 cycles(tsc) 39.610 ns (step:8)
 Per elem: 137 cycles(tsc) 38.155 ns (step:16)
 Per elem: 135 cycles(tsc) 37.739 ns (step:32)
 Per elem: 134 cycles(tsc) 37.282 ns (step:64)
 Per elem: 133 cycles(tsc) 36.993 ns (step:128)

ARRAY variant: time_bulk_page_alloc_free_array: step=bulk size

 Per elem: 202 cycles(tsc) 56.383 ns (step:1)
 Per elem: 144 cycles(tsc) 40.047 ns (step:2)
 Per elem: 134 cycles(tsc) 37.339 ns (step:3)
 Per elem: 128 cycles(tsc) 35.578 ns (step:4)
 Per elem: 120 cycles(tsc) 33.592 ns (step:8)
 Per elem: 116 cycles(tsc) 32.362 ns (step:16)
 Per elem: 113 cycles(tsc) 31.476 ns (step:32)
 Per elem: 110 cycles(tsc) 30.633 ns (step:64)
 Per elem: 110 cycles(tsc) 30.596 ns (step:128)
#+end_example


** Adjust: inline __rmqueue_pcplist

When __alloc_pages_bulk() got introduced two callers of
__rmqueue_pcplist exist and the compiler chooses to not inline
this function.

#+begin_src sh
     ./scripts/bloat-o-meter vmlinux-before vmlinux-inline__rmqueue_pcplist
    add/remove: 0/1 grow/shrink: 2/0 up/down: 164/-125 (39)
    Function                                     old     new   delta
    rmqueue                                     2197    2296     +99
    __alloc_pages_bulk                          1921    1986     +65
    __rmqueue_pcplist                            125       -    -125
    Total: Before=19374127, After=19374166, chg +0.00%
#+end_src

modprobe page_bench04_bulk loops=$((10**7))

Type:time_bulk_page_alloc_free_array
 -  Per elem: 106 cycles(tsc) 29.595 ns (step:64)
 - (measurement period time:0.295955434 sec time_interval:295955434)
 - (invoke count:10000000 tsc_interval:1065447105)

Before:
 - Per elem: 110 cycles(tsc) 30.633 ns (step:64)

#+begin_src diff
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index 2cbb8da811ab..f60f51a97a7b 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -3415,7 +3415,8 @@ static inline void zone_statistics(struct zone *preferred_zone, struct zone *z)
 }
 
 /* Remove page from the per-cpu list, caller must protect the list */
-static struct page *__rmqueue_pcplist(struct zone *zone, int migratetype,
+static inline
+struct page *__rmqueue_pcplist(struct zone *zone, int migratetype,
                        unsigned int alloc_flags,
                        struct per_cpu_pages *pcp,
                        struct list_head *list)
#+end_src

Below tests include above patch.

** page_pool: use alloc_pages_bulk_list

#+begin_example
unning XDP on dev:veth2 (ifindex:12) action:XDP_PASS options:no_touch
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      4       3,961,569   0          
XDP-RX CPU      total   3,961,569  

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    0:4   3,961,569   0          
rx_queue_index    0:sum 3,961,569  
#+end_example

** page_pool: use alloc_pages_bulk_array

#+begin_example
Running XDP on dev:veth2 (ifindex:12) action:XDP_PASS options:no_touch
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      5       4,067,120   0          
XDP-RX CPU      total   4,067,120  

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    0:5   4,067,127   0          
rx_queue_index    0:sum 4,067,127  
#+end_example

The results a not super stable, as after a while I get this result:
#+begin_example
Running XDP on dev:veth2 (ifindex:12) action:XDP_PASS options:no_touch
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      3       4,218,956   0          
XDP-RX CPU      total   4,218,956  

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    0:3   4,218,960   0          
rx_queue_index    0:sum 4,218,960  
#+end_example

** I-cache layout for __alloc_pages_bulk

Looking at perf-report and ASM-code for __alloc_pages_bulk() then the code
activated is suboptimal. The compiler guess wrong and place unlikely code in
the beginning. Due to the use of WARN_ON_ONCE() macro the =UD2= asm
instruction is added to the code, which confuse the I-cache prefetcher in
the CPU.

Perf-stat *BEFORE* during 4,174,649 pps:
#+begin_example
$ perf stat -C3 -e cycles -e  instructions -e cache-references -e cache-misses -e branches:k -e branch-misses:k -e l2_rqsts.all_code_rd -e l2_rqsts.code_rd_hit -e l2_rqsts.code_rd_miss -e L1-icache-load-misses -e icache.hit -e icache.misses -e icache.ifdata_stall -r 4 sleep 1

 Performance counter stats for 'CPU(s) 3' (4 runs):

     3.967.401.581      cycles                                                        ( +-  0,02% )  (69,23%)
     9.328.404.288      instructions              #    2,35  insn per cycle           ( +-  0,04% )  (76,92%)
        40.081.612      cache-references                                              ( +-  0,06% )  (76,92%)
             1.925      cache-misses              #    0,005 % of all cache refs      ( +- 85,44% )  (76,92%)
     1.772.491.245      branches:k                                                    ( +-  0,03% )  (76,92%)
         3.897.378      branch-misses:k           #    0,22% of all branches          ( +-  0,31% )  (76,92%)
         4.909.219      l2_rqsts.all_code_rd                                          ( +-  0,32% )  (76,92%)
         4.285.616      l2_rqsts.code_rd_hit                                          ( +-  0,30% )  (76,92%)
           620.169      l2_rqsts.code_rd_miss                                         ( +-  0,38% )  (76,92%)
         1.633.584      L1-icache-load-misses                                         ( +-  0,83% )  (76,92%)
       920.823.524      icache.hit                                                    ( +-  0,03% )  (61,55%)
         1.635.497      icache.misses                                                 ( +-  0,92% )  (61,55%)
        15.893.532      icache.ifdata_stall                                           ( +-  1,38% )  (61,55%)
#+end_example

Above cycles 3.97 GHz indicate turbo-mode was engaged.

Perf-stat *AFTER* during 4,284,779 pps:
#+begin_example
$ perf stat -C4 -e cycles -e  instructions -e cache-references -e cache-misses -e branches:k -e branch-misses:k -e l2_rqsts.all_code_rd -e l2_rqsts.code_rd_hit -e l2_rqsts.code_rd_miss -e L1-icache-load-misses -e icache.hit -e icache.misses -e icache.ifdata_stall -r 4 sleep 1

 Performance counter stats for 'CPU(s) 4' (4 runs):

     3.780.344.586      cycles                                                        ( +-  0,00% )  (69,23%)
     9.208.083.065      instructions              #    2,44  insn per cycle           ( +-  0,01% )  (76,92%)
        41.010.130      cache-references                                              ( +-  0,12% )  (76,92%)
             2.063      cache-misses              #    0,005 % of all cache refs      ( +- 68,85% )  (76,92%)
     1.770.974.127      branches:k                                                    ( +-  0,01% )  (76,92%)
         3.378.947      branch-misses:k           #    0,19% of all branches          ( +-  0,10% )  (76,92%)
         4.002.071      l2_rqsts.all_code_rd                                          ( +-  0,39% )  (76,92%)
         3.596.114      l2_rqsts.code_rd_hit                                          ( +-  0,58% )  (76,92%)
           406.726      l2_rqsts.code_rd_miss                                         ( +-  2,86% )  (76,92%)
         1.315.880      L1-icache-load-misses                                         ( +-  0,55% )  (76,92%)
       860.746.134      icache.hit                                                    ( +-  0,03% )  (61,55%)
         1.315.046      icache.misses                                                 ( +-  0,52% )  (61,55%)
         9.666.533      icache.ifdata_stall                                           ( +-  0,72% )  (61,55%)
#+end_example

When comparing these perf stats then it is important to realise that
workload performance was increased +110,130 pps (4174649-4284779). Thus,
take that into account as counts can be higher due to factor.

Notice turbo-mode didn't kick in above 3.78GHz. But new measurement below it
did get "turbo-mode" enabled.

Perf-stat *AFTER* during 4,263,396 pps:
#+begin_example
 perf stat -C3 -e cycles -e  instructions -e cache-references -e cache-misses -e branches:k -e branch-misses:k -e l2_rqsts.all_code_rd -e l2_rqsts.code_rd_hit -e l2_rqsts.code_rd_miss -e L1-icache-load-misses -e icache.hit -e icache.misses -e icache.ifdata_stall -r 4 sleep 1

 Performance counter stats for 'CPU(s) 3' (4 runs):

     3.972.084.312      cycles                                                        ( +-  0,02% )  (69,23%)
     9.377.688.902      instructions              #    2,36  insn per cycle           ( +-  0,03% )  (76,92%)
        41.732.726      cache-references                                              ( +-  0,05% )  (76,92%)
             1.876      cache-misses              #    0,004 % of all cache refs      ( +- 78,93% )  (76,92%)
     1.798.074.138      branches:k                                                    ( +-  0,03% )  (76,92%)
         3.790.004      branch-misses:k           #    0,21% of all branches          ( +-  0,16% )  (76,92%)
         8.131.686      l2_rqsts.all_code_rd                                          ( +-  0,09% )  (76,92%)
         7.689.516      l2_rqsts.code_rd_hit                                          ( +-  0,11% )  (76,92%)
           442.190      l2_rqsts.code_rd_miss                                         ( +-  0,72% )  (76,92%)
         2.063.152      L1-icache-load-misses                                         ( +-  0,34% )  (76,92%)
       949.080.913      icache.hit                                                    ( +-  0,04% )  (61,55%)
         2.062.373      icache.misses                                                 ( +-  0,34% )  (61,55%)
        13.514.870      icache.ifdata_stall                                           ( +-  0,66% )  (61,55%)
#+end_example

Above result is slightly strange: Turbo-mode, but slightly slower PPS
benchmark and it have almost double l2_rqsts.all_code_rd. **UPDATE**: There
were a pcp/pmcd service running that seems to disturb the accuracy of these
measurements.


* Test on Mel git-tree: mm-bulk-rebase-v6r5

** baseline kernel

Kernel: 5.12.0-rc4-mel-mm-bulk-rebase-v6r5-baseline
#+begin_example
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      5       3,771,046   0          
XDP-RX CPU      total   3,771,046  

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    0:5   3,771,054   0          
rx_queue_index    0:sum 3,771,054  
#+end_example

#+begin_example
Running XDP on dev:veth2 (ifindex:12) action:XDP_PASS options:no_touch
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      0       3,690,327   0          
XDP-RX CPU      total   3,690,327  

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    0:0   3,690,335   0          
rx_queue_index    0:sum 3,690,335  
#+end_example

#+begin_example
[broadwell ~]
$ perf stat -C0 -e cycles -e  instructions -e cache-references \
 -e cache-misses -e branches:k -e branch-misses:k \
 -e l2_rqsts.all_code_rd -e l2_rqsts.code_rd_hit -e l2_rqsts.code_rd_miss \
 -e L1-icache-load-misses -e icache.hit -e icache.misses \
 -e icache.ifdata_stall -r 4 sleep 1

 Performance counter stats for 'CPU(s) 0' (4 runs):

     3.781.104.906      cycles                                                        ( +-  0,01% )  (69,23%)
     9.160.272.376      instructions              #    2,42  insn per cycle           ( +-  0,02% )  (76,93%)
        38.754.093      cache-references                                              ( +-  0,14% )  (76,93%)
             3.302      cache-misses              #    0,009 % of all cache refs      ( +- 38,71% )  (76,93%)
     1.702.142.682      branches:k                                                    ( +-  0,02% )  (76,93%)
         3.044.869      branch-misses:k           #    0,18% of all branches          ( +-  0,16% )  (76,93%)
         4.327.779      l2_rqsts.all_code_rd                                          ( +-  1,06% )  (76,93%)
         3.169.107      l2_rqsts.code_rd_hit                                          ( +-  1,81% )  (76,93%)
         1.156.787      l2_rqsts.code_rd_miss                                         ( +-  1,67% )  (76,93%)
         2.031.427      L1-icache-load-misses                                         ( +-  1,00% )  (76,93%)
       862.034.302      icache.hit                                                    ( +-  0,03% )  (61,53%)
         2.031.444      icache.misses                                                 ( +-  1,01% )  (61,53%)
        26.138.294      icache.ifdata_stall                                           ( +-  1,43% )  (61,53%)
#+end_example

** Use list variant

Kernel: 5.12.0-rc4-mel-mm-bulk-rebase-v6r5-jesper05-list+
#+begin_example
Running XDP on dev:veth2 (ifindex:12) action:XDP_PASS options:no_touch
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      4       3,940,242   0          
XDP-RX CPU      total   3,940,242  

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    0:4   3,940,243   0          
rx_queue_index    0:sum 3,940,243  
#+end_example

#+begin_example
     3.780.991.660      cycles                                                        ( +-  0,01% )  (69,23%)
     8.983.214.383      instructions              #    2,38  insn per cycle           ( +-  0,03% )  (76,92%)
        40.349.872      cache-references                                              ( +-  0,10% )  (76,92%)
             3.040      cache-misses              #    0,008 % of all cache refs      ( +- 55,39% )  (76,92%)
     1.717.544.811      branches:k                                                    ( +-  0,04% )  (76,92%)
         3.718.282      branch-misses:k           #    0,22% of all branches          ( +-  0,06% )  (76,92%)
         6.715.245      l2_rqsts.all_code_rd                                          ( +-  0,70% )  (76,92%)
         5.728.355      l2_rqsts.code_rd_hit                                          ( +-  0,85% )  (76,92%)
           985.961      l2_rqsts.code_rd_miss                                         ( +-  0,41% )  (76,92%)
         2.528.346      L1-icache-load-misses                                         ( +-  0,81% )  (76,92%)
       893.070.210      icache.hit                                                    ( +-  0,05% )  (61,54%)
         2.524.908      icache.misses                                                 ( +-  0,76% )  (61,54%)
        25.131.747      icache.ifdata_stall                                           ( +-  0,78% )  (61,54%)
#+end_example


* Micro optimisations

*UPDATE*: Choosing to drop this patch, it is waste too much memory and
it too fragile as it depends on compiler behaviour.

Document steps in micro optimizing page-alloactor code:
- make zone->free_area[order] access faster

** Observations

The code del_page_from_free_list() generate a strange imul operation:
#+begin_example
imul   $0x58,%rax,%rax
#+end_example

#+begin_src C
static inline void del_page_from_free_list(struct page *page, struct zone *zone,
					   unsigned int order)
{
	/* clear reported state and update reported page count */
	if (page_reported(page))
		__ClearPageReported(page);

	list_del(&page->lru);
	__ClearPageBuddy(page);
	set_page_private(page, 0);
	zone->free_area[order].nr_free--;
#+end_src

Tracked this down to:
#+begin_src C
struct zone {
    [...]
	struct free_area	free_area[MAX_ORDER];
#+end_src

This happens when accessing free_area like this:
#+begin_src C
	zone->free_area[order].nr_free--;
#+end_src

Perf show hot-spot in: rmqueue_bulk.constprop.0 / rmqueue_bulk()
#+begin_example
       │         mov    0x8(%rbx),%rax                                                                                            ▒
       │       __list_del():                                                                                                      ▒
       │         mov    %rax,0x8(%rdx)                                                                                            ▒
       │         mov    %rdx,(%rax)                                                                                               ▒
       │       del_page_from_free_list():                                                                                         ▒
 44,54 │1  e2:   imul   $0x58,%rbp,%rbp                                                                                           ▒
       │       expand():                                                                                                          ◆
       │         mov    $0x1,%r9d                                                                                                 ▒
       │         mov    %r13d,%ecx                                                                                                ▒
       │       set_page_private():                                                                                                ▒
       │         movq   $0x0,0x20(%rbx)                                                                                           ▒
       │       __ClearPageBuddy():                                                                                                ▒
       │         orl    $0x80,0x28(%rbx)                                                                                          ▒
       │         lea    -0x1(%r13),%r11d                                                                                          ▒
       │       expand():                                                                                                          ▒
       │         shl    %cl,%r9d                                                                                                  ▒
       │       list_del():                                                                                                        ▒
       │         movabs $0xdead000000000100,%rax                                                                                  ▒
       │         mov    %rax,(%rbx)                                                                                               ▒
       │         add    $0x22,%rax                                                                                                ▒
       │       expand():                                                                                                          ▒
       │         movslq %r9d,%r14                                                                                                 ▒
       │       list_del():                                                                                                        ▒
       │         mov    %rax,0x8(%rbx)                                                                                            ▒
       │       del_page_from_free_list():                                                                                         ▒
       │         subq   $0x1,0x110(%r15,%rbp,1)                                                                                   ▒
       │       expand():                                                                                                          ▒
#+end_example

** Why happening

The size of struct free_area is 88 bytes or 0x58 hex.

#+begin_src sh
$ pahole -C free_area mm/page_alloc.o
struct free_area {
	struct list_head           free_list[5];         /*     0    80 */
	/* --- cacheline 1 boundary (64 bytes) was 16 bytes ago --- */
	long unsigned int          nr_free;              /*    80     8 */

	/* size: 88, cachelines: 2, members: 2 */
	/* last cacheline: 24 bytes */
};
#+end_src

The reason for the code is to find the right struct free_area in struct
zone.  The array of 11 comes from define MAX_ORDER.

#+begin_example
struct zone {
        long unsigned int          _watermark[3];        /*     0    24 */
 [...]
        /* --- cacheline 3 boundary (192 bytes) --- */
        struct zone_padding        _pad1_ __attribute__((__aligned__(64))); /*   192     0 */
        struct free_area           free_area[11];        /*   192   968 */
        /* --- cacheline 18 boundary (1152 bytes) was 8 bytes ago --- */
        long unsigned int          flags;                /*  1160     8 */
        spinlock_t                 lock;                 /*  1168     4 */

        /* XXX 44 bytes hole, try to pack */

        /* --- cacheline 19 boundary (1216 bytes) --- */
        struct zone_padding        _pad2_ __attribute__((__aligned__(64))); /*  1216     0 */

#+end_example

The size 88 bytes or 0x58 hex. The compiler cannot find a shift
operation to use and instead choose to use a imul to find the offset
into the array free_area[].

Asm code to lookout for:
(objdump -Sr mm/page_alloc.o-use-imul)
#+begin_src asm
       zone->free_area[order].nr_free--;
    75ee:       44 89 f0                mov    %r14d,%eax
    75f1:       48 6b c0 58             imul   $0x58,%rax,%rax
    75f5:       48 03 04 24             add    (%rsp),%rax
    75f9:       49 83 ac 04 10 01 00    subq   $0x1,0x110(%r12,%rax,1)
#+end_src

It looks like it happens 45 times in =mm/page_alloc.o=:
#+begin_src C
$ objdump -Sr mm/page_alloc.o | grep imul | grep '0x58,' |wc -l
45
#+end_src

Code notes for hot-path: The del_page_from_free_list() contains the
zone->free_area[order].nr_free-- code, the __rmqueue_smallest was the
hotspot that calls this. This is called by __rmqueue, which is called by
rmqueue_bulk.

** Explaining patch with fix

#+begin_quote
mm: make zone->free_area[order] access faster

Avoid multiplication (imul) operations when accessing:
 zone->free_area[order].nr_free

This was really tricky to find. I was puzzled why perf reported that
rmqueue_bulk was using 44% of the time in an imul operation:

       │     del_page_from_free_list():
 44,54 │ e2:   imul   $0x58,%rax,%rax

This operation was generated (by compiler) because the struct free_area
have size 88 bytes or 0x58 hex. The compiler cannot find a shift
operation to use and instead choose to use a imul to find the offset
into the array free_area[].

The patch align struct free_area to a cache-line, which cause the
compiler avoid the imul operation. The imul operation is very fast on
modern Intel CPUs. To help fast-path that decrement 'nr_free' move the
member 'nr_free' to be first element, which saves one 'add' operation.

Looking up instruction latency this exchange a 3-cycle 'imul' with a
1-cycle 'shl', saving 2-cycles. It does trade some space to do this.

Used: gcc (GCC) 9.3.1 20200408 (Red Hat 9.3.1-2)
#+end_quote

Notes about moving members around:

Before: Move member 'nr_free':
#+begin_src asm
    760e:       44 89 f0                mov    %r14d,%eax
    7611:       48 83 c0 02             add    $0x2,%rax
    7615:       48 c1 e0 07             shl    $0x7,%rax
    7619:       48 03 04 24             add    (%rsp),%rax
    761d:       49 83 6c 04 10 01       subq   $0x1,0x10(%r12,%rax,1)
#+end_src

Move member 'nr_free':
#+begin_src asm
    75be:       44 89 f0                mov    %r14d,%eax
    75c1:       48 c1 e0 07             shl    $0x7,%rax
    75c5:       48 03 04 24             add    (%rsp),%rax
    75c9:       49 83 ac 04 c0 00 00    subq   $0x1,0xc0(%r12,%rax,1)
#+end_src
