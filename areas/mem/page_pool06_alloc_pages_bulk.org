# -*- fill-column: 76; -*-
#+Title: Testing page-allocator API proposal for alloc_pages_bulk
#+OPTIONS: ^:nil

Testing experimental patch from Mel Gorman that add alloc_pages_bulk() API.

This patch: https://lore.kernel.org/linux-mm/20210210130705.GC3629@suse.de/
With this fix: https://lore.kernel.org/linux-mm/20210211091235.GC3697@techsingularity.net/

Notice new patchset from Mel:
 - Message-Id: <20210224102603.19524-1-mgorman@techsingularity.net>
 - https://lore.kernel.org/netdev/20210224102603.19524-1-mgorman@techsingularity.net/

* Use-case: page_pool

The =alloc_pages_bulk()= use-case for page_pool is in
=__page_pool_alloc_pages_slow()=, for then the pool goes empty.

The mlx5 driver can easily empty the page_pool "cache", when redirected
xdp_frame's are converted into SKBs.

This happens both for veth and cpumap, as SKBs (created from xdp_frame) is
missing callback to page_pool.

** Test setup for veth

It requires some setup and two XDP programs to trigger the case, where
page_pool "cache" goes empty. And use of driver mlx5.

First: Create veth pair and enabled link up:

#+begin_src
ip link add veth1 type veth peer name veth2
ip link set veth1 up
ip link set veth2 up
#+end_src

Disable GRO/GSO/TSO on the veth devices
#+begin_example
ethtool -K veth1 gso off gro off tso off
ethtool -K veth2 gso off gro off tso off
#+end_example

When XDP-redirecting into a veth, we must remember to attach an XDP prog to
the peer device.

Redirect frame from mlx5p1 into veth1 (peer veth2)
- sudo ./xdp_redirect_map mlx5p1 veth1

Create SKBs from xdp_frame via XDP_PASS on veth2:
- sudo ./xdp_rxq_info --dev veth2 --act XDP_PASS

As the MAC-addr doesn't match the SKB packets are dropped very early, but it
suites our micro-benchmark test case.

* Baseline test

** baseline01: veth redirect

Kernel: Linux broadwell 5.11.0-net-next

#+begin_example
Running XDP on dev:veth2 (ifindex:12) action:XDP_PASS options:no_touch
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      0       3,677,958   0          
XDP-RX CPU      total   3,677,958  

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    0:0   3,677,966   0          
rx_queue_index    0:sum 3,677,966  

#+end_example

#+begin_example
Samples: 81K of event 'cycles', Event count (approx.): 73929158590
  Overhead  Symbol                                               IPC   [IPC Coverage]
+    4,87%  [k] __netif_receive_skb_core                         -      -
+    3,50%  [k] kmem_cache_free                                  -      -
+    3,26%  [k] dev_gro_receive                                  -      -
+    3,24%  [k] rmqueue                                          -      -
+    3,15%  [k] bpf_prog_943df0a1ce7ea5c2_xdp_prognum0           -      -
+    3,09%  [k] __xdp_release_frame                              -      -
+    2,99%  [k] memset_erms                                      -      -
+    2,89%  [k] get_page_from_freelist                           -      -
+    2,82%  [k] __alloc_pages_nodemask                           -      -
+    2,61%  [k] veth_xdp_rcv_one                                 -      -
+    2,54%  [k] free_unref_page_commit                           -      -
+    2,35%  [k] __list_del_entry_valid                           -      -
+    2,29%  [k] __netif_receive_skb_list_core                    -      -
+    2,13%  [k] dev_map_enqueue                                  -      -
+    2,07%  [k] mlx5e_xdp_handle                                 -      -
+    2,04%  [k] __xdp_build_skb_from_frame                       -      -
+    1,95%  [k] napi_gro_receive                                 -      -
+    1,94%  [k] xdp_do_redirect                                  -      -
+    1,85%  [k] netif_receive_skb_list_internal                  -      -
+    1,85%  [k] mlx5e_skb_from_cqe_mpwrq_linear                  -      -
+    1,84%  [k] mlx5e_handle_rx_cqe_mpwrq                        -      -
+    1,82%  [k] bpf_xdp_redirect_map                             -      -
+    1,74%  [k] kmem_cache_alloc_bulk                            -      -
+    1,71%  [k] ip_list_rcv                                      -      -
+    1,69%  [k] bpf_prog_a55118bafe28d557_xdp_redirect_map_prog  -      -
+    1,56%  [k] veth_xdp_rcv                                     -      -
+    1,48%  [k] __slab_free                                      -      -
+    1,45%  [k] free_unref_page_prepare.part.0                   -      -
+    1,43%  [k] eth_type_trans                                   -      -
+    1,40%  [k] dma_map_page_attrs                               -      -
+    1,30%  [k] mlx5e_poll_rx_cq                                 -      -
+    1,21%  [k] __list_add_valid                                 -      -
+    1,19%  [k] veth_xdp_xmit                                    -      -
+    1,11%  [k] free_unref_page                                  -      -
+    1,07%  [k] mlx5e_alloc_rx_mpwqe                             -      -
+    1,05%  [k] dma_unmap_page_attrs                             -      -
+    1,03%  [k] page_pool_release_page                           -      -
+    1,00%  [k] gro_normal_one                                   -      -
+    0,99%  [k] build_skb_around                                 -      -
+    0,94%  [k] __page_pool_alloc_pages_slow                     -      -
+    0,92%  [k] ip_rcv_core.isra.0                               -      -
+    0,90%  [k] prep_new_page                                    -      -
+    0,89%  [k] __build_skb_around                               -      -
+    0,81%  [k] free_pcp_prepare                                 -      -
#+end_example


* Using alloc_pages_bulk

Kernel: Linux broadwell 5.11.0-net-next-alloc_pages_bulk+
- With this fix [[https://patchwork.kernel.org/project/netdevbpf/patch/161402344429.1980160.4798557236979159924.stgit@firesoul/][to mlx5 driver]]

** test01: veth redirect (page_pool bulk 16)

Same veth setup as above: [[#test-setup-for-veth][setup]]

Results below with page_pool using bulk=16 for alloc_pages_bulk().

#+begin_example
Running XDP on dev:veth2 (ifindex:12) action:XDP_PASS options:no_touch
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      5       4,066,028   0          
XDP-RX CPU      total   4,066,028  

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    0:5   4,066,023   0          
rx_queue_index    0:sum 4,066,023  
#+end_example

#+begin_example
Samples: 51K of event 'cycles', Event count (approx.): 46934149161
  Overhead  Symbol                                               IPC   [IPC Coverage]
+    4,94%  [k] __netif_receive_skb_core                         -      -
+    4,02%  [k] kmem_cache_free                                  -      -
+    3,78%  [k] bpf_prog_943df0a1ce7ea5c2_xdp_prognum0           -      -
+    3,30%  [k] memset_erms                                      -      -
+    3,26%  [k] __xdp_release_frame                              -      -
+    3,23%  [k] dev_gro_receive                                  -      -
+    3,10%  [k] free_unref_page_commit                           -      -
+    3,06%  [k] veth_xdp_rcv_one                                 -      -
+    2,82%  [k] __list_del_entry_valid                           -      -
+    2,42%  [k] bpf_xdp_redirect_map                             -      -
+    2,40%  [k] __netif_receive_skb_list_core                    -      -
+    2,40%  [k] napi_gro_receive                                 -      -
+    2,21%  [k] __xdp_build_skb_from_frame                       -      -
+    2,16%  [k] mlx5e_skb_from_cqe_mpwrq_linear                  -      -
+    2,07%  [k] dev_map_enqueue                                  -      -
+    2,06%  [k] mlx5e_handle_rx_cqe_mpwrq                        -      -
+    2,01%  [k] ip_list_rcv                                      -      -
+    1,94%  [k] netif_receive_skb_list_internal                  -      -
+    1,92%  [k] kmem_cache_alloc_bulk                            -      -
+    1,91%  [k] bpf_prog_a55118bafe28d557_xdp_redirect_map_prog  -      -
+    1,90%  [k] veth_xdp_rcv                                     -      -
+    1,74%  [k] __list_add_valid                                 -      -
+    1,73%  [k] xdp_do_redirect                                  -      -
+    1,70%  [k] mlx5e_xdp_handle                                 -      -
+    1,63%  [k] free_unref_page_prepare.part.0                   -      -
+    1,61%  [k] dma_map_page_attrs                               -      -
+    1,53%  [k] __alloc_pages_bulk_nodemask                      -      -
+    1,43%  [k] __slab_free                                      -      -
+    1,42%  [k] mlx5e_poll_rx_cq                                 -      -
+    1,36%  [k] eth_type_trans                                   -      -
+    1,32%  [k] __page_pool_alloc_pages_slow                     -      -
+    1,22%  [k] free_unref_page                                  -      -
+    1,18%  [k] mlx5e_alloc_rx_mpwqe                             -      -
+    1,16%  [k] veth_xdp_xmit                                    -      -
+    1,09%  [k] build_skb_around                                 -      -
+    1,03%  [k] gro_normal_one                                   -      -
+    1,02%  [k] page_pool_release_page                           -      -
+    1,00%  [k] ip_rcv_core.isra.0                               -      -
+    1,00%  [k] dma_unmap_page_attrs                             -      -
+    0,99%  [k] __rmqueue_pcplist                                -      -
+    0,95%  [k] free_pcp_prepare                                 -      -
+    0,91%  [k] __build_skb_around                               -      -
     0,84%  [k] kfree_skb                                        -      -
#+end_example

** test02: veth redirect (page_pool bulk 64)

Same veth setup as above: [[#test-setup-for-veth][setup]]

Results below with page_pool using bulk=64 for alloc_pages_bulk().

#+begin_example
Running XDP on dev:veth2 (ifindex:12) action:XDP_PASS options:no_touch
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      1       4,297,656   0          
XDP-RX CPU      total   4,297,656  

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    0:1   4,297,649   0          
rx_queue_index    0:sum 4,297,649  
#+end_example

* Test with RFC patchset

Test with new patchset from Mel:
 - Message-Id: <20210224102603.19524-1-mgorman@techsingularity.net>
 - https://lore.kernel.org/netdev/20210224102603.19524-1-mgorman@techsingularity.net/

** initial test

bulk=64

#+begin_example
Running XDP on dev:veth2 (ifindex:12) action:XDP_PASS options:no_touch
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      3       4,368,926   0          
XDP-RX CPU      total   4,368,926  

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    0:3   4,368,917   0          
rx_queue_index    0:sum 4,368,917  
#+end_example

* patch notes

** Follow up to Mel's patchset

Patchset V1:
#+begin_example
stg mail --version='RFC net-next' --edit-cover --cc meup \
 --to mel --cc chuck.lever@oracle.com \
 --to mm --cc netdev --cc linux-nfs@vger.kernel.org --cc lkml \
 --in-reply-to='20210224102603.19524-1-mgorman@techsingularity.net' \
 03-reorder-add-page_pool_dma_map..mm-make-zone-free_area-order
#+end_example
Message-ID: <161419296941.2718959.12575257358107256094.stgit@firesoul>

V2 with minor changes and dropping micro-optimisation:
#+begin_example
stg mail --version='RFC V2 net-next' --edit-cover --cc meup \
 --to mel --cc chuck.lever@oracle.com \
 --to mm --cc netdev --cc linux-nfs@vger.kernel.org --cc lkml \
 --in-reply-to='20210224102603.19524-1-mgorman@techsingularity.net' \
 05-03-reorder-add-page_pool_dma_map..06-04-page_pool-use-alloc_pages_bulk
#+end_example
Message-ID: <161460522573.3031322.15721946341157092594.stgit@firesoul>

#+begin_quote
Use bulk order-0 page allocator API for page_pool

This is a followup to Mel Gorman's patchset:
 - Message-Id: <20210224102603.19524-1-mgorman@techsingularity.net>
 - https://lore.kernel.org/netdev/20210224102603.19524-1-mgorman@techsingularity.net/

Showing page_pool usage of the API for alloc_pages_bulk().

Maybe Mel Gorman will/can carry these patches?
(to keep it together with the alloc_pages_bulk API)
#+end_quote

** bench test veth

Test again:
#+begin_example
Running XDP on dev:veth2 (ifindex:12) action:XDP_PASS options:no_touch
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      0       4,302,291   0          
XDP-RX CPU      total   4,302,291  

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    0:0   4,302,285   0          
rx_queue_index    0:sum 4,302,285  
#+end_example

** desc: net: page_pool: use alloc_pages_bulk in refill code path

#+begin_quote
There are cases where the page_pool need to refill with pages from the
page allocator. Some workloads cause the page_pool to release pages
instead of recycling these pages.

For these workload it can improve performance to bulk alloc pages from
the page-allocator to refill the alloc cache.

For XDP-redirect workload with 100G mlx5 driver (that use page_pool)
redirecting xdp_frame packets into a veth, that does XDP_PASS to create
an SKB from the xdp_frame, which then cannot return the page to the
page_pool. In this case, we saw[1] an improvement of 18.8% from using
the alloc_pages_bulk API (3,677,958 pps -> 4,368,926 pps).

[1] https://github.com/xdp-project/xdp-project/blob/master/areas/mem/page_pool06_alloc_pages_bulk.org
#+end_quote


* Test on Mel git-tree: mm-bulk-rebase-v4r2

Tests based on Mel Gorman's git tree:
 - git://git.kernel.org/pub/scm/linux/kernel/git/mel/linux.git

Branch: mm-bulk-rebase-v4r2
 - Changed the last patch with page_pool changes

** stg mail

Promised to followup in Message-ID: <20210315094038.22d6d79a@carbon>
- Below stg [[https://lore.kernel.org/netdev/161583677541.3715498.6118778324185171839.stgit@firesoul/][Message-ID]]

#+begin_example
stg mail --version='mel-git' --edit-cover --cc meup \
 --to mel --cc chuck.lever@oracle.com --cc alex \
 --to mm --cc netdev --cc linux-nfs@vger.kernel.org --cc lkml \
 --in-reply-to='20210315094038.22d6d79a@carbon' \
 net-page_pool-use
#+end_example

#+begin_quote
Subj: Followup: Update [PATCH 7/7] in Mel's series

This patch is against Mel's git-tree:
 git://git.kernel.org/pub/scm/linux/kernel/git/mel/linux.git

Using branch: mm-bulk-rebase-v4r2 but replacing the last patch related to
the page_pool using __alloc_pages_bulk().

 https://git.kernel.org/pub/scm/linux/kernel/git/mel/linux.git/log/?h=mm-bulk-rebase-v4r2

While implementing suggestions by Alexander Duyck, I realised that I could
simplify the code further, and simply take the last page from the
pool->alloc.cache given this avoids special casing the last page.

I re-ran performance tests and the improvement have been reduced to 13% from
18% before, but I don't think the rewrite of the specific patch have
anything to do with this.

Notes on tests:
 https://github.com/xdp-project/xdp-project/blob/master/areas/mem/page_pool06_alloc_pages_bulk.org#test-on-mel-git-tree
#+end_quote

Performance summary: +13% faster
 - (3,810,013 pps -> 4,308,208 pps)
 - ((4308208/3810013)-1)*100 = 13.07%

Previous: 18.8% (3,677,958 pps -> 4,368,926 pps).
 - Thus, slower than before.
 - Mostly look like better baseline

** Updated patch

Alexander Duyck point out there was a cleaner way to implement
changes in function =__page_pool_alloc_pages_slow()=.

#+begin_example
Running XDP on dev:veth2 (ifindex:12) action:XDP_PASS options:no_touch
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      3       4,308,208   0          
XDP-RX CPU      total   4,308,208  

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    0:3   4,308,208   0          
rx_queue_index    0:sum 4,308,208  
#+end_example

** Pop patch using __alloc_pages_bulk

#+begin_example
Running XDP on dev:veth2 (ifindex:12) action:XDP_PASS options:no_touch
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      1       3,810,013   0          
XDP-RX CPU      total   3,810,013  

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    0:1   3,810,013   0          
rx_queue_index    0:sum 3,810,013  
#+end_example

* Test on Mel git-tree: mm-bulk-rebase-v5r9

Tests based on Mel Gorman's git tree:
 - git://git.kernel.org/pub/scm/linux/kernel/git/mel/linux.git
 - Branch: mm-bulk-rebase-v5r9

** micro-benchmark: page_bench04_bulk

Notice these "per elem" measurements are alloc+free cost for order-0 pages

page_bench04_bulk micro-benchmark on branch: mm-bulk-rebase-v5r9
 - https://lore.kernel.org/netdev/20210322130446.0a505db0@carbon/

CPU: Intel(R) Xeon(R) CPU E5-1650 v4 @ 3.60GHz

#+begin_example
BASELINE
 single_page alloc+put: Per elem: 199 cycles(tsc) 55.472 ns

LIST variant: time_bulk_page_alloc_free_list: step=bulk size

 Per elem: 206 cycles(tsc) 57.478 ns (step:1)
 Per elem: 154 cycles(tsc) 42.861 ns (step:2)
 Per elem: 145 cycles(tsc) 40.536 ns (step:3)
 Per elem: 142 cycles(tsc) 39.477 ns (step:4)
 Per elem: 142 cycles(tsc) 39.610 ns (step:8)
 Per elem: 137 cycles(tsc) 38.155 ns (step:16)
 Per elem: 135 cycles(tsc) 37.739 ns (step:32)
 Per elem: 134 cycles(tsc) 37.282 ns (step:64)
 Per elem: 133 cycles(tsc) 36.993 ns (step:128)

ARRAY variant: time_bulk_page_alloc_free_array: step=bulk size

 Per elem: 202 cycles(tsc) 56.383 ns (step:1)
 Per elem: 144 cycles(tsc) 40.047 ns (step:2)
 Per elem: 134 cycles(tsc) 37.339 ns (step:3)
 Per elem: 128 cycles(tsc) 35.578 ns (step:4)
 Per elem: 120 cycles(tsc) 33.592 ns (step:8)
 Per elem: 116 cycles(tsc) 32.362 ns (step:16)
 Per elem: 113 cycles(tsc) 31.476 ns (step:32)
 Per elem: 110 cycles(tsc) 30.633 ns (step:64)
 Per elem: 110 cycles(tsc) 30.596 ns (step:128)
#+end_example

* Micro optimisations

*UPDATE*: Choosing to drop this patch, it is waste too much memory and
it too fragile as it depends on compiler behaviour.

Document steps in micro optimizing page-alloactor code:
- make zone->free_area[order] access faster

** Observations

The code del_page_from_free_list() generate a strange imul operation:
#+begin_example
imul   $0x58,%rax,%rax
#+end_example

#+begin_src C
static inline void del_page_from_free_list(struct page *page, struct zone *zone,
					   unsigned int order)
{
	/* clear reported state and update reported page count */
	if (page_reported(page))
		__ClearPageReported(page);

	list_del(&page->lru);
	__ClearPageBuddy(page);
	set_page_private(page, 0);
	zone->free_area[order].nr_free--;
#+end_src

Tracked this down to:
#+begin_src C
struct zone {
    [...]
	struct free_area	free_area[MAX_ORDER];
#+end_src

This happens when accessing free_area like this:
#+begin_src C
	zone->free_area[order].nr_free--;
#+end_src

Perf show hot-spot in: rmqueue_bulk.constprop.0 / rmqueue_bulk()
#+begin_example
       │         mov    0x8(%rbx),%rax                                                                                            ▒
       │       __list_del():                                                                                                      ▒
       │         mov    %rax,0x8(%rdx)                                                                                            ▒
       │         mov    %rdx,(%rax)                                                                                               ▒
       │       del_page_from_free_list():                                                                                         ▒
 44,54 │1  e2:   imul   $0x58,%rbp,%rbp                                                                                           ▒
       │       expand():                                                                                                          ◆
       │         mov    $0x1,%r9d                                                                                                 ▒
       │         mov    %r13d,%ecx                                                                                                ▒
       │       set_page_private():                                                                                                ▒
       │         movq   $0x0,0x20(%rbx)                                                                                           ▒
       │       __ClearPageBuddy():                                                                                                ▒
       │         orl    $0x80,0x28(%rbx)                                                                                          ▒
       │         lea    -0x1(%r13),%r11d                                                                                          ▒
       │       expand():                                                                                                          ▒
       │         shl    %cl,%r9d                                                                                                  ▒
       │       list_del():                                                                                                        ▒
       │         movabs $0xdead000000000100,%rax                                                                                  ▒
       │         mov    %rax,(%rbx)                                                                                               ▒
       │         add    $0x22,%rax                                                                                                ▒
       │       expand():                                                                                                          ▒
       │         movslq %r9d,%r14                                                                                                 ▒
       │       list_del():                                                                                                        ▒
       │         mov    %rax,0x8(%rbx)                                                                                            ▒
       │       del_page_from_free_list():                                                                                         ▒
       │         subq   $0x1,0x110(%r15,%rbp,1)                                                                                   ▒
       │       expand():                                                                                                          ▒
#+end_example

** Why happening

The size of struct free_area is 88 bytes or 0x58 hex.

#+begin_src sh
$ pahole -C free_area mm/page_alloc.o
struct free_area {
	struct list_head           free_list[5];         /*     0    80 */
	/* --- cacheline 1 boundary (64 bytes) was 16 bytes ago --- */
	long unsigned int          nr_free;              /*    80     8 */

	/* size: 88, cachelines: 2, members: 2 */
	/* last cacheline: 24 bytes */
};
#+end_src

The reason for the code is to find the right struct free_area in struct
zone.  The array of 11 comes from define MAX_ORDER.

#+begin_example
struct zone {
        long unsigned int          _watermark[3];        /*     0    24 */
 [...]
        /* --- cacheline 3 boundary (192 bytes) --- */
        struct zone_padding        _pad1_ __attribute__((__aligned__(64))); /*   192     0 */
        struct free_area           free_area[11];        /*   192   968 */
        /* --- cacheline 18 boundary (1152 bytes) was 8 bytes ago --- */
        long unsigned int          flags;                /*  1160     8 */
        spinlock_t                 lock;                 /*  1168     4 */

        /* XXX 44 bytes hole, try to pack */

        /* --- cacheline 19 boundary (1216 bytes) --- */
        struct zone_padding        _pad2_ __attribute__((__aligned__(64))); /*  1216     0 */

#+end_example

The size 88 bytes or 0x58 hex. The compiler cannot find a shift
operation to use and instead choose to use a imul to find the offset
into the array free_area[].

Asm code to lookout for:
(objdump -Sr mm/page_alloc.o-use-imul)
#+begin_src asm
       zone->free_area[order].nr_free--;
    75ee:       44 89 f0                mov    %r14d,%eax
    75f1:       48 6b c0 58             imul   $0x58,%rax,%rax
    75f5:       48 03 04 24             add    (%rsp),%rax
    75f9:       49 83 ac 04 10 01 00    subq   $0x1,0x110(%r12,%rax,1)
#+end_src

It looks like it happens 45 times in =mm/page_alloc.o=:
#+begin_src C
$ objdump -Sr mm/page_alloc.o | grep imul | grep '0x58,' |wc -l
45
#+end_src

Code notes for hot-path: The del_page_from_free_list() contains the
zone->free_area[order].nr_free-- code, the __rmqueue_smallest was the
hotspot that calls this. This is called by __rmqueue, which is called by
rmqueue_bulk.

** Explaining patch with fix

#+begin_quote
mm: make zone->free_area[order] access faster

Avoid multiplication (imul) operations when accessing:
 zone->free_area[order].nr_free

This was really tricky to find. I was puzzled why perf reported that
rmqueue_bulk was using 44% of the time in an imul operation:

       │     del_page_from_free_list():
 44,54 │ e2:   imul   $0x58,%rax,%rax

This operation was generated (by compiler) because the struct free_area
have size 88 bytes or 0x58 hex. The compiler cannot find a shift
operation to use and instead choose to use a imul to find the offset
into the array free_area[].

The patch align struct free_area to a cache-line, which cause the
compiler avoid the imul operation. The imul operation is very fast on
modern Intel CPUs. To help fast-path that decrement 'nr_free' move the
member 'nr_free' to be first element, which saves one 'add' operation.

Looking up instruction latency this exchange a 3-cycle 'imul' with a
1-cycle 'shl', saving 2-cycles. It does trade some space to do this.

Used: gcc (GCC) 9.3.1 20200408 (Red Hat 9.3.1-2)
#+end_quote

Notes about moving members around:

Before: Move member 'nr_free':
#+begin_src asm
    760e:       44 89 f0                mov    %r14d,%eax
    7611:       48 83 c0 02             add    $0x2,%rax
    7615:       48 c1 e0 07             shl    $0x7,%rax
    7619:       48 03 04 24             add    (%rsp),%rax
    761d:       49 83 6c 04 10 01       subq   $0x1,0x10(%r12,%rax,1)
#+end_src

Move member 'nr_free':
#+begin_src asm
    75be:       44 89 f0                mov    %r14d,%eax
    75c1:       48 c1 e0 07             shl    $0x7,%rax
    75c5:       48 03 04 24             add    (%rsp),%rax
    75c9:       49 83 ac 04 c0 00 00    subq   $0x1,0xc0(%r12,%rax,1)
#+end_src
