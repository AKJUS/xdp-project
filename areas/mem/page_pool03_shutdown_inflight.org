# -*- fill-column: 76; -*-
#+Title: page_pool handling in-flight frames during shutdown
#+OPTIONS: ^:nil

This issue is (unfortunately) blocking drivers from using the DMA-mapping
feature of page_pool.

* Issue: lifetime of device + page_pool

There is an issue/challenge when a page_pool object itself is freed, while
there are still packets in-flight (that need to be returned to the
page_pool). (The issue is more prone to happen, when SKBs can carry
page_pool pages).

Today, this is handled by __xdp_return() via RCU lookups (in rhashtable),
which will simply call put_page() if the page_pool ID isn't in the
rhashtable. This is only valid/correct if page_pool is NOT used for
DMA-mapping.

** Solution#1 - kill performance

(*Wrong solution*)

The naive solution, that would kill performance, is to have a refcnt on
page_pool for each outstanding packet-page. And use this refcnt to know when
all pages have been returned, and thus when it is safe to free the
page_pool.

** Solution#2

Create a smaller object that can be put into rhashtable, that only store the
=struct device= pointer, that is the one used for the DMA-unmap call.

This is not optimal, as we still don't know when this smaller object can be
freed from the rhashtable. (This both pin-down rhashtable IDs and memory,
for the lifetime of the kernel)

** Solution#3

Keep counters for packets in-flight, or rather outstanding-pages. This will
allow us to know when it is safe to free (and remove the page_pool object
from rhashtable).

This solution can like solution#1, easily kill performance, depending on
implementation details. E.g. it is bad to have a single in-flight counter
that is updated on both alloc and free time, because it will cause
cache-line bouncing (stressing CPU cache coherency protocol).

Properties of page_pool to realise:
- Alloc happens RX time and is protected by NAPI/softirq, which guarantees
  no-concurrent access e.g. to page_pool pp_alloc_cache.
- Free can run concurrently on remote CPUs, and ptr_ring is used for
  synchronise return of pages (via producer lock).
- The ptr_ring doesn't account number of objects in the ring.

The proposed solution is having two (unsigned) counters, that can be on
different cache-lines, and can be used to deduct in-flight packets. This is
done by mapping the unsigned "sequence" counters onto signed Two's
complement arithmetic operations. This is e.g. used by kernel's =time_after=
macros, described in kernel commit [[https://git.kernel.org/torvalds/c/1ba3aab3033b][1ba3aab3033b]] and [[https://git.kernel.org/torvalds/c/5a581b367b5][5a581b367b5]], and also
explained in this [[https://en.wikipedia.org/wiki/Serial_number_arithmetic#General_Solution][Wikipedia article]] and [[https://tools.ietf.org/html/rfc1982][RFC1982]].

Thus, these two increment counters only need to be read and compared, when
checking if it's safe to free the page_pool structure. Which will only
happen when driver have disconnected RX/alloc side. Thus, on a
non-fast-path.

The counters should track number of "real" page allocation/free operations.
The pages getting recycled, and is storing in internal ptr_ring, is
basically counted as part of in-flight pages. This, also reduce the effect
on the fast-path recycling code.

On the alloc side the counter can be incremented lockless, as it is updated
under NAPI/softirq protection.

On the free-side, the operation can happen on remote CPUs. The operation
that can happen (on remote CPUs) is in-case ptr_ring is full at return/free
time, which result in the page being returned to page-allocator. Thus, this
counter need to be updated atomically. The atomic cost could be mitigated
via using lib/percpu_counter.c.

Number of pages held 'in-flight' by the page_pool, is also relevant for
drivers not using the DMA mapping, as indicate/include the pages stored in
the ptr_ring. Later, when/if system comes under memory pressure, we want to
allow page-allocator to request page_pool to release resources, where this
count could be used to select among page_pools.  It is also useful for stats
and debugging.

*** Complications with solution#3

Driver take down procedure need to change. Fortunately only one driver
(mlx5) uses page_pool.

Current procedure is:
#+begin_src C
static void mlx5e_free_rq(struct mlx5e_rq *rq)
{
	// [...]
	xdp_rxq_info_unreg(&rq->xdp_rxq);
	if (rq->page_pool)
		page_pool_destroy(rq->page_pool);

#+end_src

With handling of in-flight packet-pages, we might have to postpone calling
=page_pool_destroy()=. We could move this into =xdp_rxq_info_unreg()=, but
XDP mem model code need to stay allocator agnostic. Thus, we likely need to
extend =struct xdp_mem_allocator= with destructor call back.

Further more, we likely need to have both a "request-cleanup" and
"destructor" callback. As we want to release as many memory resources as
possible as early as possible, and only wait for the packet-pages in-flight.

Extra: We can add a REG_STATE_DYING to XDP (struct xdp_rxq_info), which
can/might help us catch invalid driver use-cases.

*** TODO: cleanup/error code-path in drivers

We cannot completely remove =page_pool_destroy()= from the API, as drivers
setup paths can fail, and they might need to free the page_pool resource
explicitly.  We could export a =__page_pool_free()= function.

**** Found bug/issue.

xdp: fix leak of IDA cyclic ID if rhashtable_insert_slow fails

Fix error handling case, where inserting ID with rhashtable_insert_slow
fails in xdp_rxq_info_reg_mem_model, which leads to never releasing the IDA
ID, as the lookup in xdp_rxq_info_unreg_mem_model fails and thus
ida_simple_remove() is never called.

Fix by releasing ID via ida_simple_remove(), and mark xdp_rxq->mem.id with
zero, which is already checked in xdp_rxq_info_unreg_mem_model().

#+begin_src diff
diff --git a/net/core/xdp.c b/net/core/xdp.c
index 4b2b194f4f1f..762abeb89847 100644
--- a/net/core/xdp.c
+++ b/net/core/xdp.c
@@ -301,6 +301,8 @@ int xdp_rxq_info_reg_mem_model(struct xdp_rxq_info *xdp_rxq,
        /* Insert allocator into ID lookup table */
        ptr = rhashtable_insert_slow(mem_id_ht, &id, &xdp_alloc->node);
        if (IS_ERR(ptr)) {
+               ida_simple_remove(&mem_id_pool, xdp_rxq->mem.id);
+               xdp_rxq->mem.id = 0;
                errno = PTR_ERR(ptr);
                goto err;
        }
#+end_src



* Notes while cleanup patchset

Keeping some notes while cleaning up patchset. This section might not make
sense, consider removing this section later.

** Prepare mlx5

Removed comment.
#+begin_example
+void __page_pool_free(struct page_pool *pool)
+{
+       /* API user must call page_pool_request_shutdown first, and
+        * assure that it was successful
+        */
#+end_example

page_pool: introduce page_pool_free

In case driver fails to register the page_pool with XDP return API (via
xdp_rxq_info_reg_mem_model()), then the driver can free the page_pool
resources more directly than calling page_pool_destroy(), which does a
unnecessarily RCU free procedure.

This patch is preparing for removing page_pool_destroy(), from driver
invocation.

** mlx5: more correct usage of page_pool API

The page_pool API states user is responsible for invoking page_pool_put_page
once. This were not done in mlx5e_page_release() when recycle is false. This
e.g. happens from mlx5e_free_rq() when tearing down resources.

This API omission is not critical, as mlx5 doesn't use page_pool for
DMA-mapping yet. This becomes important later when tracking in-flight
frames. This change makes the pages on the driver local page_cache, to go
through the page_pool system.

In mlx5e_free_rq() moved the page_pool_destroy() call to after the
mlx5e_page_release() calls, as it is more correct.


** Strange use in i40e

#+begin_src diff
diff --git a/drivers/net/ethernet/intel/i40e/i40e_main.c b/drivers/net/ethernet/intel/i40e/i40e_main.c
index 320562b39686..441323ca1464 100644
--- a/drivers/net/ethernet/intel/i40e/i40e_main.c
+++ b/drivers/net/ethernet/intel/i40e/i40e_main.c
@@ -3248,7 +3248,7 @@ static int i40e_configure_rx_ring(struct i40e_ring *ring)
 
        if (ring->vsi->type == I40E_VSI_MAIN)
                xdp_rxq_info_unreg_mem_model(&ring->xdp_rxq);
-
+// FIXME: Why not using xdp_rxq_info_unreg() ???
        ring->xsk_umem = i40e_xsk_umem(ring);
        if (ring->xsk_umem) {
                ring->rx_buf_len = ring->xsk_umem->chunk_size_nohr -

#+end_src

* Tests
** (Established): Test if __xdp_return() can hit no page_pool id issue

First establish if this code can be hit:

#+begin_src diff
diff --git a/net/core/xdp.c b/net/core/xdp.c
index 3d53f9f247e5..6114c80393db 100644
--- a/net/core/xdp.c
+++ b/net/core/xdp.c
@@ -338,6 +338,8 @@ static void __xdp_return(void *data, struct xdp_mem_info *mem, bool napi_direct,
                        napi_direct &= !xdp_return_frame_no_direct();
                        page_pool_put_page(xa->page_pool, page, napi_direct);
                } else {
+                       pr_warn("%s() XXX issue if page_pool(id:%d) use DMA\n",
+                               __func__, mem->id);
                        put_page(page);
                }
                rcu_read_unlock();
#+end_src

The driver mlx5 (in =mlx5e_xdp_set=) reset the NIC-ring "channels", when
changing between XDP and non-XDP mode.

In that case, the mlx5 driver doesn't reuse the page_pool, instead when a XDP
program is attached it "close" and free all the "channels", where
=mlx5e_free_rq= calls =xdp_rxq_info_unreg= as shown by this perf-probe stack
trace:

#+begin_example
xdp_rxq_info  1745 [001]  1529.547422: probe:xdp_rxq_info_unreg_2: (ffffffff8179caa6)
        ffffffff8179caa7 xdp_rxq_info_unreg+0x17 (/boot/vmlinux-5.1.0-bpf-next-page-pool+)
        ffffffffa022d32a mlx5e_free_rq+0x3a ([mlx5_core])
        ffffffffa022f0b2 mlx5e_close_channel+0x22 ([mlx5_core])
        ffffffffa0231486 mlx5e_close_channels+0x26 ([mlx5_core])
        ffffffffa0232ac7 mlx5e_close_locked+0x47 ([mlx5_core])
        ffffffffa0232d4c mlx5e_xdp+0x19c ([mlx5_core])
        ffffffff8176a39c dev_xdp_install+0x3c (/boot/vmlinux-5.1.0-bpf-next-page-pool+)
        ffffffff81788806 do_setlink+0xcd6 (/boot/vmlinux-5.1.0-bpf-next-page-pool+)
        ffffffff81788a10 rtnl_setlink+0xd0 (/boot/vmlinux-5.1.0-bpf-next-page-pool+)
        [...]
#+end_example

And =xdp_rxq_info_reg= is called by =mlx5e_alloc_rq=, but cannot be seen by
(below) call stack as it is inlined in =mlx5e_open_rq=.

#+begin_example
xdp_rxq_info  1806 [000]  1883.326305:     probe:xdp_rxq_info_reg: (ffffffff8179cae0)
        ffffffff8179cae1 xdp_rxq_info_reg+0x1 (/boot/vmlinux-5.1.0-bpf-next-page-pool+)
        ffffffffa022e6e3 mlx5e_open_rq+0x153 ([mlx5_core])
        ffffffffa0231395 mlx5e_open_channels+0xc25 ([mlx5_core])
        ffffffffa023289a mlx5e_open_locked+0x2a ([mlx5_core])
        ffffffffa0232d8a mlx5e_xdp+0x1da ([mlx5_core])
        ffffffff8176a39c dev_xdp_install+0x3c (/boot/vmlinux-5.1.0-bpf-next-page-pool+)
        ffffffff8177663e dev_change_xdp_fd+0xce (/boot/vmlinux-5.1.0-bpf-next-page-pool+)
        ffffffff81788806 do_setlink+0xcd6 (/boot/vmlinux-5.1.0-bpf-next-page-pool+)
        ffffffff81788a10 rtnl_setlink+0xd0 (/boot/vmlinux-5.1.0-bpf-next-page-pool+)
        ffffffff81786da2 rtnetlink_rcv_msg+0x122 (/boot/vmlinux-5.1.0-bpf-next-page-pool+)
        ffffffff817d4157 netlink_rcv_skb+0x37 (/boot/vmlinux-5.1.0-bpf-next-page-pool+)
        ffffffff817d3a49 netlink_unicast+0x169 (/boot/vmlinux-5.1.0-bpf-next-page-pool+)
        ffffffff817d3d71 netlink_sendmsg+0x291 (/boot/vmlinux-5.1.0-bpf-next-page-pool+)
        ffffffff817519b0 sock_sendmsg+0x30 (/boot/vmlinux-5.1.0-bpf-next-page-pool+)
        ffffffff81752eb8 __sys_sendto+0xe8 (/boot/vmlinux-5.1.0-bpf-next-page-pool+)
        ffffffff81752f04 __x64_sys_sendto+0x24 (/boot/vmlinux-5.1.0-bpf-next-page-pool+)
        ffffffff81002252 do_syscall_64+0x42 (/boot/vmlinux-5.1.0-bpf-next-page-pool+)
        ffffffff81a0008c entry_SYSCALL_64+0x7c (/boot/vmlinux-5.1.0-bpf-next-page-pool+)
            7f4d4090e4ed __send+0x1d (/usr/lib64/libc-2.26.so)
                  402332 main+0x422 (/home/jbrouer/kernel-bpf-samples/xdp_rxq_info)
#+end_example

For testing, I used XDP redirect map command:

#+begin_example
sudo ./xdp_redirect_map  $(</sys/class/net/mlx5p1/ifindex) \
                         $(</sys/class/net/ixgbe1/ifindex)
#+end_example

It took a couple of tries. *Confirmed*: The =pr_warn()= was triggered, when XDP
program was stopped, while having a packet generator running. It might have
increased the chance that the ixgbe adaptor was causing resets:

#+begin_example
May 21 16:36:14 broadwell kernel: ixgbe 0000:01:00.1 ixgbe2: initiating reset to clear Tx work after link loss
May 21 16:36:14 broadwell kernel: ixgbe 0000:01:00.1 ixgbe2: Reset adapter
May 21 16:36:15 broadwell kernel: ixgbe 0000:01:00.1 ixgbe2: detected SFP+: 4
May 21 16:36:15 broadwell kernel: ixgbe 0000:01:00.1 ixgbe2: initiating reset to clear Tx work after link loss
May 21 16:36:15 broadwell kernel: XXX mlx5e_free_rq()
May 21 16:36:15 broadwell kernel: XXX mlx5e_free_rq()
May 21 16:36:15 broadwell kernel: XXX mlx5e_free_rq()
May 21 16:36:15 broadwell kernel: XXX mlx5e_free_rq()
May 21 16:36:15 broadwell kernel: XXX mlx5e_free_rq()
May 21 16:36:15 broadwell kernel: XXX mlx5e_free_rq()
May 21 16:36:15 broadwell kernel: mlx5_core 0000:03:00.0 mlx5p1: Link up
May 21 16:36:15 broadwell kernel: ixgbe 0000:01:00.1 ixgbe2: Reset adapter
May 21 16:36:15 broadwell kernel: __xdp_return() XXX issue if page_pool(id:207) use DMA
May 21 16:36:15 broadwell kernel: __xdp_return() XXX issue if page_pool(id:207) use DMA
May 21 16:36:15 broadwell kernel: __xdp_return() XXX issue if page_pool(id:207) use DMA
May 21 16:36:15 broadwell kernel: __xdp_return() XXX issue if page_pool(id:207) use DMA
May 21 16:36:15 broadwell kernel: __xdp_return() XXX issue if page_pool(id:207) use DMA
May 21 16:36:15 broadwell kernel: __xdp_return() XXX issue if page_pool(id:207) use DMA
[...]
May 21 16:36:19 broadwell kernel: __xdp_return() XXX issue if page_pool(id:207) use DMA
May 21 16:36:19 broadwell kernel: __xdp_return() XXX issue if page_pool(id:207) use DMA
May 21 16:36:19 broadwell kernel: __xdp_return() XXX issue if page_pool(id:207) use DMA
May 21 16:36:19 broadwell kernel: ixgbe 0000:01:00.1: removed PHC on ixgbe2
May 21 16:36:19 broadwell kernel: ixgbe 0000:01:00.1 ixgbe2: detected SFP+: 4
May 21 16:36:19 broadwell kernel: ixgbe 0000:01:00.1: Multiqueue Enabled: Rx Queue count = 6, Tx Queue count = 6 XDP Queue count = 0
May 21 16:36:19 broadwell kernel: ixgbe 0000:01:00.1: registered PHC device on ixgbe2
May 21 16:36:19 broadwell kernel: ixgbe 0000:01:00.1 ixgbe2: detected SFP+: 4
May 21 16:36:19 broadwell kernel: ixgbe 0000:01:00.1 ixgbe2: NIC Link is Up 10 Gbps, Flow Control: RX/TX
#+end_example

** Test work-in-progress patch

After fixing mlx5 to call =xdp_rxq_info_unreg(&rq->xdp_rxq)= later and let
the non-recycle path call =page_pool_put_page()=.  The basics work:

#+begin_example
[ 1290.790220] XXX __mem_id_disconnect() id:184
[ 1290.797905] XXX mlx5e_free_rq()
[ 1290.802080] XXX __mem_id_disconnect() id:185
[ 1290.807158] ixgbe 0000:01:00.1 ixgbe2: detected SFP+: 4
[ 1290.813819] ixgbe 0000:01:00.1 ixgbe2: initiating reset to clear Tx work after link loss
[ 1290.814690] XXX mlx5e_free_rq()
[ 1290.827424] XXX __mem_id_disconnect() id:186
[ 1290.832841] XXX __page_pool_safe_to_destroy() inflight:511
[ 1290.839355] XXX xdp_rxq_info_unreg_mem_model() - start page_pool shutdown/destroy id(186)
[ 1290.896866] mlx5_core 0000:03:00.0 mlx5p1: Link up
[ 1291.035127] ixgbe 0000:01:00.1 ixgbe2: Reset adapter
[ 1291.335453] ixgbe 0000:01:00.1 ixgbe2: detected SFP+: 4
[ 1291.587066] ixgbe 0000:01:00.1 ixgbe2: NIC Link is Up 10 Gbps, Flow Control: RX/TX
[ 1291.875054] XXX mem_id_disconnect_defer_retry() id:186 call __mem_id_disconnect again
[ 1291.883661] XXX __mem_id_disconnect() id:186
#+end_example

It doesn't work with TCP (Update: I cannot reproduce this any-longer):
#+begin_example
[ 7600.046747] XXX mem_id_disconnect_defer_retry() id:121 call __mem_id_disconnect again
[ 7600.064706] XXX __mem_id_disconnect() id:121
[ 7600.070009] XXX page_pool_inflight() inflight:17102512 hold:17103469 released:957
[ 7600.078811] XXX __page_pool_request_shutdown() inflight:17102512
[ 7600.085844] XXX page_pool_inflight() inflight:17102512 hold:17103469 released:957
[ 7600.094627] XXX __page_pool_safe_to_destroy() inflight:17102512
[ 7600.101574] XXX mem_id_disconnect_defer_retry() id:121 call schedule_delayed_work
[ 7601.134756] XXX mem_id_disconnect_defer_retry() id:121 call __mem_id_disconnect again
[ 7601.152716] XXX __mem_id_disconnect() id:121
[ 7601.158024] XXX page_pool_inflight() inflight:17102512 hold:17103469 released:957
[ 7601.166825] XXX __page_pool_request_shutdown() inflight:17102512
[ 7601.173865] XXX page_pool_inflight() inflight:17102512 hold:17103469 released:957
[ 7601.182648] XXX __page_pool_safe_to_destroy() inflight:17102512
#+end_example


** Confused mlx5 doesn't fully use ring-size

Summary (TLDR): There was nothing wrong with the in-flight tracking, it was
just mlx5 driver that only fills it's ring-buffer with page_pool size
minus 64.

The mlx5 driver configures two different ring-sizes depending on if XDP is
used or not. For the XDP case ring-size 1024 is used, and non-XDP 512 is
used.

When debugging, I was seeing (XDP case) only 960 pages "tracked" on a unused
page_pool ring (for non-XDP case 448):
#+begin_example
[  370.223589] XXX page_pool_inflight() inflight:960 hold:960 released:0
[  370.231079] XXX __page_pool_request_shutdown() inflight:960
#+end_example

It turned out that the mlx5 driver refill function =mlx5e_post_rx_mpwqes()=
only refill up-to ring-size minus 64.

#+begin_src diff
@@ -624,7 +624,11 @@ bool mlx5e_post_rx_mpwqes(struct mlx5e_rq *rq)
        mlx5e_poll_ico_cq(&sq->cq, rq);
 
        missing = mlx5_wq_ll_missing(wq) - rq->mpwqe.umr_in_progress;
-
+       pr_warn("XXX DEBUG %s()  missing:%d x64:%d (%ld)\n", __func__,
+               missing, (missing*64), MLX5_MPWRQ_PAGES_PER_WQE); //DEBUG
+// Results first time called:
+// missing:7 x64:448 (64)   <-- non-XDP
+// missing:15 x64:960 (64)  <-- XDP-mode
        if (unlikely(rq->mpwqe.umr_in_progress > rq->mpwqe.umr_last_bulk))
                rq->stats->congst_umr++;
 
@@ -635,7 +639,7 @@ bool mlx5e_post_rx_mpwqes(struct mlx5e_rq *rq)
        head = rq->mpwqe.actual_wq_head;
        i = missing;
        do {
-               if (unlikely(mlx5e_alloc_rx_mpwqe(rq, head)))
+               if (unlikely(mlx5e_alloc_rx_mpwqe(rq, head))) // bulks 64
                        break;
                head = mlx5_wq_ll_get_wqe_next_ix(wq, head);
        } while (--i);
#+end_src

** TCP performance difference with XDP_PASS

Tom Barbette reported that when loading an XDP_PASS program, then TCP flows
were slower and cost more CPU, but only for driver mlx5 (not for e.g. i40e).
(via video link: https://www.youtube.com/watch?v=o5hlJZbN4Tk&feature=youtu.be)

On my system it was even worse, with an XDP_PASS program loaded, and iperf
(6 parallel TCP flows) I would see 100% CPU usage and total 83.3 Gbits/sec.
With non-XDP case, I saw 58% CPU (43% idle) and total 89.7 Gbits/sec

This was kind of hard to root-cause, but I solved it by increasing the TCP
socket size used by the iperf tool, like this:

$ iperf -s --window 4M
------------------------------------------------------------
Server listening on TCP port 5001
TCP window size:  416 KByte (WARNING: requested 4.00 MByte)
------------------------------------------------------------

Given I could reproduce, I took at closer look at perf record/report stats,
and it was actually quite clear that this was related to stalling on getting
pages from the page allocator (function calls top#6 get_page_from_freelist
and free_pcppages_bulk).

Using my tool: ethtool_stats.pl
 https://github.com/netoptimizer/network-testing/blob/master/bin/ethtool_stats.pl

It was clear that the mlx5 driver page-cache was not working:
#+begin_example
Ethtool(mlx5p1  ) stat:     6653761 (   6,653,761) <= rx_cache_busy /sec
Ethtool(mlx5p1  ) stat:     6653732 (   6,653,732) <= rx_cache_full /sec
Ethtool(mlx5p1  ) stat:      669481 (     669,481) <= rx_cache_reuse /sec
Ethtool(mlx5p1  ) stat:           1 (           1) <= rx_congst_umr /sec
Ethtool(mlx5p1  ) stat:     7323230 (   7,323,230) <= rx_csum_unnecessary /sec
Ethtool(mlx5p1  ) stat:        1034 (       1,034) <= rx_discards_phy /sec
Ethtool(mlx5p1  ) stat:     7323230 (   7,323,230) <= rx_packets /sec
Ethtool(mlx5p1  ) stat:     7324244 (   7,324,244) <= rx_packets_phy /sec
#+end_example

While the non-XDP case looked like this:
#+begin_example
Ethtool(mlx5p1  ) stat:      298929 (     298,929) <= rx_cache_busy /sec
Ethtool(mlx5p1  ) stat:      298971 (     298,971) <= rx_cache_full /sec
Ethtool(mlx5p1  ) stat:     3548789 (   3,548,789) <= rx_cache_reuse /sec
Ethtool(mlx5p1  ) stat:     7695476 (   7,695,476) <= rx_csum_complete /sec
Ethtool(mlx5p1  ) stat:     7695476 (   7,695,476) <= rx_packets /sec
Ethtool(mlx5p1  ) stat:     7695169 (   7,695,169) <= rx_packets_phy /sec
Manual consistence calc: 7695476-((3548789*2)+(298971*2)) = -44
#+end_example

With the increased TCP window size, the mlx5 driver cache is working better,
but not optimally, see below. I'm getting 88.0 Gbits/sec with 68% CPU usage.
#+begin_example
Ethtool(mlx5p1  ) stat:      894438 (     894,438) <= rx_cache_busy /sec
Ethtool(mlx5p1  ) stat:      894453 (     894,453) <= rx_cache_full /sec
Ethtool(mlx5p1  ) stat:     6638518 (   6,638,518) <= rx_cache_reuse /sec
Ethtool(mlx5p1  ) stat:           6 (           6) <= rx_congst_umr /sec
Ethtool(mlx5p1  ) stat:     7532983 (   7,532,983) <= rx_csum_unnecessary /sec
Ethtool(mlx5p1  ) stat:         164 (         164) <= rx_discards_phy /sec
Ethtool(mlx5p1  ) stat:     7532983 (   7,532,983) <= rx_packets /sec
Ethtool(mlx5p1  ) stat:     7533193 (   7,533,193) <= rx_packets_phy /sec
Manual consistence calc: 7532983-(6638518+894453) = 12
#+end_example

To understand why this is happening, you first have to know that the
difference is between the two RX-memory modes used by mlx5 for non-XDP vs
XDP. With non-XDP two frames are stored per memory-page, while for XDP only
a single frame per page is used.  The packets available in the RX-rings are
actually the same, as the ring sizes are non-XDP=512 vs. XDP=1024.

I believe, the real issue is that TCP use the SKB->truesize (based on frame
size) for different memory pressure and window calculations, which is why it
solved the issue to increase the window size manually.




Case: XDP_PASS and 6 parallel iperf TCP flows:
#+begin_example
May 31 15:24:23 broadwell kernel: XXX mlx5e_free_rq()
May 31 15:24:23 broadwell kernel: XXX __mem_id_disconnect() id:49
May 31 15:24:23 broadwell kernel: XXX page_pool_inflight() inflight:1136 hold:10665185 released:10664049
May 31 15:24:23 broadwell kernel: XXX __page_pool_request_shutdown() inflight:1136
May 31 15:24:23 broadwell kernel: XXX page_pool_inflight() inflight:0 hold:10665185 released:10665185
May 31 15:24:23 broadwell kernel: XXX mlx5e_free_rq()
May 31 15:24:23 broadwell kernel: XXX __mem_id_disconnect() id:50
May 31 15:24:23 broadwell kernel: XXX page_pool_inflight() inflight:0 hold:10665185 released:10665185
May 31 15:24:23 broadwell kernel: XXX page_pool_inflight() inflight:1039 hold:10766397 released:10765358
May 31 15:24:23 broadwell kernel: XXX __page_pool_request_shutdown() inflight:1039
May 31 15:24:23 broadwell kernel: XXX page_pool_inflight() inflight:0 hold:10766397 released:10766397
May 31 15:24:23 broadwell kernel: XXX mlx5e_free_rq()
May 31 15:24:23 broadwell kernel: XXX __mem_id_disconnect() id:51
May 31 15:24:23 broadwell kernel: XXX page_pool_inflight() inflight:0 hold:10766397 released:10766397
May 31 15:24:23 broadwell kernel: XXX page_pool_inflight() inflight:1109 hold:10648725 released:10647616
May 31 15:24:23 broadwell kernel: XXX __page_pool_request_shutdown() inflight:1109
May 31 15:24:23 broadwell kernel: XXX page_pool_inflight() inflight:0 hold:10648725 released:10648725
May 31 15:24:23 broadwell kernel: XXX mlx5e_free_rq()
May 31 15:24:23 broadwell kernel: XXX page_pool_inflight() inflight:0 hold:10648725 released:10648725
May 31 15:24:23 broadwell kernel: XXX __mem_id_disconnect() id:52
May 31 15:24:23 broadwell kernel: XXX page_pool_inflight() inflight:1104 hold:10770818 released:10769714
May 31 15:24:23 broadwell kernel: XXX __page_pool_request_shutdown() inflight:1104
May 31 15:24:23 broadwell kernel: XXX page_pool_inflight() inflight:0 hold:10770818 released:10770818
May 31 15:24:24 broadwell kernel: XXX mlx5e_free_rq()
May 31 15:24:24 broadwell kernel: XXX __mem_id_disconnect() id:53
May 31 15:24:24 broadwell kernel: XXX page_pool_inflight() inflight:0 hold:10770818 released:10770818
May 31 15:24:24 broadwell kernel: XXX page_pool_inflight() inflight:1132 hold:10635235 released:10634103
May 31 15:24:24 broadwell kernel: XXX __page_pool_request_shutdown() inflight:1132
May 31 15:24:24 broadwell kernel: XXX page_pool_inflight() inflight:0 hold:10635235 released:10635235

$ nstat -n && sleep 1 && nstat
#kernel
IpInReceives                    221138             0.0
IpInDelivers                    221138             0.0
IpOutRequests                   60230              0.0
TcpInSegs                       221136             0.0
TcpOutSegs                      60230              0.0
TcpExtTCPHPHits                 216175             0.0
TcpExtTCPHPAcks                 1                  0.0
TcpExtTCPBacklogCoalesce        4595               0.0
TcpExtTCPBacklogDrop            1                  0.0
TcpExtTCPRcvCoalesce            4270               0.0
TcpExtTCPOFOQueue               87                 0.0
TcpExtTCPWantZeroWindowAdv      215                0.0
TcpExtTCPOrigDataSent           1                  0.0
TcpExtTCPDelivered              1                  0.0
TcpExtTCPAckCompressed          48                 0.0
IpExtInOctets                   10644770764        0.0
IpExtOutOctets                  3132896            0.0
IpExtInNoECTPkts                7357218            0.0
#+end_example

Case: non-XDP and 6 parallel iperf TCP flows:
#+begin_example
May 31 15:24:24 broadwell kernel: XXX mlx5e_alloc_rq() pool_size: 512
May 31 15:24:24 broadwell kernel: XXX mlx5e_alloc_rq() pool_size: 512
May 31 15:24:24 broadwell kernel: mlx5_core 0000:03:00.0 mlx5p1: Link up
[...]
May 31 15:28:58 broadwell kernel: XXX mlx5e_free_rq()
May 31 15:28:58 broadwell kernel: XXX __mem_id_disconnect() id:55
May 31 15:28:58 broadwell kernel: XXX page_pool_inflight() inflight:589 hold:13434 released:12845
May 31 15:28:58 broadwell kernel: XXX __page_pool_request_shutdown() inflight:589
May 31 15:28:58 broadwell kernel: XXX page_pool_inflight() inflight:0 hold:13434 released:13434
May 31 15:28:58 broadwell kernel: XXX mlx5e_free_rq()
May 31 15:28:58 broadwell kernel: XXX __mem_id_disconnect() id:56
May 31 15:28:58 broadwell kernel: XXX page_pool_inflight() inflight:0 hold:13434 released:13434
May 31 15:28:58 broadwell kernel: XXX page_pool_inflight() inflight:514 hold:11891 released:11377
May 31 15:28:58 broadwell kernel: XXX __page_pool_request_shutdown() inflight:514
May 31 15:28:58 broadwell kernel: XXX page_pool_inflight() inflight:0 hold:11891 released:11891
May 31 15:28:58 broadwell kernel: XXX mlx5e_free_rq()
May 31 15:28:58 broadwell kernel: XXX __mem_id_disconnect() id:57
May 31 15:28:58 broadwell kernel: XXX page_pool_inflight() inflight:595 hold:14093 released:13498
May 31 15:28:58 broadwell kernel: XXX __page_pool_request_shutdown() inflight:595
May 31 15:28:58 broadwell kernel: XXX page_pool_inflight() inflight:0 hold:14093 released:14093
May 31 15:28:58 broadwell kernel: XXX page_pool_inflight() inflight:0 hold:11891 released:11891
May 31 15:28:58 broadwell kernel: XXX mlx5e_free_rq()
May 31 15:28:58 broadwell kernel: XXX page_pool_inflight() inflight:0 hold:14093 released:14093
May 31 15:28:58 broadwell kernel: XXX __mem_id_disconnect() id:58
May 31 15:28:58 broadwell kernel: XXX page_pool_inflight() inflight:598 hold:180742 released:180144
May 31 15:28:58 broadwell kernel: XXX __page_pool_request_shutdown() inflight:598
May 31 15:28:58 broadwell kernel: XXX page_pool_inflight() inflight:0 hold:180742 released:180742
May 31 15:28:58 broadwell kernel: XXX mlx5e_free_rq()
May 31 15:28:58 broadwell kernel: XXX __mem_id_disconnect() id:59
May 31 15:28:58 broadwell kernel: XXX page_pool_inflight() inflight:0 hold:180742 released:180742
May 31 15:28:58 broadwell kernel: XXX page_pool_inflight() inflight:608 hold:173294 released:172686
May 31 15:28:58 broadwell kernel: XXX __page_pool_request_shutdown() inflight:608
May 31 15:28:58 broadwell kernel: XXX page_pool_inflight() inflight:0 hold:173294 released:173294

$ nstat -n && sleep 1 && nstat
#kernel
IpInReceives                    286604             0.0
IpInDelivers                    286604             0.0
IpOutRequests                   205151             0.0
TcpInSegs                       286604             0.0
TcpOutSegs                      205147             0.0
TcpExtDelayedACKLost            1                  0.0
TcpExtTCPHPHits                 280054             0.0
TcpExtTCPBacklogCoalesce        5925               0.0
TcpExtTCPDSACKOldSent           1                  0.0
TcpExtTCPRcvCoalesce            5959               0.0
TcpExtTCPWantZeroWindowAdv      2                  0.0
IpExtInOctets                   11276032476        0.0
IpExtOutOctets                  10668956           0.0
IpExtInNoECTPkts                7778473            0.0
#+end_example

* Tracepoints

I got inspired by proof-reading Brendan Gregg's book on BPF, specifically
the bpftrace tools examples. By adding some tracepoints, we can allow for
debugging and getting stats from page_pool, without adding more kernel code.

