# -*- fill-column: 76; -*-
#+Title: page_pool handling in-flight frames during shutdown
#+OPTIONS: ^:nil



* Issue: lifetime of device + page_pool

There is an issue/challenge when a page_pool object itself is freed, while
there are still packets in-flight (that need to be returned to the
page_pool). (The issue is more prone to happen, when SKBs can carry
page_pool pages).

Today, this is handled by __xdp_return() via RCU lookups (in rhashtable),
which will simply call put_page() if the page_pool ID isn't in the
rhashtable. This is only valid/correct if page_pool is NOT used for
DMA-mapping.

** Solution#1 - kill performance

(*Wrong solution*)

The naive solution, that would kill performance, is to have a refcnt on
page_pool for each outstanding packet-page. And use this refcnt to know when
all pages have been returned, and thus when it is safe to free the
page_pool.

** Solution#2

Create a smaller object that can be put into rhashtable, that only store the
=struct device= pointer, that is the one used for the DMA-unmap call.

This is not optimal, as we still don't know when this smaller object can be
freed from the rhashtable. (This both pin-down rhashtable IDs and memory,
for the lifetime of the kernel)

** Solution#3

Keep counters for packets in-flight, or rather outstanding-pages. This will
allow us to know when it is safe to free (and remove the page_pool object
from rhashtable).

This solution can like solution#1, easily kill performance, depending on
implementation details. E.g. it is bad to have a single in-flight counter
that is updated on both alloc and free time, because it will cause
cache-line bouncing (stressing CPU cache coherency protocol).

Properties of page_pool to realise:
- Alloc happens RX time and is protected by NAPI/softirq, which guarantees
  no-concurrent access e.g. to page_pool pp_alloc_cache.
- Free can run concurrently on remote CPUs, and ptr_ring is used for
  synchronise return of pages (via producer lock).
- The ptr_ring doesn't account number of objects in the ring.

The proposed solution is having two (unsigned) counters, that can be on
different cache-lines, and can be used to deduct in-flight packets. This is
done by mapping the unsigned "sequence" counters onto signed Two's
complement arithmetic operations. This is e.g. used by kernel's =time_after=
macros, described in kernel commit [[https://git.kernel.org/torvalds/c/1ba3aab3033b][1ba3aab3033b]] and [[https://git.kernel.org/torvalds/c/5a581b367b5][5a581b367b5]], and also
explained in this [[https://en.wikipedia.org/wiki/Serial_number_arithmetic#General_Solution][Wikipedia article]] and [[https://tools.ietf.org/html/rfc1982][RFC1982]].

Thus, these two increment counters only need to be read and compared, when
checking if it's safe to free the page_pool structure. Which will only
happen when driver have disconnected RX/alloc side. Thus, on a
non-fast-path.

The counters should track number of "real" page allocation/free operations.
The pages getting recycled is basically counted as part of in-flight pages.
This, also reduce the effect on the fast-path recycling code.

On the alloc side the counter can be incremented lockless, as it is updated
under NAPI/softirq protection.

On the free-side, the operation can happen on remote CPUs. The operation
that can happen (on remote CPUs) is in-case ptr_ring is full at return/free
time, which result in the page being returned to page-allocator. Thus, this
counter need to be updated atomically. The atomic cost could be mitigated
via using lib/percpu_counter.c.

Number of pages held 'in-flight' by the page_pool, is also relevant for
drivers not using the DMA mapping, as indicate/include the pages stored in
the ptr_ring. Later, when/if system comes under memory pressure, we want to
allow page-allocator to request page_pool to release resources, where this
count could be used to select among page_pools.  It is also useful for stats
and debugging.

*** Complications with solution#3

Driver take down procedure need to change. Fortunately only one driver
(mlx5) uses page_pool.

Current procedure is:
#+begin_src C
static void mlx5e_free_rq(struct mlx5e_rq *rq)
{
	// [...]
	xdp_rxq_info_unreg(&rq->xdp_rxq);
	if (rq->page_pool)
		page_pool_destroy(rq->page_pool);

#+end_src

With handling of in-flight packet-pages, we might have to postpone calling
=page_pool_destroy()=. We could move this into =xdp_rxq_info_unreg()=, but
XDP mem model code need to stay allocator agnostic. Thus, we likely need to
extend =struct xdp_mem_allocator= with destructor call back.

Further more, we likely need to have both a "request-cleanup" and
"destructor" callback. As we want to release as many memory resources as
possible as early as possible, and only wait for the packet-pages in-flight.

Extra: We can add a REG_STATE_DYING to XDP (struct xdp_rxq_info), which
can/might help us catch invalid driver use-cases.


* Tests
** (Established): Test if __xdp_return() can hit no page_pool id issue

First establish if this code can be hit:

#+begin_src diff
diff --git a/net/core/xdp.c b/net/core/xdp.c
index 3d53f9f247e5..6114c80393db 100644
--- a/net/core/xdp.c
+++ b/net/core/xdp.c
@@ -338,6 +338,8 @@ static void __xdp_return(void *data, struct xdp_mem_info *mem, bool napi_direct,
                        napi_direct &= !xdp_return_frame_no_direct();
                        page_pool_put_page(xa->page_pool, page, napi_direct);
                } else {
+                       pr_warn("%s() XXX issue if page_pool(id:%d) use DMA\n",
+                               __func__, mem->id);
                        put_page(page);
                }
                rcu_read_unlock();
#+end_src

The driver mlx5 (in =mlx5e_xdp_set=) reset the NIC-ring "channels", when
changing between XDP and non-XDP mode.

In that case, the mlx5 driver doesn't reuse the page_pool, instead when a XDP
program is attached it "close" and free all the "channels", where
=mlx5e_free_rq= calls =xdp_rxq_info_unreg= as shown by this perf-probe stack
trace:

#+begin_example
xdp_rxq_info  1745 [001]  1529.547422: probe:xdp_rxq_info_unreg_2: (ffffffff8179caa6)
        ffffffff8179caa7 xdp_rxq_info_unreg+0x17 (/boot/vmlinux-5.1.0-bpf-next-page-pool+)
        ffffffffa022d32a mlx5e_free_rq+0x3a ([mlx5_core])
        ffffffffa022f0b2 mlx5e_close_channel+0x22 ([mlx5_core])
        ffffffffa0231486 mlx5e_close_channels+0x26 ([mlx5_core])
        ffffffffa0232ac7 mlx5e_close_locked+0x47 ([mlx5_core])
        ffffffffa0232d4c mlx5e_xdp+0x19c ([mlx5_core])
        ffffffff8176a39c dev_xdp_install+0x3c (/boot/vmlinux-5.1.0-bpf-next-page-pool+)
        ffffffff81788806 do_setlink+0xcd6 (/boot/vmlinux-5.1.0-bpf-next-page-pool+)
        ffffffff81788a10 rtnl_setlink+0xd0 (/boot/vmlinux-5.1.0-bpf-next-page-pool+)
        [...]
#+end_example

And =xdp_rxq_info_reg= is called by =mlx5e_alloc_rq=, but cannot be seen by
(below) call stack as it is inlined in =mlx5e_open_rq=.

#+begin_example
xdp_rxq_info  1806 [000]  1883.326305:     probe:xdp_rxq_info_reg: (ffffffff8179cae0)
        ffffffff8179cae1 xdp_rxq_info_reg+0x1 (/boot/vmlinux-5.1.0-bpf-next-page-pool+)
        ffffffffa022e6e3 mlx5e_open_rq+0x153 ([mlx5_core])
        ffffffffa0231395 mlx5e_open_channels+0xc25 ([mlx5_core])
        ffffffffa023289a mlx5e_open_locked+0x2a ([mlx5_core])
        ffffffffa0232d8a mlx5e_xdp+0x1da ([mlx5_core])
        ffffffff8176a39c dev_xdp_install+0x3c (/boot/vmlinux-5.1.0-bpf-next-page-pool+)
        ffffffff8177663e dev_change_xdp_fd+0xce (/boot/vmlinux-5.1.0-bpf-next-page-pool+)
        ffffffff81788806 do_setlink+0xcd6 (/boot/vmlinux-5.1.0-bpf-next-page-pool+)
        ffffffff81788a10 rtnl_setlink+0xd0 (/boot/vmlinux-5.1.0-bpf-next-page-pool+)
        ffffffff81786da2 rtnetlink_rcv_msg+0x122 (/boot/vmlinux-5.1.0-bpf-next-page-pool+)
        ffffffff817d4157 netlink_rcv_skb+0x37 (/boot/vmlinux-5.1.0-bpf-next-page-pool+)
        ffffffff817d3a49 netlink_unicast+0x169 (/boot/vmlinux-5.1.0-bpf-next-page-pool+)
        ffffffff817d3d71 netlink_sendmsg+0x291 (/boot/vmlinux-5.1.0-bpf-next-page-pool+)
        ffffffff817519b0 sock_sendmsg+0x30 (/boot/vmlinux-5.1.0-bpf-next-page-pool+)
        ffffffff81752eb8 __sys_sendto+0xe8 (/boot/vmlinux-5.1.0-bpf-next-page-pool+)
        ffffffff81752f04 __x64_sys_sendto+0x24 (/boot/vmlinux-5.1.0-bpf-next-page-pool+)
        ffffffff81002252 do_syscall_64+0x42 (/boot/vmlinux-5.1.0-bpf-next-page-pool+)
        ffffffff81a0008c entry_SYSCALL_64+0x7c (/boot/vmlinux-5.1.0-bpf-next-page-pool+)
            7f4d4090e4ed __send+0x1d (/usr/lib64/libc-2.26.so)
                  402332 main+0x422 (/home/jbrouer/kernel-bpf-samples/xdp_rxq_info)
#+end_example

For testing, I used XDP redirect map command:

#+begin_example
sudo ./xdp_redirect_map  $(</sys/class/net/mlx5p1/ifindex) \
                         $(</sys/class/net/ixgbe1/ifindex)
#+end_example

It took a couple of tries. *Confirmed*: The =pr_warn()= was triggered, when XDP
program was stopped, while having a packet generator running. It might have
increased the chance that the ixgbe adaptor was causing resets:

#+begin_example
May 21 16:36:14 broadwell kernel: ixgbe 0000:01:00.1 ixgbe2: initiating reset to clear Tx work after link loss
May 21 16:36:14 broadwell kernel: ixgbe 0000:01:00.1 ixgbe2: Reset adapter
May 21 16:36:15 broadwell kernel: ixgbe 0000:01:00.1 ixgbe2: detected SFP+: 4
May 21 16:36:15 broadwell kernel: ixgbe 0000:01:00.1 ixgbe2: initiating reset to clear Tx work after link loss
May 21 16:36:15 broadwell kernel: XXX mlx5e_free_rq()
May 21 16:36:15 broadwell kernel: XXX mlx5e_free_rq()
May 21 16:36:15 broadwell kernel: XXX mlx5e_free_rq()
May 21 16:36:15 broadwell kernel: XXX mlx5e_free_rq()
May 21 16:36:15 broadwell kernel: XXX mlx5e_free_rq()
May 21 16:36:15 broadwell kernel: XXX mlx5e_free_rq()
May 21 16:36:15 broadwell kernel: mlx5_core 0000:03:00.0 mlx5p1: Link up
May 21 16:36:15 broadwell kernel: ixgbe 0000:01:00.1 ixgbe2: Reset adapter
May 21 16:36:15 broadwell kernel: __xdp_return() XXX issue if page_pool(id:207) use DMA
May 21 16:36:15 broadwell kernel: __xdp_return() XXX issue if page_pool(id:207) use DMA
May 21 16:36:15 broadwell kernel: __xdp_return() XXX issue if page_pool(id:207) use DMA
May 21 16:36:15 broadwell kernel: __xdp_return() XXX issue if page_pool(id:207) use DMA
May 21 16:36:15 broadwell kernel: __xdp_return() XXX issue if page_pool(id:207) use DMA
May 21 16:36:15 broadwell kernel: __xdp_return() XXX issue if page_pool(id:207) use DMA
[...]
May 21 16:36:19 broadwell kernel: __xdp_return() XXX issue if page_pool(id:207) use DMA
May 21 16:36:19 broadwell kernel: __xdp_return() XXX issue if page_pool(id:207) use DMA
May 21 16:36:19 broadwell kernel: __xdp_return() XXX issue if page_pool(id:207) use DMA
May 21 16:36:19 broadwell kernel: ixgbe 0000:01:00.1: removed PHC on ixgbe2
May 21 16:36:19 broadwell kernel: ixgbe 0000:01:00.1 ixgbe2: detected SFP+: 4
May 21 16:36:19 broadwell kernel: ixgbe 0000:01:00.1: Multiqueue Enabled: Rx Queue count = 6, Tx Queue count = 6 XDP Queue count = 0
May 21 16:36:19 broadwell kernel: ixgbe 0000:01:00.1: registered PHC device on ixgbe2
May 21 16:36:19 broadwell kernel: ixgbe 0000:01:00.1 ixgbe2: detected SFP+: 4
May 21 16:36:19 broadwell kernel: ixgbe 0000:01:00.1 ixgbe2: NIC Link is Up 10 Gbps, Flow Control: RX/TX
#+end_example

