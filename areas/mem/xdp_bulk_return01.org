# -*- fill-column: 76; -*-
#+Title: Testing XDP redirect bulk return API
#+Options: ^:nil

Upstream patchset proposal
 https://lore.kernel.org/netdev/cover.1604686496.git.lorenzo@kernel.org/

Testing this git tree + branch:
 - https://github.com/LorenzoBianconi/net-next/tree/xdp_bulk_tx_return_jesper

From cover letter:
#+begin_quote
XDP bulk APIs introduce a defer/flush mechanism to return
pages belonging to the same xdp_mem_allocator object
(identified via the mem.id field) in bulk to optimize
I-cache and D-cache since xdp_return_frame is usually run
inside the driver NAPI tx completion loop.

Convert mvneta, mvpp2 and mlx5 drivers to xdp_return_frame_bulk APIs.
#+end_quote


* Baseline testing

#+begin_src sh
[broadwell ~]$ uname -r -v
5.10.0-rc2-lorenzo-baseline-for-bulk+ #4 SMP PREEMPT Mon Nov 9 11:46:02 CET 2020
#+end_src

Testing at kernel commit bff6f1db91e330d7fba56f815cdbc412c75fe163

Generator running:
#+begin_src sh
[firesoul pktgen]$ ./pktgen_sample03_burst_single_flow.sh -vi mlx5p1 -d 198.18.1.1 -m ec:0d:9a:db:11:c4 -t 12 
#+end_src

Generator sending speed:
- Ethtool(mlx5p1  ) stat: 44992839 ( 44,992,839) <= tx_packets /sec
- Ethtool(mlx5p1  ) stat: 44993648 ( 44,993,648) <= tx_packets_phy /sec

** XDP_TX test

This XDP_TX should not really be affected by this optimisation, but it shows
the max/upper performance bound achievable with a single RX-to-TX queue. In
principle this can be used to deduce the overhead of XDP_REDIRECT net-core
code infrastructure.

Max XDP_TX performance:
#+begin_example
sudo ./xdp_rxq_info --dev mlx5p1 --act XDP_TX
[...]
Running XDP on dev:mlx5p1 (ifindex:7) action:XDP_TX options:swapmac
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      1       16,046,587  0          
XDP-RX CPU      total   16,046,587 

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    1:1   16,046,598  0          
rx_queue_index    1:sum 16,046,598 
#+end_example

Verifying the actual transmitted packets, as above XDP counter can only
count the RX-events. Something is wrong, as we see only 9,184,940
tx_packets_phy/sec (phy level transmits).

#+begin_example
Show adapter(s) (mlx5p1) statistics (ONLY that changed!)
Ethtool(mlx5p1  ) stat:     253253 (        253,253) <= ch1_poll /sec
Ethtool(mlx5p1  ) stat:     253253 (        253,253) <= ch_poll /sec
Ethtool(mlx5p1  ) stat:   16208223 (     16,208,223) <= rx1_cache_reuse /sec
Ethtool(mlx5p1  ) stat:    7023234 (      7,023,234) <= rx1_xdp_drop /sec
Ethtool(mlx5p1  ) stat:     143515 (        143,515) <= rx1_xdp_tx_cqes /sec
Ethtool(mlx5p1  ) stat:    7023234 (      7,023,234) <= rx1_xdp_tx_full /sec
Ethtool(mlx5p1  ) stat:    9184953 (      9,184,953) <= rx1_xdp_tx_inlnw /sec
Ethtool(mlx5p1  ) stat:     861089 (        861,089) <= rx1_xdp_tx_mpwqe /sec
Ethtool(mlx5p1  ) stat:     975901 (        975,901) <= rx1_xdp_tx_nops /sec
Ethtool(mlx5p1  ) stat:    9184953 (      9,184,953) <= rx1_xdp_tx_xmit /sec
Ethtool(mlx5p1  ) stat:   44354640 (     44,354,640) <= rx_64_bytes_phy /sec
Ethtool(mlx5p1  ) stat: 2838694013 (  2,838,694,013) <= rx_bytes_phy /sec
Ethtool(mlx5p1  ) stat:   16208181 (     16,208,181) <= rx_cache_reuse /sec
Ethtool(mlx5p1  ) stat:   28146411 (     28,146,411) <= rx_out_of_buffer /sec
Ethtool(mlx5p1  ) stat:   44354594 (     44,354,594) <= rx_packets_phy /sec
Ethtool(mlx5p1  ) stat: 2838703690 (  2,838,703,690) <= rx_prio0_bytes /sec
Ethtool(mlx5p1  ) stat:   44354740 (     44,354,740) <= rx_prio0_packets /sec
Ethtool(mlx5p1  ) stat: 2661276172 (  2,661,276,172) <= rx_vport_unicast_bytes /sec
Ethtool(mlx5p1  ) stat:   44354604 (     44,354,604) <= rx_vport_unicast_packets /sec
Ethtool(mlx5p1  ) stat:    7023223 (      7,023,223) <= rx_xdp_drop /sec
Ethtool(mlx5p1  ) stat:     143515 (        143,515) <= rx_xdp_tx_cqe /sec
Ethtool(mlx5p1  ) stat:    7023222 (      7,023,222) <= rx_xdp_tx_full /sec
Ethtool(mlx5p1  ) stat:    9184949 (      9,184,949) <= rx_xdp_tx_inlnw /sec
Ethtool(mlx5p1  ) stat:     861089 (        861,089) <= rx_xdp_tx_mpwqe /sec
Ethtool(mlx5p1  ) stat:     975901 (        975,901) <= rx_xdp_tx_nops /sec
Ethtool(mlx5p1  ) stat:    9184949 (      9,184,949) <= rx_xdp_tx_xmit /sec
Ethtool(mlx5p1  ) stat:  587836166 (    587,836,166) <= tx_bytes_phy /sec
Ethtool(mlx5p1  ) stat:    9184940 (      9,184,940) <= tx_packets_phy /sec
Ethtool(mlx5p1  ) stat:  587836799 (    587,836,799) <= tx_prio0_bytes /sec
Ethtool(mlx5p1  ) stat:    9184951 (      9,184,951) <= tx_prio0_packets /sec
Ethtool(mlx5p1  ) stat:  551096207 (    551,096,207) <= tx_vport_unicast_bytes /sec
Ethtool(mlx5p1  ) stat:    9184940 (      9,184,940) <= tx_vport_unicast_packets /sec
#+end_example

** XDP-redirect bounce

Use XDP-redirect to act like XDP_TX and redirect packets back-out same
interface. This is done as-a-test to keep same net_device in cache and same
driver code (I-cache) active.

Testing xdp_redirect_map:
#+begin_example
jbrouer@broadwell kernel-bpf-samples]$ sudo ./xdp_redirect_map mlx5p1 mlx5p1
input: 7 output: 7
libbpf: Kernel error message: XDP program already attached
WARN: link set xdp fd failed on 7
ifindex 7:    8900610 pkt/s
ifindex 7:    8996142 pkt/s
ifindex 7:    8985280 pkt/s
ifindex 7:    8980360 pkt/s
ifindex 7:    8988103 pkt/s
#+end_example

Ethtool stats to verify packets are transmitted:
#+begin_example
Show adapter(s) (mlx5p1) statistics (ONLY that changed!)
Ethtool(mlx5p1  ) stat:       140436 (        140,436) <= ch1_poll /sec
Ethtool(mlx5p1  ) stat:       140436 (        140,436) <= ch_poll /sec
Ethtool(mlx5p1  ) stat:      8987891 (      8,987,891) <= rx1_cache_empty /sec
Ethtool(mlx5p1  ) stat:      8987880 (      8,987,880) <= rx1_xdp_redirect /sec
Ethtool(mlx5p1  ) stat:     44748662 (     44,748,662) <= rx_64_bytes_phy /sec
Ethtool(mlx5p1  ) stat:   2863921010 (  2,863,921,010) <= rx_bytes_phy /sec
Ethtool(mlx5p1  ) stat:      8987894 (      8,987,894) <= rx_cache_empty /sec
Ethtool(mlx5p1  ) stat:     35760982 (     35,760,982) <= rx_out_of_buffer /sec
Ethtool(mlx5p1  ) stat:     44748762 (     44,748,762) <= rx_packets_phy /sec
Ethtool(mlx5p1  ) stat:   2863907667 (  2,863,907,667) <= rx_prio0_bytes /sec
Ethtool(mlx5p1  ) stat:     44748558 (     44,748,558) <= rx_prio0_packets /sec
Ethtool(mlx5p1  ) stat:   2684927295 (  2,684,927,295) <= rx_vport_unicast_bytes /sec
Ethtool(mlx5p1  ) stat:     44748791 (     44,748,791) <= rx_vport_unicast_packets /sec
Ethtool(mlx5p1  ) stat:      8987876 (      8,987,876) <= rx_xdp_redirect /sec
Ethtool(mlx5p1  ) stat:       140435 (        140,435) <= tx1_xdp_cqes /sec
Ethtool(mlx5p1  ) stat:       280871 (        280,871) <= tx1_xdp_mpwqe /sec
Ethtool(mlx5p1  ) stat:       608555 (        608,555) <= tx1_xdp_nops /sec
Ethtool(mlx5p1  ) stat:      8987882 (      8,987,882) <= tx1_xdp_xmit /sec
Ethtool(mlx5p1  ) stat:    575223027 (    575,223,027) <= tx_bytes_phy /sec
Ethtool(mlx5p1  ) stat:      8987852 (      8,987,852) <= tx_packets_phy /sec
Ethtool(mlx5p1  ) stat:    575219919 (    575,219,919) <= tx_prio0_bytes /sec
Ethtool(mlx5p1  ) stat:      8987809 (      8,987,809) <= tx_prio0_packets /sec
Ethtool(mlx5p1  ) stat:    539271092 (    539,271,092) <= tx_vport_unicast_bytes /sec
Ethtool(mlx5p1  ) stat:      8987852 (      8,987,852) <= tx_vport_unicast_packets /sec
Ethtool(mlx5p1  ) stat:       140436 (        140,436) <= tx_xdp_cqes /sec
Ethtool(mlx5p1  ) stat:       280871 (        280,871) <= tx_xdp_mpwqe /sec
Ethtool(mlx5p1  ) stat:       608555 (        608,555) <= tx_xdp_nops /sec
Ethtool(mlx5p1  ) stat:      8987878 (      8,987,878) <= tx_xdp_xmit /sec
#+end_example
