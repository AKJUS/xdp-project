# -*- fill-column: 76; -*-
#+Title: Testing XDP redirect bulk return API
#+Options: ^:nil

Upstream patchset proposal
 https://lore.kernel.org/netdev/cover.1604686496.git.lorenzo@kernel.org/

Testing this git tree + branch:
 - https://github.com/LorenzoBianconi/net-next/tree/xdp_bulk_tx_return_jesper

From cover letter:
#+begin_quote
XDP bulk APIs introduce a defer/flush mechanism to return
pages belonging to the same xdp_mem_allocator object
(identified via the mem.id field) in bulk to optimize
I-cache and D-cache since xdp_return_frame is usually run
inside the driver NAPI tx completion loop.

Convert mvneta, mvpp2 and mlx5 drivers to xdp_return_frame_bulk APIs.
#+end_quote


* Baseline testing

#+begin_src sh
[broadwell ~]$ uname -r -v
5.10.0-rc2-lorenzo-baseline-for-bulk+ #4 SMP PREEMPT Mon Nov 9 11:46:02 CET 2020
#+end_src

Testing at kernel commit bff6f1db91e330d7fba56f815cdbc412c75fe163

Generator running:
#+begin_src sh
[firesoul pktgen]$ ./pktgen_sample03_burst_single_flow.sh -vi mlx5p1 -d 198.18.1.1 -m ec:0d:9a:db:11:c4 -t 12 
#+end_src

Generator sending speed:
- Ethtool(mlx5p1  ) stat: 44992839 ( 44,992,839) <= tx_packets /sec
- Ethtool(mlx5p1  ) stat: 44993648 ( 44,993,648) <= tx_packets_phy /sec

** XDP_TX test

This XDP_TX should not really be affected by this optimisation, but it shows
the max/upper performance bound achievable with a single RX-to-TX queue. In
principle this can be used to deduce the overhead of XDP_REDIRECT net-core
code infrastructure.

Max XDP_TX performance:
#+begin_example
sudo ./xdp_rxq_info --dev mlx5p1 --act XDP_TX
[...]
Running XDP on dev:mlx5p1 (ifindex:7) action:XDP_TX options:swapmac
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      1       16,046,587  0          
XDP-RX CPU      total   16,046,587 

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    1:1   16,046,598  0          
rx_queue_index    1:sum 16,046,598 
#+end_example

Verifying the actual transmitted packets, as above XDP counter can only
count the RX-events. Something is wrong, as we see only 9,184,940
tx_packets_phy/sec (phy level transmits).

#+begin_example
Show adapter(s) (mlx5p1) statistics (ONLY that changed!)
Ethtool(mlx5p1  ) stat:     253253 (        253,253) <= ch1_poll /sec
Ethtool(mlx5p1  ) stat:     253253 (        253,253) <= ch_poll /sec
Ethtool(mlx5p1  ) stat:   16208223 (     16,208,223) <= rx1_cache_reuse /sec
Ethtool(mlx5p1  ) stat:    7023234 (      7,023,234) <= rx1_xdp_drop /sec
Ethtool(mlx5p1  ) stat:     143515 (        143,515) <= rx1_xdp_tx_cqes /sec
Ethtool(mlx5p1  ) stat:    7023234 (      7,023,234) <= rx1_xdp_tx_full /sec
Ethtool(mlx5p1  ) stat:    9184953 (      9,184,953) <= rx1_xdp_tx_inlnw /sec
Ethtool(mlx5p1  ) stat:     861089 (        861,089) <= rx1_xdp_tx_mpwqe /sec
Ethtool(mlx5p1  ) stat:     975901 (        975,901) <= rx1_xdp_tx_nops /sec
Ethtool(mlx5p1  ) stat:    9184953 (      9,184,953) <= rx1_xdp_tx_xmit /sec
Ethtool(mlx5p1  ) stat:   44354640 (     44,354,640) <= rx_64_bytes_phy /sec
Ethtool(mlx5p1  ) stat: 2838694013 (  2,838,694,013) <= rx_bytes_phy /sec
Ethtool(mlx5p1  ) stat:   16208181 (     16,208,181) <= rx_cache_reuse /sec
Ethtool(mlx5p1  ) stat:   28146411 (     28,146,411) <= rx_out_of_buffer /sec
Ethtool(mlx5p1  ) stat:   44354594 (     44,354,594) <= rx_packets_phy /sec
Ethtool(mlx5p1  ) stat: 2838703690 (  2,838,703,690) <= rx_prio0_bytes /sec
Ethtool(mlx5p1  ) stat:   44354740 (     44,354,740) <= rx_prio0_packets /sec
Ethtool(mlx5p1  ) stat: 2661276172 (  2,661,276,172) <= rx_vport_unicast_bytes /sec
Ethtool(mlx5p1  ) stat:   44354604 (     44,354,604) <= rx_vport_unicast_packets /sec
Ethtool(mlx5p1  ) stat:    7023223 (      7,023,223) <= rx_xdp_drop /sec
Ethtool(mlx5p1  ) stat:     143515 (        143,515) <= rx_xdp_tx_cqe /sec
Ethtool(mlx5p1  ) stat:    7023222 (      7,023,222) <= rx_xdp_tx_full /sec
Ethtool(mlx5p1  ) stat:    9184949 (      9,184,949) <= rx_xdp_tx_inlnw /sec
Ethtool(mlx5p1  ) stat:     861089 (        861,089) <= rx_xdp_tx_mpwqe /sec
Ethtool(mlx5p1  ) stat:     975901 (        975,901) <= rx_xdp_tx_nops /sec
Ethtool(mlx5p1  ) stat:    9184949 (      9,184,949) <= rx_xdp_tx_xmit /sec
Ethtool(mlx5p1  ) stat:  587836166 (    587,836,166) <= tx_bytes_phy /sec
Ethtool(mlx5p1  ) stat:    9184940 (      9,184,940) <= tx_packets_phy /sec
Ethtool(mlx5p1  ) stat:  587836799 (    587,836,799) <= tx_prio0_bytes /sec
Ethtool(mlx5p1  ) stat:    9184951 (      9,184,951) <= tx_prio0_packets /sec
Ethtool(mlx5p1  ) stat:  551096207 (    551,096,207) <= tx_vport_unicast_bytes /sec
Ethtool(mlx5p1  ) stat:    9184940 (      9,184,940) <= tx_vport_unicast_packets /sec
#+end_example

** XDP-redirect bounce (baseline)

Use XDP-redirect to act like XDP_TX and redirect packets back-out same
interface. This is done as-a-test to keep same net_device in cache and same
driver code (I-cache) active.

Testing xdp_redirect_map:
#+begin_example
jbrouer@broadwell kernel-bpf-samples]$ sudo ./xdp_redirect_map mlx5p1 mlx5p1
input: 7 output: 7
libbpf: Kernel error message: XDP program already attached
WARN: link set xdp fd failed on 7
ifindex 7:    8900610 pkt/s
ifindex 7:    8996142 pkt/s
ifindex 7:    8985280 pkt/s
ifindex 7:    8980360 pkt/s
ifindex 7:    8988103 pkt/s
#+end_example

Ethtool stats to verify packets are transmitted:
#+begin_example
Show adapter(s) (mlx5p1) statistics (ONLY that changed!)
Ethtool(mlx5p1  ) stat:       140436 (        140,436) <= ch1_poll /sec
Ethtool(mlx5p1  ) stat:       140436 (        140,436) <= ch_poll /sec
Ethtool(mlx5p1  ) stat:      8987891 (      8,987,891) <= rx1_cache_empty /sec
Ethtool(mlx5p1  ) stat:      8987880 (      8,987,880) <= rx1_xdp_redirect /sec
Ethtool(mlx5p1  ) stat:     44748662 (     44,748,662) <= rx_64_bytes_phy /sec
Ethtool(mlx5p1  ) stat:   2863921010 (  2,863,921,010) <= rx_bytes_phy /sec
Ethtool(mlx5p1  ) stat:      8987894 (      8,987,894) <= rx_cache_empty /sec
Ethtool(mlx5p1  ) stat:     35760982 (     35,760,982) <= rx_out_of_buffer /sec
Ethtool(mlx5p1  ) stat:     44748762 (     44,748,762) <= rx_packets_phy /sec
Ethtool(mlx5p1  ) stat:   2863907667 (  2,863,907,667) <= rx_prio0_bytes /sec
Ethtool(mlx5p1  ) stat:     44748558 (     44,748,558) <= rx_prio0_packets /sec
Ethtool(mlx5p1  ) stat:   2684927295 (  2,684,927,295) <= rx_vport_unicast_bytes /sec
Ethtool(mlx5p1  ) stat:     44748791 (     44,748,791) <= rx_vport_unicast_packets /sec
Ethtool(mlx5p1  ) stat:      8987876 (      8,987,876) <= rx_xdp_redirect /sec
Ethtool(mlx5p1  ) stat:       140435 (        140,435) <= tx1_xdp_cqes /sec
Ethtool(mlx5p1  ) stat:       280871 (        280,871) <= tx1_xdp_mpwqe /sec
Ethtool(mlx5p1  ) stat:       608555 (        608,555) <= tx1_xdp_nops /sec
Ethtool(mlx5p1  ) stat:      8987882 (      8,987,882) <= tx1_xdp_xmit /sec
Ethtool(mlx5p1  ) stat:    575223027 (    575,223,027) <= tx_bytes_phy /sec
Ethtool(mlx5p1  ) stat:      8987852 (      8,987,852) <= tx_packets_phy /sec
Ethtool(mlx5p1  ) stat:    575219919 (    575,219,919) <= tx_prio0_bytes /sec
Ethtool(mlx5p1  ) stat:      8987809 (      8,987,809) <= tx_prio0_packets /sec
Ethtool(mlx5p1  ) stat:    539271092 (    539,271,092) <= tx_vport_unicast_bytes /sec
Ethtool(mlx5p1  ) stat:      8987852 (      8,987,852) <= tx_vport_unicast_packets /sec
Ethtool(mlx5p1  ) stat:       140436 (        140,436) <= tx_xdp_cqes /sec
Ethtool(mlx5p1  ) stat:       280871 (        280,871) <= tx_xdp_mpwqe /sec
Ethtool(mlx5p1  ) stat:       608555 (        608,555) <= tx_xdp_nops /sec
Ethtool(mlx5p1  ) stat:      8987878 (      8,987,878) <= tx_xdp_xmit /sec
#+end_example

Baseline: choosing TX-pps at phy level
- Baseline: 8987852 ( 8,987,852) <= tx_packets_phy /sec

* Testing patchset

Upstream patchset proposal
 https://lore.kernel.org/netdev/cover.1604686496.git.lorenzo@kernel.org/

Testing this git tree + branch:
 - https://github.com/LorenzoBianconi/net-next/tree/xdp_bulk_tx_return_jesper

** XDP_TX test

#+begin_example
sudo ./xdp_rxq_info --dev mlx5p1 --act XDP_TX
[...]
Running XDP on dev:mlx5p1 (ifindex:7) action:XDP_TX options:swapmac
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      5       15,282,307  0          
XDP-RX CPU      total   15,282,307 

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    5:5   15,282,316  0          
rx_queue_index    5:sum 15,282,316 
#+end_example

Something is still wrong on actual transmitted packet size, so even-though
this slower than baseline, I'm not sure that these XDP_TX results or counter
from the XDP side can be trusted.

Ethtool stats to verify packets are transmitted:
#+begin_example
Show adapter(s) (mlx5p1) statistics (ONLY that changed!)
Ethtool(mlx5p1  ) stat:       233354 (        233,354) <= ch5_poll /sec
Ethtool(mlx5p1  ) stat:       233355 (        233,355) <= ch_poll /sec
Ethtool(mlx5p1  ) stat:          424 (            424) <= rx5_cache_empty /sec
Ethtool(mlx5p1  ) stat:          424 (            424) <= rx5_cache_full /sec
Ethtool(mlx5p1  ) stat:     14934253 (     14,934,253) <= rx5_cache_reuse /sec
Ethtool(mlx5p1  ) stat:      5581066 (      5,581,066) <= rx5_xdp_drop /sec
Ethtool(mlx5p1  ) stat:       146150 (        146,150) <= rx5_xdp_tx_cqes /sec
Ethtool(mlx5p1  ) stat:      5581066 (      5,581,066) <= rx5_xdp_tx_full /sec
Ethtool(mlx5p1  ) stat:      9349435 (      9,349,435) <= rx5_xdp_tx_inlnw /sec
Ethtool(mlx5p1  ) stat:       876639 (        876,639) <= rx5_xdp_tx_mpwqe /sec
Ethtool(mlx5p1  ) stat:       993656 (        993,656) <= rx5_xdp_tx_nops /sec
Ethtool(mlx5p1  ) stat:      9353579 (      9,353,579) <= rx5_xdp_tx_xmit /sec
Ethtool(mlx5p1  ) stat:     44387430 (     44,387,430) <= rx_64_bytes_phy /sec
Ethtool(mlx5p1  ) stat:   2840787378 (  2,840,787,378) <= rx_bytes_phy /sec
Ethtool(mlx5p1  ) stat:          424 (            424) <= rx_cache_empty /sec
Ethtool(mlx5p1  ) stat:          424 (            424) <= rx_cache_full /sec
Ethtool(mlx5p1  ) stat:     14934296 (     14,934,296) <= rx_cache_reuse /sec
Ethtool(mlx5p1  ) stat:     29452362 (     29,452,362) <= rx_out_of_buffer /sec
Ethtool(mlx5p1  ) stat:     44387303 (     44,387,303) <= rx_packets_phy /sec
Ethtool(mlx5p1  ) stat:   2840830503 (  2,840,830,503) <= rx_prio0_bytes /sec
Ethtool(mlx5p1  ) stat:     44387977 (     44,387,977) <= rx_prio0_packets /sec
Ethtool(mlx5p1  ) stat:   2663234382 (  2,663,234,382) <= rx_vport_unicast_bytes /sec
Ethtool(mlx5p1  ) stat:     44387241 (     44,387,241) <= rx_vport_unicast_packets /sec
Ethtool(mlx5p1  ) stat:      5581087 (      5,581,087) <= rx_xdp_drop /sec
Ethtool(mlx5p1  ) stat:       146150 (        146,150) <= rx_xdp_tx_cqe /sec
Ethtool(mlx5p1  ) stat:      5581087 (      5,581,087) <= rx_xdp_tx_full /sec
Ethtool(mlx5p1  ) stat:      9349461 (      9,349,461) <= rx_xdp_tx_inlnw /sec
Ethtool(mlx5p1  ) stat:       876641 (        876,641) <= rx_xdp_tx_mpwqe /sec
Ethtool(mlx5p1  ) stat:       993657 (        993,657) <= rx_xdp_tx_nops /sec
Ethtool(mlx5p1  ) stat:      9353605 (      9,353,605) <= rx_xdp_tx_xmit /sec
Ethtool(mlx5p1  ) stat:    598635096 (    598,635,096) <= tx_bytes_phy /sec
Ethtool(mlx5p1  ) stat:      9353671 (      9,353,671) <= tx_packets_phy /sec
Ethtool(mlx5p1  ) stat:    598644630 (    598,644,630) <= tx_prio0_bytes /sec
Ethtool(mlx5p1  ) stat:      9353822 (      9,353,822) <= tx_prio0_packets /sec
Ethtool(mlx5p1  ) stat:    561220066 (    561,220,066) <= tx_vport_unicast_bytes /sec
Ethtool(mlx5p1  ) stat:      9353664 (      9,353,664) <= tx_vport_unicast_packets /sec
#+end_example

** XDP-redirect bounce

Testing xdp_redirect_map back-out same interface.

#+begin_example
brouer@broadwell kernel-bpf-samples]$ sudo ./xdp_redirect_map mlx5p1 mlx5p1
input: 7 output: 7
libbpf: Kernel error message: XDP program already attached
WARN: link set xdp fd failed on 7
ifindex 7:    9475880 pkt/s
ifindex 7:    9548121 pkt/s
ifindex 7:    9548336 pkt/s
ifindex 7:    9545295 pkt/s
#+end_example

Ethtool stats to verify packets are transmitted:
#+begin_example
Show adapter(s) (mlx5p1) statistics (ONLY that changed!)
Ethtool(mlx5p1  ) stat:       149101 (        149,101) <= ch5_poll /sec
Ethtool(mlx5p1  ) stat:       149101 (        149,101) <= ch_poll /sec
Ethtool(mlx5p1  ) stat:      9542483 (      9,542,483) <= rx5_cache_empty /sec
Ethtool(mlx5p1  ) stat:      9542491 (      9,542,491) <= rx5_xdp_redirect /sec
Ethtool(mlx5p1  ) stat:     44715715 (     44,715,715) <= rx_64_bytes_phy /sec
Ethtool(mlx5p1  ) stat:   2861806193 (  2,861,806,193) <= rx_bytes_phy /sec
Ethtool(mlx5p1  ) stat:      9542483 (      9,542,483) <= rx_cache_empty /sec
Ethtool(mlx5p1  ) stat:     35173205 (     35,173,205) <= rx_out_of_buffer /sec
Ethtool(mlx5p1  ) stat:     44715726 (     44,715,726) <= rx_packets_phy /sec
Ethtool(mlx5p1  ) stat:   2861812704 (  2,861,812,704) <= rx_prio0_bytes /sec
Ethtool(mlx5p1  ) stat:     44715823 (     44,715,823) <= rx_prio0_packets /sec
Ethtool(mlx5p1  ) stat:   2682943266 (  2,682,943,266) <= rx_vport_unicast_bytes /sec
Ethtool(mlx5p1  ) stat:     44715721 (     44,715,721) <= rx_vport_unicast_packets /sec
Ethtool(mlx5p1  ) stat:      9542499 (      9,542,499) <= rx_xdp_redirect /sec
Ethtool(mlx5p1  ) stat:       149102 (        149,102) <= tx5_xdp_cqes /sec
Ethtool(mlx5p1  ) stat:       298203 (        298,203) <= tx5_xdp_mpwqe /sec
Ethtool(mlx5p1  ) stat:       646106 (        646,106) <= tx5_xdp_nops /sec
Ethtool(mlx5p1  ) stat:      9542490 (      9,542,490) <= tx5_xdp_xmit /sec
Ethtool(mlx5p1  ) stat:    610718914 (    610,718,914) <= tx_bytes_phy /sec
Ethtool(mlx5p1  ) stat:      9542483 (      9,542,483) <= tx_packets_phy /sec
Ethtool(mlx5p1  ) stat:    610720267 (    610,720,267) <= tx_prio0_bytes /sec
Ethtool(mlx5p1  ) stat:      9542504 (      9,542,504) <= tx_prio0_packets /sec
Ethtool(mlx5p1  ) stat:    572550251 (    572,550,251) <= tx_vport_unicast_bytes /sec
Ethtool(mlx5p1  ) stat:      9542504 (      9,542,504) <= tx_vport_unicast_packets /sec
Ethtool(mlx5p1  ) stat:       149102 (        149,102) <= tx_xdp_cqes /sec
Ethtool(mlx5p1  ) stat:       298203 (        298,203) <= tx_xdp_mpwqe /sec
Ethtool(mlx5p1  ) stat:       646106 (        646,106) <= tx_xdp_nops /sec
Ethtool(mlx5p1  ) stat:      9542499 (      9,542,499) <= tx_xdp_xmit /sec
#+end_example

Comparing TX-pps at phy level
- Baseline: 8987852 ( 8,987,852) <= tx_packets_phy /sec
- Patchset: 9542483 ( 9,542,483) <= tx_packets_phy /sec


These numbers are lower than I measured before in earlier iterations
(10165419 pkt/s (10,165,419 pps)) of patchset. And non-patched was 8514144
pkt/s (8,514,144 pps).  Something need investing.
