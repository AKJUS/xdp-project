# -*- fill-column: 76; -*-
#+Title: Change page_pool handling in-flight frames during shutdown
#+OPTIONS: ^:nil

This is related to work done in [[file:page_pool03_shutdown_inflight.org]],
which have been accepted in upstream kernels.

As pointed out by Jonathan Lemon, we have to fix the page_pool shutdown
mechanism, when wanting page_pool for SKB-recycling.

* Background - design issue in shutdown

The current page_pool shutdown have a design problem, it assumes that if the
page_pool have pages 'inflight' for more that 120 sec, then it's a bug. It
frees and removes the page_pool object from the mem_id lookup system, and
catch pages returned 'late', does a simple put_page() and leaks the DMA
mapping (note: same code even without page_pool DMA-mapping, to catch this
before we start using it for DMA),

Once we start to use page_pool for SKBs with recycling, then the 120 sec max
time for outstanding 'inflight' packets are bogus. SKBs can sit in a socket
queue for an undetermined amount of time. (Should be easy to reproduce, with
UDP socket without any userspace program consuming packets).

I've been trying to come up with a solution, that does not hurt the
page_pool fast-path. The idea is that we simply have to accept, that the
page_pool object have to stay alive as long as it has 'inflight' pages.
Thus, once a page_pool on 'death-row', the trigger point is pages being
returned, that will check if 'inflight' have reached zero, and free the
object for real.

** Lost Catch-bug-feature

With above design, we also loose one bug catching feature. I've used the
timeout feature to catch bugs, where the 'inflight' count never reach zero,
due to some driver bug, that use page_pool wrong. E.g. can easily happen in
drivers, doing put_page in error handling cases.

Maybe we should discuss this in another thread, but I want us to think about
debugging and troubleshooting tools for page_pool. Due to the performance
requirement, we need to add these kind of facilities without slowing down
normal operation. I've look at using tracepoint to provide troubleshooting
mechanisms, it does introduce a small overhead when activating a tracepoint,
but when not troubleshooting the overhead is zero.  I've started writing
[[https://github.com/xdp-project/xdp-project/blob/master/areas/mem/bpftrace/][bpftrace tools here]], but for now there is only a single tool.

* Upstream patch with issue

The main patch that caused this issue is:
- d956a048cd3f ("xdp: force mem allocator removal and periodic warning")
- link: https://git.kernel.org/torvalds/c/d956a048cd3f

#+begin_example
commit d956a048cd3fc1ba154101a1a50fb37950081ff6
Author: Jesper Dangaard Brouer <brouer@redhat.com>
Date:   Tue Jun 18 15:05:53 2019 +0200

    xdp: force mem allocator removal and periodic warning
    
    If bugs exists or are introduced later e.g. by drivers misusing the API,
    then we want to warn about the issue, such that developer notice. This patch
    will generate a bit of noise in form of periodic pr_warn every 30 seconds.
    
    It is not nice to have this stall warning running forever. Thus, this patch
    will (after 120 attempts) force disconnect the mem id (from the rhashtable)
    and free the page_pool object. This will cause fallback to the put_page() as
    before, which only potentially leak DMA-mappings, if objects are really
    stuck for this long. In that unlikely case, a WARN_ONCE should show us the
    call stack.
    
    Signed-off-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>
#+end_example

** Investigated if we can just revert this.

Assessment is that might be useful to keep around the periodic warning, but
e.g. after 5 minutes.  This can help us spot bugs, where inflight packets
never reach zero, indicating some kind of leak.

Another advantage of having a work-queue to periodically flush the
page_pool, was to avoid adding extra checks to fast-path code, about
page_pool being in a shutdown state.

* Patchset#1 - REJECTED UPSTREAM

*Update*: Patchset rejected upstream.

Instead this patch is being considered:
- Subject: [net-next PATCH v3 1/1] page_pool: do not release pool until inflight == 0.
- https://patchwork.ozlabs.org/project/netdev/list/?series=142975&state=%2a
- https://patchwork.ozlabs.org/patch/1195227/
- Message-ID: <20191114221300.1002982-2-jonathan.lemon@gmail.com>
- http://lore.kernel.org/netdev/20191114221300.1002982-2-jonathan.lemon@gmail.com

** Cover-letter

Subj: Change XDP lifetime guarantees for page_pool objects

This patchset change XDP lifetime guarantees for page_pool objects
(registered via the xdp_rxq_info_reg_mem_model API). The new guarantee
is that page_pool objects stay valid (are not free'ed) until all
inflight pages are returned.

It was commit d956a048cd3f (“xdp: force mem allocator removal and
periodic warning”) that introduce the force removal of page_pool
objects (after 120 second). While working on extending page_pool
recycling to cover SKBs[1], we[2] realised that this force removal
approach was a mistake.

Tested and monitored via bpftrace scripts provide here[3].

[1] https://github.com/xdp-project/xdp-project/tree/master/areas/mem
[2] we == Ilias, Jonathan, Tariq, Saeed and me
[3] https://github.com/xdp-project/xdp-project/tree/master/areas/mem/bpftrace

Note, patches are based on-top of Saeed's page_pool NUMA changes.

*** stg mail

Commands used for stg-mail:

#+begin_src sh
stg mail --prefix="net-next v1" -e --cc meup \
 --cc netdev \
 --cc lemon --cc ilias --cc tariq --cc saeed \
 --cc tokerh --cc matteo --cc lore \
 xdp-revert-forced-mem..block-alloc-cache
#+end_src

** Patch1: xdp: revert forced mem allocator removal for page_pool

Forced removal of XDP mem allocator, specifically related to page_pool,
turned out to be a wrong approach. Special thanks to Jonathan Lemon for
convincing me. This patch is revert the force part of commit d956a048cd3f
(“xdp: force mem allocator removal and periodic warning”).

It is much better to provide a guarantee that page_pool object stays valid
until 'inflight' pages reach zero, making it safe to remove.

We keep the periodic warning via a work-queue, but increased interval to
5-minutes. The reason is to have a way to catch bugs, where inflight
pages/packets never reach zero, indicating some kind of leak. These kind of
bugs have been observed while converting drivers over to use page_pool API.

Details on when to crash the kernel. If page_pool API is misused and
somehow __page_pool_free() is invoked while there are still inflight
frames, then (like before) a WARN() is triggered and not a BUG(). This can
potentially lead to use-after-free, which we try to catch via poisoning the
page_pool object memory with some NULL pointers. Doing it this way,
pinpoint both the driver (likely) prematurely freeing page_pool via WARN(),
and crash-dump for inflight page/packet show who to blame for late return.

Fixes: d956a048cd3f (“xdp: force mem allocator removal and periodic warning”)

** patch2: page_pool: make inflight returns more robust via blocking alloc cache

When requesting page_pool shutdown, it's a requirement that consumer
RX-side have been disconnected, but __page_pool_request_shutdown()
have a loop that empty RX alloc cache each time. Producers can still
be inflight, but they MUST NOT return pages into RX alloc cache. Thus,
the RX alloc cache MUST remain empty after the first clearing, else it
is considered a bug. Lets make it more clear this is only cleared once.

This patch only empty RX alloc cache once and then block alloc cache.
The alloc cache is blocked via pretending it is full, and then also
poisoning the last element. This blocks producers from using fast-path,
and consumer (which is not allowed) will see a NULL pointer.

** Potential bug?

*Update*: below issue should be addressed in patch2.

I think, I might have spotted a potential bug in the shutdown phase, or at
least something that can be made more safe/robust during shutdown.

During page_pool shutdown it is a requirement that driver alloc RX-side have
been disconnected, to make sure it cannot consume from the alloc cache. This
allows shutdown to flush the alloc cache.

During page_pool shutdown pages can still be inflight. This means producers
can still be returning pages to the page_pool object. All the producers left
MUST not use the 'allow_direct' flag (call __page_pool_recycle_direct). If
some driver broke that rule, it can result in very difficult to catch bugs.

One way to avoid this during shutdown, is to block the alloc cache by (after
flushing it) pretend that it is full, by setting pool->alloc.count equal to
max size PP_ALLOC_CACHE_SIZE.  Further more we can poison last entry to
catch users still allocating from it.



* Notes

** Performance notes

Result with patchset + Saeed NUMA changes + mlx5-cache-removal-patch
#+begin_src sh
[broadwell kernel-bpf-samples]$ sudo ./xdp_rxq_info --dev mlx5p1 --action XDP_DROP
[...]
Running XDP on dev:mlx5p1 (ifindex:7) action:XDP_DROP options:no_touch
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      2       22,188,213  0          
XDP-RX CPU      total   22,188,213 

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    2:2   22,188,216  0          
rx_queue_index    2:sum 22,188,216 
^CInterrupted: Removing XDP program on ifindex:7 device:mlx5p1
#+end_src

Result with only described patches:
#+begin_src sh
Running XDP on dev:mlx5p1 (ifindex:7) action:XDP_DROP options:no_touch
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      5       22,901,924  0          
XDP-RX CPU      total   22,901,924 

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    1:5   22,901,924  0          
rx_queue_index    1:sum 22,901,924 
#+end_src

Calc: (1/22901924-1/22188213)*10^9 = -1.4045 ns


Result with only described patches + mlx5-cache-removal-patch:
#+begin_src sh
Running XDP on dev:mlx5p1 (ifindex:7) action:XDP_DROP options:no_touch
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      4       22,359,831  0          
XDP-RX CPU      total   22,359,831 

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    3:4   22,359,842  0          
rx_queue_index    3:sum 22,359,842 
#+end_src

Calc: (1/22359831-1/22188213)*10^9 = -0.3459 ns

** bpftrace measurement tool output

Repeating test of patchset and monitor that the testcase of activating
work-queue does happen. The specific test run is xdp_redirect_map from
mlx5 (100G) into ixgbe (10G) driver, and causing an error case, where
ixgbe driver is repeatably resetting itself, causing it to flush its
TX queue slightly later than mlx5 driver unregister the page_pool,
when unloading XDP program.

The output is from script: [[file:bpftrace/xdp_mem_track02.bt][xdp_mem_track02.bt]]
- in https://github.com/xdp-project/xdp-project/tree/master/areas/mem/bpftrace

#+begin_example
18:32:47
tracepoint:xdp:mem_disconnect: type:PAGE_POOL mem.id:19 0xFFFF88881DAEF800 safe_to_remove:1 (lived 8732 ms)
18:32:47
tracepoint:xdp:mem_disconnect: type:PAGE_POOL mem.id:20 0xFFFF88881DAE9800 safe_to_remove:1 (lived 8732 ms)
18:32:47
tracepoint:xdp:mem_disconnect: type:PAGE_POOL mem.id:21 0xFFFF88881DAEA000 safe_to_remove:1 (lived 8732 ms)
18:32:47
tracepoint:xdp:mem_disconnect: type:PAGE_POOL mem.id:22 0xFFFF88881DAE9000 safe_to_remove:1 (lived 8732 ms)
18:32:47
tracepoint:xdp:mem_disconnect: type:PAGE_POOL mem.id:23 0xFFFF888818AE5000 safe_to_remove:1 (lived 8732 ms)
18:32:47
tracepoint:xdp:mem_disconnect: type:PAGE_POOL mem.id:24 0xFFFF8888514F8000 safe_to_remove:1 (lived 8732 ms)
18:32:59
tracepoint:xdp:mem_connect: type:PAGE_POOL mem.id:31 0xFFFF88882BD70000
18:32:59
tracepoint:xdp:mem_connect: type:PAGE_POOL mem.id:32 0xFFFF88887D836800
18:32:59
tracepoint:xdp:mem_connect: type:PAGE_POOL mem.id:33 0xFFFF88887C8A5800
18:32:59
tracepoint:xdp:mem_connect: type:PAGE_POOL mem.id:34 0xFFFF88884F873000
18:32:59
tracepoint:xdp:mem_connect: type:PAGE_POOL mem.id:35 0xFFFF88884F870800
18:32:59
tracepoint:xdp:mem_connect: type:PAGE_POOL mem.id:36 0xFFFF88884F870000
18:32:59
tracepoint:xdp:mem_disconnect: type:PAGE_POOL mem.id:25 0xFFFF88881A8D7800 safe_to_remove:1 (lived 12105 ms)
18:32:59
tracepoint:xdp:mem_disconnect: type:PAGE_POOL mem.id:26 0xFFFF88881A8D4000 safe_to_remove:1 (lived 12105 ms)
18:32:59
tracepoint:xdp:mem_disconnect: type:PAGE_POOL mem.id:27 0xFFFF88881A8D2800 safe_to_remove:1 (lived 12105 ms)
tracepoint:page_pool:page_pool_inflight: pp:0xFFFF888848D59800 inflight:511 (hold:1982 release:1471)
18:32:59
tracepoint:xdp:mem_disconnect: type:PAGE_POOL mem.id=28 NOT-safe_to_remove:0 (*HIT*)
tracepoint:xdp:mem_disconnect: type:PAGE_POOL mem.id=28 inflight:511 (hold:1982 release:1471)
tracepoint:xdp:mem_disconnect: type:PAGE_POOL mem.id:28 0xFFFF888848D59800 safe_to_remove:0 (lived 12106 ms)
18:32:59
tracepoint:xdp:mem_disconnect: type:PAGE_POOL mem.id:29 0xFFFF888848D5E000 safe_to_remove:1 (lived 12106 ms)
18:32:59
tracepoint:xdp:mem_disconnect: type:PAGE_POOL mem.id:30 0xFFFF888848D5A000 safe_to_remove:1 (lived 12107 ms)
18:33:00
tracepoint:xdp:mem_disconnect: type:PAGE_POOL mem.id=28 re-sched(2) safe_to_remove:1
tracepoint:xdp:mem_disconnect: type:PAGE_POOL mem.id:28 0xFFFF888848D59800 safe_to_remove:1 (lived 13125 ms)
^C

@active_mem_ids[31]: 262480223701
@active_mem_ids[32]: 262482165027
@active_mem_ids[33]: 262483959614
@active_mem_ids[34]: 262485769530
@active_mem_ids[35]: 262487541744
@active_mem_ids[36]: 262489362487

@pp_stat_hold[-131356057823232]: 1982

@pp_stat_inflight[-131356057823232]: 511

@pp_stat_release[-131356057823232]: 1471

@test_case_hit[28, 1, 12106]: 511
#+end_example
