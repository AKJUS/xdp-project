# -*- fill-column: 76; -*-
#+TITLE: Test cpumap running 2nd XDP-prog on remote CPU
#+CATEGORY: CPUMAP
#+OPTIONS: ^:nil

Work-in-progress patches upstream by Lorenzo and Jesper, are adding ability
to run another (2nd) XDP-prog on the remote CPU the XDP packets is getting
redirected to.

The 2nd XDP-program is installed via inserting its FD into the map.

* Testing git tree

Kernel git tree under testing
- https://github.com/LorenzoBianconi/bpf-next/
- branch: cpu_map_program_v9

Local adjustments
- branch: cpu_map_program_v9.jesper_adjust02.stgit

* Testlab machine

The testlab machine:
- Intel CPU E5-1650 v4 @ 3.60GHz
- Disabled HT (HyperThreading)
- Fedora 31

* Baseline benchmarks RX-CPU

The processing and (deliberate) packet drops happens on same CPU as packet
was RX-ed on, which have many cache advantages.

Two drivers: i40e and mlx5 because they have two memory models.
- i40e: refcnt pages, recycle depend on < 512 outstanding pages
- mlx5: page_pool based, recycle works for XDP_DROP

Kernel:
- Linux broadwell 5.8.0-rc1-bpf-next-lorenzo+ #9 SMP PREEMPT
- Contains cpumap changes, but okay as that code isn't active here

** Baseline: RX-NAPI CPU handle (driver: i40e)

*** baseline(i40e): UdpNoPorts: 3,649,402 pps

No listen-ing UDP service.
No iptables.

#+begin_example
$ nstat -n && sleep 1 && nstat
#kernel
IpInReceives                    3649400            0.0
IpInDelivers                    3649397            0.0
IpOutRequests                   1                  0.0
IcmpOutMsgs                     1                  0.0
IcmpOutDestUnreachs             1                  0.0
IcmpMsgOutType3                 1                  0.0
UdpNoPorts                      3649402            0.0
IpExtInOctets                   167868398          0.0
IpExtOutOctets                  74                 0.0
IpExtInNoECTPkts                3649314            0.0
#+end_example

*** baseline(i40e): iptables-raw drop: 4,727,128 pps (GRO-enabled)

Command used to drop packets:
- iptables -t raw -I PREROUTING -p udp --dport 9 -j DROP

#+begin_example
$ nstat -n && sleep 1 && nstat
#kernel
IpInReceives                    4727128            0.0
IpExtInOctets                   217450096          0.0
IpExtInNoECTPkts                4727176            0.0
#+end_example

Command to disable GRO:
- ethtool -K i40e2 gro off tso off

#+begin_example
$ ethtool -K i40e2 gro off tso off
$ nstat -n && sleep 1 && nstat
#kernel
IpInReceives                    5596808            0.0
IpExtInOctets                   257453030          0.0
IpExtInNoECTPkts                5596800            0.0
#+end_example


** Baseline: RX-NAPI CPU handle (driver: mlx5)

The mlx5 drivers memory model is special and combines refcnt and page_pool
system for recycling. It have a 128 (per queue) page recycle cache, before
the page_pool. When XDP is NOT loaded, it still allocate via page_pool, but
the pages use a split-model with two packets per page with refcnt to
determine recycle-ability. When XDP gets loaded it uses one packet per page,
but still tries to do refcnt recycling towards network stack.

*** baseline(mlx5): UdpNoPorts: 3,548,400 pps

#+begin_example
$ nstat -n && sleep 1 && nstat
#kernel
IpInReceives                    3548400            0.0
IpInDelivers                    3548403            0.0
IpOutRequests                   1                  0.0
IcmpOutMsgs                     1                  0.0
IcmpOutDestUnreachs             1                  0.0
IcmpMsgOutType3                 1                  0.0
UdpNoPorts                      3548400            0.0
IpExtInOctets                   163227826          0.0
IpExtOutOctets                  74                 0.0
IpExtInNoECTPkts                3548432            0.0
#+end_example

*** baseline(mlx5): iptables-raw drop: 4,484,640 pps (GRO-enabled)

Command used to drop packets:
- iptables -t raw -I PREROUTING -p udp --dport 9 -j DROP

#+begin_example
$ nstat -n && sleep 1 && nstat
#kernel
IpInReceives                    4484640            0.0
IpExtInOctets                   206293440          0.0
IpExtInNoECTPkts                4484640            0.0
#+end_example

ethtool_stats showing cache_reuse counters:
#+begin_example
$ ethtool_stats.pl --dev mlx5p1 --sec 2

Show adapter(s) (mlx5p1) statistics (ONLY that changed!)
Ethtool(mlx5p1  ) stat:        69667 (         69,667) <= ch2_poll /sec
Ethtool(mlx5p1  ) stat:        69667 (         69,667) <= ch_poll /sec
Ethtool(mlx5p1  ) stat:    267522383 (    267,522,383) <= rx2_bytes /sec
Ethtool(mlx5p1  ) stat:      2229360 (      2,229,360) <= rx2_cache_reuse /sec
Ethtool(mlx5p1  ) stat:      4458706 (      4,458,706) <= rx2_csum_unnecessary /sec
Ethtool(mlx5p1  ) stat:      4458706 (      4,458,706) <= rx2_packets /sec
Ethtool(mlx5p1  ) stat:     44978045 (     44,978,045) <= rx_64_bytes_phy /sec
Ethtool(mlx5p1  ) stat:    267522236 (    267,522,236) <= rx_bytes /sec
Ethtool(mlx5p1  ) stat:   2878598428 (  2,878,598,428) <= rx_bytes_phy /sec
Ethtool(mlx5p1  ) stat:      2229360 (      2,229,360) <= rx_cache_reuse /sec
Ethtool(mlx5p1  ) stat:      4458704 (      4,458,704) <= rx_csum_unnecessary /sec
Ethtool(mlx5p1  ) stat:     40519382 (     40,519,382) <= rx_out_of_buffer /sec
Ethtool(mlx5p1  ) stat:      4458704 (      4,458,704) <= rx_packets /sec
Ethtool(mlx5p1  ) stat:     44978101 (     44,978,101) <= rx_packets_phy /sec
Ethtool(mlx5p1  ) stat:   2878595049 (  2,878,595,049) <= rx_prio0_bytes /sec
Ethtool(mlx5p1  ) stat:     44978045 (     44,978,045) <= rx_prio0_packets /sec
Ethtool(mlx5p1  ) stat:   2698685498 (  2,698,685,498) <= rx_vport_unicast_bytes /sec
Ethtool(mlx5p1  ) stat:     44978090 (     44,978,090) <= rx_vport_unicast_packets /sec
#+end_example

Command to disable GRO:
- ethtool -K mlx5p1 gro off tso off

#+begin_example
$ nstat -n && sleep 1 && nstat
#kernel
IpInReceives                    5288656            0.0
IpExtInOctets                   243278498          0.0
IpExtInNoECTPkts                5288664            0.0
#+end_example

* Testing patchset on driver i40e`

** i40e qsize adjustment (64)

The i40e driver (as mentioned) uses a refcnt based recycle scheme, that
depend on depend on < 512 outstanding pages. The default queue size (between
the CPUs) in CPUMAP program =xdp_redirect_cpu= (from =samples/bpf/=) is 192
packets, which cause the i40e drivers recycle scheme to fail. This cause
pages to go-through the page-allocator, which causes a significant slowdown.

Changing queue size to 64 (=--qsize=64=) seems to allow recycle to work.
Thus, using this in below tests for i40e driver.

Example with qsize=192:
#+begin_example
$ sudo ./xdp_redirect_cpu --dev i40e2 --qsize 192 --cpu 4 --prog xdp_cpu_map0

unning XDP/eBPF prog_name:xdp_cpu_map0
XDP-cpumap      CPU:to  pps            drop-pps    extra-info
XDP-RX          2       13,292,641     0           0          
XDP-RX          total   13,292,641     0          
cpumap-enqueue    2:4   13,292,647     9,838,519   8.00       bulk-average
cpumap-enqueue  sum:4   13,292,647     9,838,519   8.00       bulk-average
cpumap_kthread  4       3,454,127      0           0          
cpumap_kthread  total   3,454,127      0           0          
redirect_err    total   0              0          
xdp_exception   total   0              0          

2nd remote XDP/eBPF prog_name: xdp_redirect_dummy
XDP-cpumap      CPU:to  xdp-pass       xdp-drop    xdp-redir
xdp-in-kthread  4       3,454,128      0           0         
xdp-in-kthread  total   3,454,128      0           0         
#+end_example

Unfortunately ethtool stats does not show that recycling are failing:
#+begin_example
Show adapter(s) (i40e2) statistics (ONLY that changed!)
Ethtool(i40e2   ) stat:   2920468143 (  2,920,468,143) <= port.rx_bytes /sec
Ethtool(i40e2   ) stat:     11907326 (     11,907,326) <= port.rx_dropped /sec
Ethtool(i40e2   ) stat:     45632337 (     45,632,337) <= port.rx_size_64 /sec
Ethtool(i40e2   ) stat:     45632326 (     45,632,326) <= port.rx_unicast /sec
Ethtool(i40e2   ) stat:           91 (             91) <= port.tx_bytes /sec
Ethtool(i40e2   ) stat:            1 (              1) <= port.tx_size_127 /sec
Ethtool(i40e2   ) stat:            1 (              1) <= port.tx_unicast /sec
Ethtool(i40e2   ) stat:    795753110 (    795,753,110) <= rx-2.bytes /sec
Ethtool(i40e2   ) stat:     13262552 (     13,262,552) <= rx-2.packets /sec
Ethtool(i40e2   ) stat:     20462471 (     20,462,471) <= rx_dropped /sec
Ethtool(i40e2   ) stat:     33725009 (     33,725,009) <= rx_unicast /sec
Ethtool(i40e2   ) stat:           87 (             87) <= tx-4.bytes /sec
Ethtool(i40e2   ) stat:            1 (              1) <= tx-4.packets /sec
Ethtool(i40e2   ) stat:           87 (             87) <= tx_bytes /sec
Ethtool(i40e2   ) stat:            1 (              1) <= tx_packets /sec
Ethtool(i40e2   ) stat:            1 (              1) <= tx_unicast /sec
#+end_example

Example with qsize=64:
#+begin_example
 sudo ./xdp_redirect_cpu --dev i40e2 --qsize 64 --cpu 4 --prog xdp_cpu_map0
Running XDP/eBPF prog_name:xdp_cpu_map0
XDP-cpumap      CPU:to  pps            drop-pps    extra-info
XDP-RX          2       17,809,657     0           0          
XDP-RX          total   17,809,657     0          
cpumap-enqueue    2:4   17,809,652     13,713,438  8.00       bulk-average
cpumap-enqueue  sum:4   17,809,652     13,713,438  8.00       bulk-average
cpumap_kthread  4       4,096,217      0           0          
cpumap_kthread  total   4,096,217      0           0          
redirect_err    total   0              0          
xdp_exception   total   0              0          

2nd remote XDP/eBPF prog_name: xdp_redirect_dummy
XDP-cpumap      CPU:to  xdp-pass       xdp-drop    xdp-redir
xdp-in-kthread  4       4,096,218      0           0         
xdp-in-kthread  total   4,096,218      0           0       
#+end_example

Calculate slowdown:
 - (1/3454128-1/4096217)*10^9 = 45.38 ns

** CPU-redirect (i40e): UdpNoPorts: 4,102,929 pps

BPF-prog command used:
#+begin_src sh
sudo ./xdp_redirect_cpu --dev i40e2 --qsize 64 --cpu 4 --prog xdp_cpu_map0
#+end_src

The xdp_redirect_dummy program running as 2nd XDP-prog in kthread does
nothing and returns =XDP_PASS=.

#+begin_example
unning XDP/eBPF prog_name:xdp_cpu_map0
XDP-cpumap      CPU:to  pps            drop-pps    extra-info
XDP-RX          2       17,767,786     0           0          
kXDP-RX          total   17,767,787     0          
cpumap-enqueue    2:4   17,767,785     13,664,852  8.00       bulk-average
cpumap-enqueue  sum:4   17,767,786     13,664,853  8.00       bulk-average
cpumap_kthread  4       4,102,929      0           0          
cpumap_kthread  total   4,102,929      0           0          
redirect_err    total   0              0          
xdp_exception   total   0              0          

2nd remote XDP/eBPF prog_name: xdp_redirect_dummy
XDP-cpumap      CPU:to  xdp-pass       xdp-drop    xdp-redir
xdp-in-kthread  4       4,102,930      0           0         
xdp-in-kthread  total   4,102,930      0           0         
#+end_example

#+begin_example
$ nstat -n && sleep 1 && nstat
#kernel
IpInReceives                    4118695            0.0
IpInDelivers                    4118696            0.0
IpOutRequests                   1                  0.0
IcmpOutMsgs                     1                  0.0
IcmpOutDestUnreachs             1                  0.0
IcmpMsgOutType3                 1                  0.0
UdpNoPorts                      4118694            0.0
IpExtInOctets                   189459786          0.0
IpExtOutOctets                  74                 0.0
IpExtInNoECTPkts                4118691            0.0
#+end_example

Disabling loading the "mprog" change the performance a bit
- From: 4,102,929 pps
- To  : 4,202,953 pps
- Diff:  +100,024 pps
- Diff: (1/4102929-1/4202953)*10^9 = 5.8 ns

It is actually surprisingly little overhead, 5.8 nanosec, to run the
XDP-prog on the remote/target CPU.

#+begin_example
sudo ./xdp_redirect_cpu --dev i40e2 --qsize 64 --cpu 4 --prog xdp_cpu_map0 --mprog-disable

Running XDP/eBPF prog_name:xdp_cpu_map0
XDP-cpumap      CPU:to  pps            drop-pps    extra-info
XDP-RX          2       17,730,736     0           0          
XDP-RX          total   17,730,736     0          
cpumap-enqueue    2:4   17,730,742     13,527,783  8.00       bulk-average
cpumap-enqueue  sum:4   17,730,742     13,527,783  8.00       bulk-average
cpumap_kthread  4       4,202,953      0           0          
cpumap_kthread  total   4,202,953      0           0          
redirect_err    total   0              0          
xdp_exception   total   0              0          
#+end_example




** CPU-redirect (i40e): iptables-raw drop: 7,004,219 pps

Command used to drop packets:
- iptables -t raw -I PREROUTING -p udp --dport 9 -j DROP

CPU-redirect command:
#+begin_example
sudo ./xdp_redirect_cpu --dev i40e2 --qsize 64 --cpu 4 --prog xdp_cpu_map0
#+end_example

Notice the result is very impressive compared to RX-CPU raw-drop:
- 4,727,128 pps - baseline(i40e): iptables-raw drop
- 7,004,219 pps - this test: iptables-raw drop on remote CPU
- Diff +2,277,092 pps
- Diff (1/4727128-1/7004220)*10^9 = 68.77 ns

#+begin_example
Running XDP/eBPF prog_name:xdp_cpu_map0
XDP-cpumap      CPU:to  pps            drop-pps    extra-info
XDP-RX          2       17,717,224     0           0          
XDP-RX          total   17,717,224     0          
cpumap-enqueue    2:4   17,717,226     10,713,002  8.00       bulk-average
cpumap-enqueue  sum:4   17,717,226     10,713,002  8.00       bulk-average
cpumap_kthread  4       7,004,219      0           0          
cpumap_kthread  total   7,004,219      0           0          
redirect_err    total   0              0          
xdp_exception   total   0              0          

2nd remote XDP/eBPF prog_name: xdp_redirect_dummy
XDP-cpumap      CPU:to  xdp-pass       xdp-drop    xdp-redir
xdp-in-kthread  4       7,004,220      0           0         
xdp-in-kthread  total   7,004,220      0           0         
#+end_example

With disabled mprog:
#+begin_example
Running XDP/eBPF prog_name:xdp_cpu_map0
XDP-cpumap      CPU:to  pps            drop-pps    extra-info
XDP-RX          2       17,861,630     0           0          
XDP-RX          total   17,861,630     0          
cpumap-enqueue    2:4   17,861,631     10,731,216  8.00       bulk-average
cpumap-enqueue  sum:4   17,861,631     10,731,216  8.00       bulk-average
cpumap_kthread  4       7,130,415      0           0          
cpumap_kthread  total   7,130,415      0           0          
redirect_err    total   0              0          
xdp_exception   total   0     
#+end_example

Diff vs mprog:
- (7130415-7004220) = 126195 pps
- (1/7130415-1/7004220)*10^9 = -2.53 ns

*** Touch data on RX-CPU + iptables-raw drop

Using prog =prog_name:xdp_cpu_map1_touch_data= we can force RX-CPU to touch
payload, as this will show cost of moving these cache-lines across the CPUs.

XDP-redirect command:
#+begin_example
sudo ./xdp_redirect_cpu --dev i40e2 --qsize 64 --cpu 4 --prog xdp_cpu_map1_touch_data
#+end_example

Output:
#+begin_example
Running XDP/eBPF prog_name:xdp_cpu_map1_touch_data
XDP-cpumap      CPU:to  pps            drop-pps    extra-info
XDP-RX          2       17,220,167     0           0          
XDP-RX          total   17,220,167     0          
cpumap-enqueue    2:4   17,220,165     10,748,391  8.00       bulk-average
cpumap-enqueue  sum:4   17,220,165     10,748,391  8.00       bulk-average
cpumap_kthread  4       6,471,781      0           0          
cpumap_kthread  total   6,471,781      0           0          
redirect_err    total   0              0          
xdp_exception   total   0              0          

2nd remote XDP/eBPF prog_name: xdp_redirect_dummy
XDP-cpumap      CPU:to  xdp-pass       xdp-drop    xdp-redir
xdp-in-kthread  4       6,471,781      0           0         
xdp-in-kthread  total   6,471,781      0           0         
#+end_example

Compared against: 7,004,220 pps
 - (6471781-7004220) =  -532439 pps
 - (1/6471781-1/7004220)*10^9 = 11.75 ns

*** RX-CPU do hashing of packets + iptables-raw drop

Do a full parsing of the packet and calculate a hash in RX CPU.

XDP-redirect command:
#+begin_example
sudo ./xdp_redirect_cpu --dev i40e2 --qsize 64 --cpu 4 \
 --prog xdp_cpu_map5_lb_hash_ip_pairs
#+end_example

Output:
#+begin_example
Running XDP/eBPF prog_name:xdp_cpu_map5_lb_hash_ip_pairs
XDP-cpumap      CPU:to  pps            drop-pps    extra-info
XDP-RX          2       12,740,194     0           0          
XDP-RX          total   12,740,194     0          
cpumap-enqueue    2:4   12,740,190     6,274,416   8.00       bulk-average
cpumap-enqueue  sum:4   12,740,190     6,274,416   8.00       bulk-average
cpumap_kthread  4       6,465,781      0           0          
cpumap_kthread  total   6,465,781      0           0          
redirect_err    total   0              0          
xdp_exception   total   0              0          

2nd remote XDP/eBPF prog_name: xdp_redirect_dummy
XDP-cpumap      CPU:to  xdp-pass       xdp-drop    xdp-redir
xdp-in-kthread  4       6,465,782      0           0         
xdp-in-kthread  total   6,465,782      0           0         
#+end_example

There is almost no performance change on target-CPU running =cpumap_kthread=.

The XDP-RX CPU performance is reduced significant:
- From: 17,220,167 pps 
- To  : 12,740,190 pps

But it doesn't really matter, as the processing capacity on target/remote
CPU is the bottleneck anyhow.  Thus, we have cycles to spare on RX-CPU.
