# -*- fill-column: 79; -*-
#+TITLE: Testing Hellwig "dma-direct-calls" patchset

Christoph Hellwig <hch@lst.de> is helping out addressing/mitigating the
RETPOLINE overhead, which XDP suffers under.

* Patchset avail here

Gitweb: on branch "dma-direct-calls"
 - http://git.infradead.org/users/hch/misc.git/shortlog/refs/heads/dma-direct-calls

Git checkout procedure:
#+BEGIN_EXAMPLE
$ git clone git://git.infradead.org/users/hch/misc.git hellwig
$ cd hellwig
$ git checkout dma-direct-calls
#+END_EXAMPLE

* General notes

Even if removing/mitigating all DMA indirect calls, then there are still some
indirect calls, that we cannot avoid:
 - 1. For every packet: Indirect eBPF XDP prog call
 - 2. For every 16th packet: net_device->ndo_xdp_xmit
 - 3. For every 64th packet: NAPI net_rx_action call drivers napi_poll funcptr


* Summary of results

Using XDP_REDIRECT between drivers RX ixgbe(10G) redirect TX i40e(40G),
via BPF devmap (used samples/bpf/xdp_redirect_map) . (Note choose
higher TX link-speed to assure that we don't to have a TX bottleneck).
The baseline-kernel is at commit [[https://git.kernel.org/torvalds/c/ef78e5ec9214][ef78e5ec9214]], which is commit just
before Hellwigs changes in this tree.

Performance numbers in packets/sec (XDP_REDIRECT ixgbe -> i40e):
 - 11913154 (11,913,154) pps - baseline compiled without retpoline
 -  7438283  (7,438,283) pps - regression due to CONFIG_RETPOLINE
 -  9610088  (9,610,088) pps - mitigation via Hellwig dma-direct-calls
 - 10049223 (10,049,223) pps - Hellwig branch dma-direct-calls.2 + RETPOLINE
 - 11762603 (11,762,603) pps - Hellwig branch dma-direct-calls.2 but no-retpoline

Do notice at these extreme speeds the pps number increase rabbit with
small changes, e.g. difference to new branch is:
 - (1/9610088-1/10049223)*10^9 = 4.54 nanosec faster
 - Diff: (1/11913154-1/10049223)*10^9 = -15.57 nanosec
 - Diff: (1/11913154-1/11761275)*10^9 =  -1.08 nanosec

From the inst per cycle, it is clear that retpolines are stalling the CPU
pipeline:

| pps        | insn per cycle |
|------------+----------------|
| 11,913,154 |           2.39 |
| 7,438,283  |           1.54 |
| 9,610,088  |           2.04 |
| 10,049,223 |           1.99 |
| 11,762,603 |           2.30 |
|            |                |


Strangely the Instruction-Cache is also under heavier pressure:

| pps        | l2_rqsts.all_code_rd | l2_rqsts.code_rd_hit | l2_rqsts.code_rd_miss |
|------------+----------------------+----------------------+-----------------------|
| 11,913,154 | 874,547              | 742,335              | 132,198               |
| 7,438,283  | 649,513              | 547,581              | 101,945               |
| 9,610,088  | 2,568,064            | 2,001,369            | 566,683               |
| 10,049,223 | 1,232,818            | 1,152,514            | 80,299                |
| 11,762,603 | 1,383,597            | 1,148,387            | 235,186               |
|            |                      |                      |                       |


* Testing Hellwig branch "dma-direct-calls"

Testing with Hellwig's branch "dma-direct-calls"
 - 4.20.0-rc4-hellwig-dma-direct+

** Driver ixgbe

*** ixgbe: XDP_DROP branch "dma-direct-calls"

XDP_DROP performance okay, but AFAICR it was also okay before, as it was only
affected by DMA-sync call.

#+BEGIN_EXAMPLE
Running XDP on dev:ixgbe2 (ifindex:9) action:XDP_DROP options:no_touch
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      3       14,435,891  0          
XDP-RX CPU      total   14,435,891 

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    3:3   14,435,902  0          
rx_queue_index    3:sum 14,435,902 
#+END_EXAMPLE

*** ixgbe: XDP_REDIRECT devmap

XDP redirect (via devmap) RX:ixgbe2 -> TX:i40e1, this is redirect from RX
link-speed 10Gbit/s out TXC 40Gbit/s link-speed to make sure TX side is not
bottleneck.

#+BEGIN_EXAMPLE
sudo ./xdp_redirect_map  $(</sys/class/net/ixgbe2/ifindex) \
  $(</sys/class/net/i40e1/ifindex)
$ sudo ./xdp_redirect_map  $(</sys/class/net/ixgbe2/ifindex) \
                           $(</sys/class/net/i40e1/ifindex)
input: 9 output: 2
map[0] (vports) = 4, map[1] (map) = 5, map[2] (count) = 0
ifindex 2:    4153765 pkt/s
ifindex 2:    9618042 pkt/s
ifindex 2:    9614312 pkt/s
#+END_EXAMPLE

Notice we cannot trust the output from =xdp_redirect_map=, as it only measures
XDP-RX packets, it doesn't know if packets gets dropped. Thus, measure this via
ethtool stats counters and program [[https://github.com/netoptimizer/network-testing/blob/master/bin/ethtool_stats.pl][ethtool_stats.pl]], which also use highres
timers to get correct time interval.

First *ALWAYS* make sure generator is sending fast enough:
#+BEGIN_EXAMPLE
./pktgen_sample03_burst_single_flow.sh -i ixgbe2 -d 10.10.10.2 -m 00:1b:21:bb:9a:86 -t2
[generator ~]$ ~/git/network-testing/bin/ethtool_stats.pl --sec 2 --dev ixgbe2
Show adapter(s) (ixgbe2) statistics (ONLY that changed!)
Ethtool(ixgbe2  ) stat:    892843312 (    892,843,312) <= tx_bytes /sec
Ethtool(ixgbe2  ) stat:    952366643 (    952,366,643) <= tx_bytes_nic /sec
Ethtool(ixgbe2  ) stat:     14880722 (     14,880,722) <= tx_packets /sec
Ethtool(ixgbe2  ) stat:     14880725 (     14,880,725) <= tx_pkts_nic /sec
Ethtool(ixgbe2  ) stat:    446544842 (    446,544,842) <= tx_queue_0_bytes /sec
Ethtool(ixgbe2  ) stat:      7442414 (      7,442,414) <= tx_queue_0_packets /sec
Ethtool(ixgbe2  ) stat:    446298470 (    446,298,470) <= tx_queue_1_bytes /sec
Ethtool(ixgbe2  ) stat:      7438308 (      7,438,308) <= tx_queue_1_packets /sec
#+END_EXAMPLE

Device-Under-Test (DUT):
#+BEGIN_EXAMPLE
Show adapter(s) (ixgbe2 i40e1) statistics (ONLY that changed!)
Ethtool(ixgbe2  ) stat:     12192478 (     12,192,478) <= fdir_miss /sec
Ethtool(ixgbe2  ) stat:    575904896 (    575,904,896) <= rx_bytes /sec
Ethtool(ixgbe2  ) stat:    952519117 (    952,519,117) <= rx_bytes_nic /sec
Ethtool(ixgbe2  ) stat:      2970896 (      2,970,896) <= rx_missed_errors /sec
Ethtool(ixgbe2  ) stat:      2313797 (      2,313,797) <= rx_no_dma_resources /sec
Ethtool(ixgbe2  ) stat:      9598415 (      9,598,415) <= rx_packets /sec
Ethtool(ixgbe2  ) stat:     11912216 (     11,912,216) <= rx_pkts_nic /sec
Ethtool(ixgbe2  ) stat:    575904896 (    575,904,896) <= rx_queue_3_bytes /sec
Ethtool(ixgbe2  ) stat:      9598415 (      9,598,415) <= rx_queue_3_packets /sec
Ethtool(i40e1   ) stat:    615042613 (    615,042,613) <= port.tx_bytes /sec
Ethtool(i40e1   ) stat:      9610060 (      9,610,060) <= port.tx_size_64 /sec
Ethtool(i40e1   ) stat:      9610058 (      9,610,058) <= port.tx_unicast /sec
Ethtool(i40e1   ) stat:    576603475 (    576,603,475) <= rx_bytes /sec
Ethtool(i40e1   ) stat:      9610058 (      9,610,058) <= rx_packets /sec
Ethtool(i40e1   ) stat:      9610088 (      9,610,088) <= tx_unicast /sec
#+END_EXAMPLE

The result: 9610088 (9,610,088) packets/sec

Some perf stats during this redirect (happend to run on CPU-3):
#+BEGIN_EXAMPLE
$ sudo ~/perf stat -C3 -e cycles -e  instructions -e cache-references -e cache-misses -e branches:k -e branch-misses:k -e l2_rqsts.all_code_rd -e l2_rqsts.code_rd_hit -e l2_rqsts.code_rd_miss -r 4 sleep 1

 Performance counter stats for 'CPU(s) 3' (4 runs):

  3,961,360,880  cycles                                             ( +-  0.03% )
  8,086,316,342  instructions          #  2.04  insn per cycle      ( +-  0.03% )
     49,625,870  cache-references                                   ( +-  0.03% )
          2,010  cache-misses          #  0.004 % of all cache refs ( +-  9.50% )
  1,615,852,192  branches:k                                         ( +-  0.03% )
     23,732,952  branch-misses:k       #  1.47% of all branches     ( +-  0.03% )
      2,568,064  l2_rqsts.all_code_rd                               ( +-  0.12% )
      2,001,369  l2_rqsts.code_rd_hit                               ( +-  0.15% )
        566,683  l2_rqsts.code_rd_miss                              ( +-  0.10% )
#+END_EXAMPLE

* Baseline kernel with retpoline

Need a baseline kernel, just before Hellwigs patches:
 - Linux broadwell 4.20.0-rc4-hellwig-baseline+ #3 SMP PREEMPT

#+BEGIN_EXAMPLE
 git checkout -b hellwig-baseline ef78e5ec9214
#+END_EXAMPLE

** Driver ixgbe

*** ixgbe: XDP_DROP (baseline-RETPOLINE)

#+BEGIN_EXAMPLE
sudo ./xdp_rxq_info --dev ixgbe2 --action XDP_DROP
Running XDP on dev:ixgbe2 (ifindex:7) action:XDP_DROP options:no_touch
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      4       14,602,534  0          
XDP-RX CPU      total   14,602,534 

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    4:4   14,602,528  0          
rx_queue_index    4:sum 14,602,528 
#+END_EXAMPLE

*** ixgbe: XDP_REDIRECT devmap (baseline-RETPOLINE)

XDP redirect (via devmap) RX:ixgbe2 -> TX:i40e1, this is redirect from RX
link-speed 10Gbit/s out TXC 40Gbit/s link-speed to make sure TX side is not
bottleneck.

#+BEGIN_EXAMPLE
[broadwell kernel-bpf-samples]$ sudo ./xdp_redirect_map  $(</sys/class/net/ixgbe2/ifindex) $(</sys/class/net/i40e1/ifindex)
input: 7 output: 3
map[0] (vports) = 4, map[1] (map) = 5, map[2] (count) = 0
ifindex 3:    1926575 pkt/s
ifindex 3:    7445550 pkt/s
ifindex 3:    7443763 pkt/s
ifindex 3:    7445031 pkt/s
#+END_EXAMPLE

Need ethtool_stats evidence:
#+BEGIN_EXAMPLE
$ ethtool_stats.pl --dev i40e1 --dev ixgbe2 --dev ixgbe1  --sec 2
Show adapter(s) (i40e1 ixgbe2 ixgbe1) statistics (ONLY that changed!)
Ethtool(i40e1   ) stat:    476049953 (    476,049,953) <= port.tx_bytes /sec
Ethtool(i40e1   ) stat:      7438296 (      7,438,296) <= port.tx_size_64 /sec
Ethtool(i40e1   ) stat:      7438281 (      7,438,281) <= port.tx_unicast /sec
Ethtool(i40e1   ) stat:    446296831 (    446,296,831) <= rx_bytes /sec
Ethtool(i40e1   ) stat:      7438281 (      7,438,281) <= rx_packets /sec
Ethtool(i40e1   ) stat:      7438283 (      7,438,283) <= tx_unicast /sec
Ethtool(ixgbe2  ) stat:     11442358 (     11,442,358) <= fdir_miss /sec
Ethtool(ixgbe2  ) stat:    446127207 (    446,127,207) <= rx_bytes /sec
Ethtool(ixgbe2  ) stat:    951162765 (    951,162,765) <= rx_bytes_nic /sec
Ethtool(ixgbe2  ) stat:      3662929 (      3,662,929) <= rx_missed_errors /sec
Ethtool(ixgbe2  ) stat:      3763511 (      3,763,511) <= rx_no_dma_resources /sec
Ethtool(ixgbe2  ) stat:      7435453 (      7,435,453) <= rx_packets /sec
Ethtool(ixgbe2  ) stat:     11198987 (     11,198,987) <= rx_pkts_nic /sec
Ethtool(ixgbe2  ) stat:    446127207 (    446,127,207) <= rx_queue_4_bytes /sec
Ethtool(ixgbe2  ) stat:      7435453 (      7,435,453) <= rx_queue_4_packets /sec
#+END_EXAMPLE

Result: i40e1 sending  7438283 (7,438,283) <= tx_unicast /sec

#+BEGIN_EXAMPLE
$ sudo ~/perf stat -C4 -e cycles -e  instructions -e cache-references -e cache-misses -e branches:k -e branch-misses:k -e l2_rqsts.all_code_rd -e l2_rqsts.code_rd_hit -e l2_rqsts.code_rd_miss -r 4 sleep 1

 Performance counter stats for 'CPU(s) 4' (4 runs):

 3,804,156,271  cycles                                            ( +-  0.01% )
 5,855,352,513  instructions         #  1.54  insn per cycle      ( +-  0.00% )
    37,489,166  cache-references                                  ( +-  0.00% )
           225  cache-misses         #  0.001 % of all cache refs ( +- 38.96% )
 1,233,166,715  branches:k                                        ( +-  0.00% )
    55,575,551  branch-misses:k      #  4.51% of all branches     ( +-  0.00% )
       649,513  l2_rqsts.all_code_rd                              ( +-  0.45% )
       547,581  l2_rqsts.code_rd_hit                              ( +-  0.41% )
       101,945  l2_rqsts.code_rd_miss                             ( +-  0.80% )

     1.0011470 +- 0.0000522 seconds time elapsed  ( +-  0.01% )
#+END_EXAMPLE


* Baseline kernel with no-retpoline

What was performance before RETPOLINE? Testing without CONFIG_RETPOLINE
 - Linux broadwell 4.20.0-rc4-hellwig-baseline-no-retpoline+ #4 SMP PREEMPT

** Driver ixgbe

*** ixgbe: XDP_REDIRECT devmap (baseline-NO-retpoline)

#+BEGIN_EXAMPLE
[jbrouer@broadwell kernel-bpf-samples]$ sudo ./xdp_redirect_map  $(</sys/class/net/ixgbe2/ifindex) $(</sys/class/net/i40e1/ifindex)
input: 7 output: 2
map[0] (vports) = 4, map[1] (map) = 5, map[2] (count) = 0
ifindex 2:    2049760 pkt/s
ifindex 2:   11913696 pkt/s
ifindex 2:   11930501 pkt/s
ifindex 2:   11930700 pkt/s
ifindex 2:   11930911 pkt/s
#+END_EXAMPLE

Need ethtool_stats evidence:
#+BEGIN_EXAMPLE
Show adapter(s) (i40e1 ixgbe2 ixgbe1) statistics (ONLY that changed!)
Ethtool(i40e1   ) stat:    762445780 (    762,445,780) <= port.tx_bytes /sec
Ethtool(i40e1   ) stat:     11913151 (     11,913,151) <= port.tx_size_64 /sec
Ethtool(i40e1   ) stat:     11913224 (     11,913,224) <= port.tx_unicast /sec
Ethtool(i40e1   ) stat:    714789220 (    714,789,220) <= rx_bytes /sec
Ethtool(i40e1   ) stat:     11913154 (     11,913,154) <= rx_packets /sec
Ethtool(i40e1   ) stat:     11913154 (     11,913,154) <= tx_unicast /sec
Ethtool(ixgbe2  ) stat:     13562215 (     13,562,215) <= fdir_miss /sec
Ethtool(ixgbe2  ) stat:    716557813 (    716,557,813) <= rx_bytes /sec
Ethtool(ixgbe2  ) stat:    953785825 (    953,785,825) <= rx_bytes_nic /sec
Ethtool(ixgbe2  ) stat:      1734254 (      1,734,254) <= rx_missed_errors /sec
Ethtool(ixgbe2  ) stat:      1226028 (      1,226,028) <= rx_no_dma_resources /sec
Ethtool(ixgbe2  ) stat:     11942630 (     11,942,630) <= rx_packets /sec
Ethtool(ixgbe2  ) stat:     13168654 (     13,168,654) <= rx_pkts_nic /sec
Ethtool(ixgbe2  ) stat:    716557813 (    716,557,813) <= rx_queue_2_bytes /sec
Ethtool(ixgbe2  ) stat:     11942630 (     11,942,630) <= rx_queue_2_packets /sec
#+END_EXAMPLE

Result: i40e1 = 11913154 (11,913,154) <= tx_unicast /sec

#+BEGIN_EXAMPLE
$ sudo ~/perf stat -C2 -e cycles -e  instructions -e cache-references -e cache-misses -e branches:k -e branch-misses:k -e l2_rqsts.all_code_rd -e l2_rqsts.code_rd_hit -e l2_rqsts.code_rd_miss -r 4 sleep 1

 Performance counter stats for 'CPU(s) 2' (4 runs):

  3,804,824,894  cycles                                            ( +-  0.01% )
  9,088,780,992  instructions         # 2.39  insn per cycle       ( +-  0.01% )
     60,232,927  cache-references                                  ( +-  0.01% )
            231  cache-misses         # 0.000 % of all cache refs  ( +- 28.11% )
  1,802,487,890  branches:k                                        ( +-  0.01% )
      2,434,529  branch-misses:k      # 0.14% of all branches      ( +-  0.04% )
        874,547  l2_rqsts.all_code_rd                              ( +-  2.29% )
        742,335  l2_rqsts.code_rd_hit                              ( +-  1.85% )
        132,198  l2_rqsts.code_rd_miss                             ( +-  4.78% )
#+END_EXAMPLE

* Kernel new git branch dma-direct-calls.2

Branch: dma-direct-calls.2
 - Tree: git://git.infradead.org/users/hch/misc.git

** ixgbe: XDP_REDIRECT devmap (branch dma-direct-calls.2)

Redirect via:
#+BEGIN_EXAMPLE
$ sudo ./xdp_redirect_map  $(</sys/class/net/ixgbe2/ifindex) $(</syslass/net/i40e1/ifindex)
#+END_EXAMPLE

Evidence from ethtool_stats.pl of TX:
 - Result: 10049223 (10,049,223) <= tx_unicast /sec

#+BEGIN_EXAMPLE
Show adapter(s) (i40e1 ixgbe2 ixgbe1) statistics (ONLY that changed!)
Ethtool(i40e1   ) stat:    643150456 (    643,150,456) <= port.tx_bytes /sec
Ethtool(i40e1   ) stat:     10049223 (     10,049,223) <= port.tx_size_64 /sec
Ethtool(i40e1   ) stat:     10049223 (     10,049,223) <= port.tx_unicast /sec
Ethtool(i40e1   ) stat:    602953350 (    602,953,350) <= rx_bytes /sec
Ethtool(i40e1   ) stat:     10049223 (     10,049,223) <= rx_packets /sec
Ethtool(i40e1   ) stat:     10049223 (     10,049,223) <= tx_unicast /sec
Ethtool(ixgbe2  ) stat:     12416007 (     12,416,007) <= fdir_miss /sec
Ethtool(ixgbe2  ) stat:    603733053 (    603,733,053) <= rx_bytes /sec
Ethtool(ixgbe2  ) stat:    953235369 (    953,235,369) <= rx_bytes_nic /sec
Ethtool(ixgbe2  ) stat:      2822742 (      2,822,742) <= rx_missed_errors /sec
Ethtool(ixgbe2  ) stat:      2009357 (      2,009,357) <= rx_no_dma_resources /sec
Ethtool(ixgbe2  ) stat:     10062218 (     10,062,218) <= rx_packets /sec
Ethtool(ixgbe2  ) stat:     12071567 (     12,071,567) <= rx_pkts_nic /sec
Ethtool(ixgbe2  ) stat:    603733053 (    603,733,053) <= rx_queue_5_bytes /sec
Ethtool(ixgbe2  ) stat:     10062218 (     10,062,218) <= rx_queue_5_packets /sec
#+END_EXAMPLE

Perf stat:
#+BEGIN_EXAMPLE
$ sudo ~/perf stat -C5 -e cycles -e  instructions -e cache-references -e cache-misses -e branches:k -e branch-misses:k -e l2_rqsts.all_code_rd -e l2_rqsts.code_rd_hit -e l2_rqsts.code_rd_miss -r 4 sleep 1

 Performance counter stats for 'CPU(s) 5' (4 runs):

  3,804,336,574  cycles                                              ( +-  0.00% )
  7,578,166,786  instructions           # 1.99  insn per cycle       ( +-  0.00% )
     50,265,409  cache-references                                    ( +-  0.00% )
            223  cache-misses           # 0.000 % of all cache refs  ( +- 34.62% )
  1,512,375,528  branches:k                                          ( +-  0.00% )
     24,152,484  branch-misses:k        # 1.60% of all branches      ( +-  0.00% )
      1,232,818  l2_rqsts.all_code_rd                                ( +-  1.02% )
      1,152,514  l2_rqsts.code_rd_hit                                ( +-  1.07% )
         80,299  l2_rqsts.code_rd_miss                               ( +-  0.23% )
#+END_EXAMPLE

* Kernel git branch dma-direct-calls.2 but NO-retpoline

What is the effect of the DMA API changes (branch dma-direct-calls.2),
when NOT compiled with RETPOLINE, that is an intersting question?  As
we want to make sure we don't introduce any regressions while working
on fixing/mitigating retpoline.

Result: 11762603 (11,762,603) <= tx_unicast /sec

#+BEGIN_EXAMPLE
Show adapter(s) (i40e1 ixgbe2 ixgbe1) statistics (ONLY that changed!)
Ethtool(i40e1   ) stat:    752806181 (    752,806,181) <= port.tx_bytes /sec
Ethtool(i40e1   ) stat:     11762598 (     11,762,598) <= port.tx_size_64 /sec
Ethtool(i40e1   ) stat:     11762587 (     11,762,587) <= port.tx_unicast /sec
Ethtool(i40e1   ) stat:    705755244 (    705,755,244) <= rx_bytes /sec
Ethtool(i40e1   ) stat:     11762587 (     11,762,587) <= rx_packets /sec
Ethtool(i40e1   ) stat:     11762603 (     11,762,603) <= tx_unicast /sec
Ethtool(ixgbe2  ) stat:     13494045 (     13,494,045) <= fdir_miss /sec
Ethtool(ixgbe2  ) stat:    706543665 (    706,543,665) <= rx_bytes /sec
Ethtool(ixgbe2  ) stat:    953585273 (    953,585,273) <= rx_bytes_nic /sec
Ethtool(ixgbe2  ) stat:      1796888 (      1,796,888) <= rx_missed_errors /sec
Ethtool(ixgbe2  ) stat:      1327168 (      1,327,168) <= rx_no_dma_resources /sec
Ethtool(ixgbe2  ) stat:     11775728 (     11,775,728) <= rx_packets /sec
Ethtool(ixgbe2  ) stat:     13102884 (     13,102,884) <= rx_pkts_nic /sec
Ethtool(ixgbe2  ) stat:    706543665 (    706,543,665) <= rx_queue_0_bytes /sec
Ethtool(ixgbe2  ) stat:     11775728 (     11,775,728) <= rx_queue_0_packets /sec
#+END_EXAMPLE

#+BEGIN_EXAMPLE
$ sudo ~/perf stat -C0 -e cycles -e  instructions -e cache-references -e cache-misses -e branches:k -e branch-misses:k -e l2_rqsts.all_code_rd -e l2_rqsts.code_rd_hit -e l2_rqsts.code_rd_miss -r 4 sleep 1

 Performance counter stats for 'CPU(s) 0' (4 runs):

     3,804,288,765      cycles                                                        ( +-  0.00% )
     8,760,707,840      instructions              #    2.30  insn per cycle           ( +-  0.01% )
        58,578,076      cache-references                                              ( +-  0.02% )
               688      cache-misses              #    0.001 % of all cache refs      ( +- 72.00% )
     1,717,330,768      branches:k                                                    ( +-  0.01% )
         2,393,738      branch-misses:k           #    0.14% of all branches          ( +-  0.10% )
         1,383,597      l2_rqsts.all_code_rd                                          ( +-  3.21% )
         1,148,387      l2_rqsts.code_rd_hit                                          ( +-  3.02% )
           235,186      l2_rqsts.code_rd_miss                                         ( +-  4.21% )

        1.00117914 +- 0.00000689 seconds time elapsed  ( +-  0.00% )
#+END_EXAMPLE

* Investigate overhead of BPF-indirect retpoline

Find way to disable retpoline for BPF XDP indirect call to test DMA
patches. As described earlier:

Even if removing/mitigating all DMA indirect calls, then there are still some
indirect calls, that we cannot avoid:
 - 1. For every packet: Indirect eBPF XDP prog call
 - 2. For every 16th packet: net_device->ndo_xdp_xmit
 - 3. For every 64th packet: NAPI net_rx_action call drivers napi_poll funcptr

I would like to benchmark removing the per packet retpoline, as this
will tell us how close the DMA-patchset can bring a RETPOLINE enabled
kernel to the performance we see without retpoline.

Found that it is possible via GCC attributes to disable retpoline on a
per function basis via __attribute__.

** First-attempt: Disable retpoline in bpf_prog_run_xdp

#+BEGIN_SRC diff
diff --git a/include/linux/filter.h b/include/linux/filter.h
index de629b706d1d..ed0a5153e2a0 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -619,7 +619,9 @@ static inline u32 bpf_prog_run_clear_cb(const struct bpf_prog *prog,
        return BPF_PROG_RUN(prog, skb);
 }
 
-static __always_inline u32 bpf_prog_run_xdp(const struct bpf_prog *prog,
+static __always_inline
+__attribute__((indirect_branch("keep")))
+u32 bpf_prog_run_xdp(const struct bpf_prog *prog,
                                            struct xdp_buff *xdp)
 {
        /* Caller needs to hold rcu_read_lock() (!), otherwise program
#+END_SRC

Not much difference:
#+BEGIN_EXAMPLE
Show adapter(s) (i40e1 ixgbe2 ixgbe1) statistics (ONLY that changed!)
Ethtool(i40e1   ) stat:    658856830 (    658,856,830) <= port.tx_bytes /sec
Ethtool(i40e1   ) stat:     10294638 (     10,294,638) <= port.tx_size_64 /sec
Ethtool(i40e1   ) stat:     10294622 (     10,294,622) <= port.tx_unicast /sec
Ethtool(i40e1   ) stat:    617678278 (    617,678,278) <= rx_bytes /sec
Ethtool(i40e1   ) stat:     10294638 (     10,294,638) <= rx_packets /sec
Ethtool(i40e1   ) stat:     10294622 (     10,294,622) <= tx_unicast /sec
Ethtool(ixgbe2  ) stat:     12439699 (     12,439,699) <= fdir_miss /sec
Ethtool(ixgbe2  ) stat:    616748552 (    616,748,552) <= rx_bytes /sec
Ethtool(ixgbe2  ) stat:    951259886 (    951,259,886) <= rx_bytes_nic /sec
Ethtool(ixgbe2  ) stat:      2724778 (      2,724,778) <= rx_missed_errors /sec
Ethtool(ixgbe2  ) stat:      1859527 (      1,859,527) <= rx_no_dma_resources /sec
Ethtool(ixgbe2  ) stat:     10279143 (     10,279,143) <= rx_packets /sec
Ethtool(ixgbe2  ) stat:     12138660 (     12,138,660) <= rx_pkts_nic /sec
Ethtool(ixgbe2  ) stat:    616748552 (    616,748,552) <= rx_queue_1_bytes /sec
Ethtool(ixgbe2  ) stat:     10279143 (     10,279,143) <= rx_queue_1_packets /sec
#+END_EXAMPLE

#+BEGIN_EXAMPLE
$ sudo ~/perf stat -C1 -e cycles -e  instructions -e cache-references -e cache-misses -e branches:k -e branch-misses:k -e l2_rqsts.all_code_rd -e l2_rqsts.code_rd_hit -e l2_rqsts.code_rd_miss -r 4 sleep 1

 Performance counter stats for 'CPU(s) 1' (4 runs):

  3,985,155,205  cycles                                             ( +-  0.01% )
  7,762,244,304  instructions          # 1.95  insn per cycle       ( +-  0.02% )
     50,783,229  cache-references                                   ( +-  0.01% )
            304  cache-misses          # 0.001 % of all cache refs  ( +- 29.14% )
  1,549,114,820  branches:k                                         ( +-  0.02% )
     24,751,253  branch-misses:k       # 1.60% of all branches      ( +-  0.03% )
      1,115,008  l2_rqsts.all_code_rd                               ( +-  0.62% )
      1,026,484  l2_rqsts.code_rd_hit                               ( +-  0.46% )
         88,493  l2_rqsts.code_rd_miss                              ( +-  2.59% )
#+END_EXAMPLE

** Second attempt: Disable a lot more retpoline function calls

Wonder it is the dev->ndo_xdp_xmit call?
Or napi->poll call?

Disable a lot more retpoline function calls:

#+BEGIN_SRC diff
b/drivers/net/ethernet/intel/ixgbe/ixgbe_maidiff --git a/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c b/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
index 113b38e0defb..528bdeb73e73 100644
--- a/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
+++ b/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
@@ -2269,7 +2269,9 @@ static void ixgbe_rx_buffer_flip(struct ixgbe_ring *rx_ring,
  *
  * Returns amount of work completed
  **/
-static int ixgbe_clean_rx_irq(struct ixgbe_q_vector *q_vector,
+static
+__attribute__((indirect_branch("keep")))
+int ixgbe_clean_rx_irq(struct ixgbe_q_vector *q_vector,
                               struct ixgbe_ring *rx_ring,
                               const int budget)
 {
diff --git a/include/linux/filter.h b/include/linux/filter.h
index de629b706d1d..ed0a5153e2a0 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -619,7 +619,9 @@ static inline u32 bpf_prog_run_clear_cb(const struct bpf_prog *prog,
        return BPF_PROG_RUN(prog, skb);
 }
 
-static __always_inline u32 bpf_prog_run_xdp(const struct bpf_prog *prog,
+static __always_inline
+__attribute__((indirect_branch("keep")))
+u32 bpf_prog_run_xdp(const struct bpf_prog *prog,
                                            struct xdp_buff *xdp)
 {
        /* Caller needs to hold rcu_read_lock() (!), otherwise program
diff --git a/kernel/bpf/devmap.c b/kernel/bpf/devmap.c
index 191b79948424..05e7aa9a7dcc 100644
--- a/kernel/bpf/devmap.c
+++ b/kernel/bpf/devmap.c
@@ -217,7 +217,10 @@ void __dev_map_insert_ctx(struct bpf_map *map, u32 bit)
        __set_bit(bit, bitmap);
 }
 
-static int bq_xmit_all(struct bpf_dtab_netdev *obj,
+
+static
+__attribute__((indirect_branch("keep")))
+int bq_xmit_all(struct bpf_dtab_netdev *obj,
                       struct xdp_bulk_queue *bq, u32 flags,
                       bool in_napi_ctx)
 {
diff --git a/net/core/dev.c b/net/core/dev.c
index ddc551f24ba2..0374e4ab920f 100644
--- a/net/core/dev.c
+++ b/net/core/dev.c
@@ -6027,7 +6027,9 @@ static struct napi_struct *napi_by_id(unsigned int napi_id)
 
 #define BUSY_POLL_BUDGET 8
 
-static void busy_poll_stop(struct napi_struct *napi, void *have_poll_lock)
+static
+__attribute__((indirect_branch("keep")))
+void busy_poll_stop(struct napi_struct *napi, void *have_poll_lock)
 {
        int rc;
 
@@ -6260,7 +6262,9 @@ void netif_napi_del(struct napi_struct *napi)
 }
 EXPORT_SYMBOL(netif_napi_del);
 
-static int napi_poll(struct napi_struct *n, struct list_head *repoll)
+static
+__attribute__((indirect_branch("keep")))
+int napi_poll(struct napi_struct *n, struct list_head *repoll)
 {
        void *have;
        int work, weight;
@@ -6322,7 +6326,9 @@ static int napi_poll(struct napi_struct *n, struct list_head *repoll)
        return work;
 }
 
-static __latent_entropy void net_rx_action(struct softirq_action *h)
+static __latent_entropy
+__attribute__((indirect_branch("keep")))
+void net_rx_action(struct softirq_action *h)
 {
        struct softnet_data *sd = this_cpu_ptr(&softnet_data);
        unsigned long time_limit = jiffies +
#+END_SRC

That seems to work!
 - Result: 11700004 (11,700,004) <= tx_unicast /sec

#+BEGIN_EXAMPLE
Show adapter(s) (i40e1 ixgbe2 ixgbe1) statistics (ONLY that changed!)
Ethtool(i40e1   ) stat:    748800324 (    748,800,324) <= port.tx_bytes /sec
Ethtool(i40e1   ) stat:     11700005 (     11,700,005) <= port.tx_size_64 /sec
Ethtool(i40e1   ) stat:     11700005 (     11,700,005) <= port.tx_unicast /sec
Ethtool(i40e1   ) stat:    702000304 (    702,000,304) <= rx_bytes /sec
Ethtool(i40e1   ) stat:     11700005 (     11,700,005) <= rx_packets /sec
Ethtool(i40e1   ) stat:     11700004 (     11,700,004) <= tx_unicast /sec
Ethtool(ixgbe2  ) stat:     13418440 (     13,418,440) <= fdir_miss /sec
Ethtool(ixgbe2  ) stat:    702554251 (    702,554,251) <= rx_bytes /sec
Ethtool(ixgbe2  ) stat:    952906870 (    952,906,870) <= rx_bytes_nic /sec
Ethtool(ixgbe2  ) stat:      1836691 (      1,836,691) <= rx_missed_errors /sec
Ethtool(ixgbe2  ) stat:      1343245 (      1,343,245) <= rx_no_dma_resources /sec
Ethtool(ixgbe2  ) stat:     11709238 (     11,709,238) <= rx_packets /sec
Ethtool(ixgbe2  ) stat:     13052485 (     13,052,485) <= rx_pkts_nic /sec
Ethtool(ixgbe2  ) stat:    702554251 (    702,554,251) <= rx_queue_4_bytes /sec
Ethtool(ixgbe2  ) stat:     11709238 (     11,709,238) <= rx_queue_4_packets /sec
#+END_EXAMPLE

Perf show very high insn per cycle:
#+BEGIN_EXAMPLE
$ sudo ~/perf stat -C4 -e cycles -e  instructions -e cache-references -e cache-misses -e branches:k -e branch-misses:k -e l2_rqsts.all_code_rd -e l2_rqsts.code_rd_hit -e l2_rqsts.code_rd_miss -r 4 sleep 1

 Performance counter stats for 'CPU(s) 4' (4 runs):

     3,804,219,793      cycles                                                        ( +-  0.00% )
     8,705,888,459      instructions              #    2.29  insn per cycle           ( +-  0.01% )
        57,981,158      cache-references                                              ( +-  0.01% )
               760      cache-misses              #    0.001 % of all cache refs      ( +- 60.81% )
     1,703,235,429      branches:k                                                    ( +-  0.01% )
         3,529,431      branch-misses:k           #    0.21% of all branches          ( +-  0.04% )
           698,408      l2_rqsts.all_code_rd                                          ( +-  0.38% )
           601,881      l2_rqsts.code_rd_hit                                          ( +-  0.42% )
            96,514      l2_rqsts.code_rd_miss                                         ( +-  0.18% )
#+END_EXAMPLE

