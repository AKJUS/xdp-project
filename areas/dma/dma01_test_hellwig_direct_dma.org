# -*- fill-column: 79; -*-
#+TITLE: Testing Hellwig "dma-direct-calls" patchset

Christoph Hellwig <hch@lst.de> is helping out addressing/mitigating the
RETPOLINE overhead, which XDP suffers under.

* Patchset avail here

Gitweb: on branch "dma-direct-calls"
 - http://git.infradead.org/users/hch/misc.git/shortlog/refs/heads/dma-direct-calls

Git checkout procedure:
#+BEGIN_EXAMPLE
$ git clone git://git.infradead.org/users/hch/misc.git hellwig
$ cd hellwig
$ git checkout dma-direct-calls
#+END_EXAMPLE

* General notes

Even if removing/mitigating all DMA indirect calls, then there is still two
indirect calls, that we cannot avoid.
 - 1. Indirect eBPF XDP call invoked for every xdp_buff
 - 2. NAPI net_rx_action call drivers napi_poll function per 64 packets


* Testing Hellwig branch "dma-direct-calls"

Testing with Hellwig's branch "dma-direct-calls"
 - 4.20.0-rc4-hellwig-dma-direct+

** Driver ixgbe

*** ixgbe: XDP_DROP branch "dma-direct-calls"

XDP_DROP performance okay, but AFAICR it was also okay before, as it was only
affected by DMA-sync call.

#+BEGIN_EXAMPLE
Running XDP on dev:ixgbe2 (ifindex:9) action:XDP_DROP options:no_touch
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      3       14,435,891  0          
XDP-RX CPU      total   14,435,891 

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    3:3   14,435,902  0          
rx_queue_index    3:sum 14,435,902 
#+END_EXAMPLE

*** ixgbe: XDP_REDIRECT devmap

XDP redirect (via devmap) RX:ixgbe2 -> TX:i40e1, this is redirect from RX
link-speed 10Gbit/s out TXC 40Gbit/s link-speed to make sure TX side is not
bottleneck.

#+BEGIN_EXAMPLE
sudo ./xdp_redirect_map  $(</sys/class/net/ixgbe2/ifindex) \
  $(</sys/class/net/i40e1/ifindex)
$ sudo ./xdp_redirect_map  $(</sys/class/net/ixgbe2/ifindex) \
                           $(</sys/class/net/i40e1/ifindex)
input: 9 output: 2
map[0] (vports) = 4, map[1] (map) = 5, map[2] (count) = 0
ifindex 2:    4153765 pkt/s
ifindex 2:    9618042 pkt/s
ifindex 2:    9614312 pkt/s
#+END_EXAMPLE

Notice we cannot trust the output from =xdp_redirect_map=, as it only measures
XDP-RX packets, it doesn't know if packets gets dropped. Thus, measure this via
ethtool stats counters and program [[https://github.com/netoptimizer/network-testing/blob/master/bin/ethtool_stats.pl][ethtool_stats.pl]], which also use highres
timers to get correct time interval.

First *ALWAYS* make sure generator is sending fast enough:
#+BEGIN_EXAMPLE
./pktgen_sample03_burst_single_flow.sh -i ixgbe2 -d 10.10.10.2 -m 00:1b:21:bb:9a:86 -t2
[generator ~]$ ~/git/network-testing/bin/ethtool_stats.pl --sec 2 --dev ixgbe2
Show adapter(s) (ixgbe2) statistics (ONLY that changed!)
Ethtool(ixgbe2  ) stat:    892843312 (    892,843,312) <= tx_bytes /sec
Ethtool(ixgbe2  ) stat:    952366643 (    952,366,643) <= tx_bytes_nic /sec
Ethtool(ixgbe2  ) stat:     14880722 (     14,880,722) <= tx_packets /sec
Ethtool(ixgbe2  ) stat:     14880725 (     14,880,725) <= tx_pkts_nic /sec
Ethtool(ixgbe2  ) stat:    446544842 (    446,544,842) <= tx_queue_0_bytes /sec
Ethtool(ixgbe2  ) stat:      7442414 (      7,442,414) <= tx_queue_0_packets /sec
Ethtool(ixgbe2  ) stat:    446298470 (    446,298,470) <= tx_queue_1_bytes /sec
Ethtool(ixgbe2  ) stat:      7438308 (      7,438,308) <= tx_queue_1_packets /sec
#+END_EXAMPLE

Device-Under-Test (DUT):
#+BEGIN_EXAMPLE
Show adapter(s) (ixgbe2 i40e1) statistics (ONLY that changed!)
Ethtool(ixgbe2  ) stat:     12192478 (     12,192,478) <= fdir_miss /sec
Ethtool(ixgbe2  ) stat:    575904896 (    575,904,896) <= rx_bytes /sec
Ethtool(ixgbe2  ) stat:    952519117 (    952,519,117) <= rx_bytes_nic /sec
Ethtool(ixgbe2  ) stat:      2970896 (      2,970,896) <= rx_missed_errors /sec
Ethtool(ixgbe2  ) stat:      2313797 (      2,313,797) <= rx_no_dma_resources /sec
Ethtool(ixgbe2  ) stat:      9598415 (      9,598,415) <= rx_packets /sec
Ethtool(ixgbe2  ) stat:     11912216 (     11,912,216) <= rx_pkts_nic /sec
Ethtool(ixgbe2  ) stat:    575904896 (    575,904,896) <= rx_queue_3_bytes /sec
Ethtool(ixgbe2  ) stat:      9598415 (      9,598,415) <= rx_queue_3_packets /sec
Ethtool(i40e1   ) stat:    615042613 (    615,042,613) <= port.tx_bytes /sec
Ethtool(i40e1   ) stat:      9610060 (      9,610,060) <= port.tx_size_64 /sec
Ethtool(i40e1   ) stat:      9610058 (      9,610,058) <= port.tx_unicast /sec
Ethtool(i40e1   ) stat:    576603475 (    576,603,475) <= rx_bytes /sec
Ethtool(i40e1   ) stat:      9610058 (      9,610,058) <= rx_packets /sec
Ethtool(i40e1   ) stat:      9610088 (      9,610,088) <= tx_unicast /sec
#+END_EXAMPLE

The result: 9610088 (9,610,088) packets/sec

Some perf stats during this redirect (happend to run on CPU-3):
#+BEGIN_EXAMPLE
$ sudo ~/perf stat -C3 -e cycles -e  instructions -e cache-references -e cache-misses -e branches:k -e branch-misses:k -e l2_rqsts.all_code_rd -e l2_rqsts.code_rd_hit -e l2_rqsts.code_rd_miss -r 4 sleep 1

 Performance counter stats for 'CPU(s) 3' (4 runs):

  3,961,360,880  cycles                                             ( +-  0.03% )
  8,086,316,342  instructions          #  2.04  insn per cycle      ( +-  0.03% )
     49,625,870  cache-references                                   ( +-  0.03% )
          2,010  cache-misses          #  0.004 % of all cache refs ( +-  9.50% )
  1,615,852,192  branches:k                                         ( +-  0.03% )
     23,732,952  branch-misses:k       #  1.47% of all branches     ( +-  0.03% )
      2,568,064  l2_rqsts.all_code_rd                               ( +-  0.12% )
      2,001,369  l2_rqsts.code_rd_hit                               ( +-  0.15% )
        566,683  l2_rqsts.code_rd_miss                              ( +-  0.10% )
#+END_EXAMPLE

* Baseline kernel with retpoline

Need a baseline kernel, just before Hellwigs patches:
 - Linux broadwell 4.20.0-rc4-hellwig-baseline+ #3 SMP PREEMPT

#+BEGIN_EXAMPLE
 git checkout -b hellwig-baseline ef78e5ec9214
#+END_EXAMPLE

** Driver ixgbe

*** ixgbe: XDP_DROP (baseline-RETPOLINE)

#+BEGIN_EXAMPLE
sudo ./xdp_rxq_info --dev ixgbe2 --action XDP_DROP
Running XDP on dev:ixgbe2 (ifindex:7) action:XDP_DROP options:no_touch
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      4       14,602,534  0          
XDP-RX CPU      total   14,602,534 

RXQ stats       RXQ:CPU pps         issue-pps  
rx_queue_index    4:4   14,602,528  0          
rx_queue_index    4:sum 14,602,528 
#+END_EXAMPLE

*** ixgbe: XDP_REDIRECT devmap (baseline-RETPOLINE)

XDP redirect (via devmap) RX:ixgbe2 -> TX:i40e1, this is redirect from RX
link-speed 10Gbit/s out TXC 40Gbit/s link-speed to make sure TX side is not
bottleneck.

#+BEGIN_EXAMPLE
[broadwell kernel-bpf-samples]$ sudo ./xdp_redirect_map  $(</sys/class/net/ixgbe2/ifindex) $(</sys/class/net/i40e1/ifindex)
input: 7 output: 3
map[0] (vports) = 4, map[1] (map) = 5, map[2] (count) = 0
ifindex 3:    1926575 pkt/s
ifindex 3:    7445550 pkt/s
ifindex 3:    7443763 pkt/s
ifindex 3:    7445031 pkt/s
#+END_EXAMPLE

Need ethtool_stats evidence:
#+BEGIN_EXAMPLE
$ ethtool_stats.pl --dev i40e1 --dev ixgbe2 --dev ixgbe1  --sec 2
Show adapter(s) (i40e1 ixgbe2 ixgbe1) statistics (ONLY that changed!)
Ethtool(i40e1   ) stat:    476049953 (    476,049,953) <= port.tx_bytes /sec
Ethtool(i40e1   ) stat:      7438296 (      7,438,296) <= port.tx_size_64 /sec
Ethtool(i40e1   ) stat:      7438281 (      7,438,281) <= port.tx_unicast /sec
Ethtool(i40e1   ) stat:    446296831 (    446,296,831) <= rx_bytes /sec
Ethtool(i40e1   ) stat:      7438281 (      7,438,281) <= rx_packets /sec
Ethtool(i40e1   ) stat:      7438283 (      7,438,283) <= tx_unicast /sec
Ethtool(ixgbe2  ) stat:     11442358 (     11,442,358) <= fdir_miss /sec
Ethtool(ixgbe2  ) stat:    446127207 (    446,127,207) <= rx_bytes /sec
Ethtool(ixgbe2  ) stat:    951162765 (    951,162,765) <= rx_bytes_nic /sec
Ethtool(ixgbe2  ) stat:      3662929 (      3,662,929) <= rx_missed_errors /sec
Ethtool(ixgbe2  ) stat:      3763511 (      3,763,511) <= rx_no_dma_resources /sec
Ethtool(ixgbe2  ) stat:      7435453 (      7,435,453) <= rx_packets /sec
Ethtool(ixgbe2  ) stat:     11198987 (     11,198,987) <= rx_pkts_nic /sec
Ethtool(ixgbe2  ) stat:    446127207 (    446,127,207) <= rx_queue_4_bytes /sec
Ethtool(ixgbe2  ) stat:      7435453 (      7,435,453) <= rx_queue_4_packets /sec
#+END_EXAMPLE

Result: i40e1 sending  7438283 (7,438,283) <= tx_unicast /sec

#+BEGIN_EXAMPLE
$ sudo ~/perf stat -C4 -e cycles -e  instructions -e cache-references -e cache-misses -e branches:k -e branch-misses:k -e l2_rqsts.all_code_rd -e l2_rqsts.code_rd_hit -e l2_rqsts.code_rd_miss -r 4 sleep 1

 Performance counter stats for 'CPU(s) 4' (4 runs):

 3,804,156,271  cycles                                            ( +-  0.01% )
 5,855,352,513  instructions         #  1.54  insn per cycle      ( +-  0.00% )
    37,489,166  cache-references                                  ( +-  0.00% )
           225  cache-misses         #  0.001 % of all cache refs ( +- 38.96% )
 1,233,166,715  branches:k                                        ( +-  0.00% )
    55,575,551  branch-misses:k      #  4.51% of all branches     ( +-  0.00% )
       649,513  l2_rqsts.all_code_rd                              ( +-  0.45% )
       547,581  l2_rqsts.code_rd_hit                              ( +-  0.41% )
       101,945  l2_rqsts.code_rd_miss                             ( +-  0.80% )

     1.0011470 +- 0.0000522 seconds time elapsed  ( +-  0.01% )
#+END_EXAMPLE


* Baseline kernel with no-retpoline

What was performance before RETPOLINE? Testing without CONFIG_RETPOLINE
 - Linux broadwell 4.20.0-rc4-hellwig-baseline-no-retpoline+ #4 SMP PREEMPT

** Driver ixgbe

*** ixgbe: XDP_REDIRECT devmap (baseline-NO-retpoline)

#+BEGIN_EXAMPLE
[jbrouer@broadwell kernel-bpf-samples]$ sudo ./xdp_redirect_map  $(</sys/class/net/ixgbe2/ifindex) $(</sys/class/net/i40e1/ifindex)
input: 7 output: 2
map[0] (vports) = 4, map[1] (map) = 5, map[2] (count) = 0
ifindex 2:    2049760 pkt/s
ifindex 2:   11913696 pkt/s
ifindex 2:   11930501 pkt/s
ifindex 2:   11930700 pkt/s
ifindex 2:   11930911 pkt/s
#+END_EXAMPLE

Need ethtool_stats evidence:
#+BEGIN_EXAMPLE
Show adapter(s) (i40e1 ixgbe2 ixgbe1) statistics (ONLY that changed!)
Ethtool(i40e1   ) stat:    762445780 (    762,445,780) <= port.tx_bytes /sec
Ethtool(i40e1   ) stat:     11913151 (     11,913,151) <= port.tx_size_64 /sec
Ethtool(i40e1   ) stat:     11913224 (     11,913,224) <= port.tx_unicast /sec
Ethtool(i40e1   ) stat:    714789220 (    714,789,220) <= rx_bytes /sec
Ethtool(i40e1   ) stat:     11913154 (     11,913,154) <= rx_packets /sec
Ethtool(i40e1   ) stat:     11913154 (     11,913,154) <= tx_unicast /sec
Ethtool(ixgbe2  ) stat:     13562215 (     13,562,215) <= fdir_miss /sec
Ethtool(ixgbe2  ) stat:    716557813 (    716,557,813) <= rx_bytes /sec
Ethtool(ixgbe2  ) stat:    953785825 (    953,785,825) <= rx_bytes_nic /sec
Ethtool(ixgbe2  ) stat:      1734254 (      1,734,254) <= rx_missed_errors /sec
Ethtool(ixgbe2  ) stat:      1226028 (      1,226,028) <= rx_no_dma_resources /sec
Ethtool(ixgbe2  ) stat:     11942630 (     11,942,630) <= rx_packets /sec
Ethtool(ixgbe2  ) stat:     13168654 (     13,168,654) <= rx_pkts_nic /sec
Ethtool(ixgbe2  ) stat:    716557813 (    716,557,813) <= rx_queue_2_bytes /sec
Ethtool(ixgbe2  ) stat:     11942630 (     11,942,630) <= rx_queue_2_packets /sec
#+END_EXAMPLE

Result: i40e1 = 11913154 (11,913,154) <= tx_unicast /sec

#+BEGIN_EXAMPLE
$ sudo ~/perf stat -C2 -e cycles -e  instructions -e cache-references -e cache-misses -e branches:k -e branch-misses:k -e l2_rqsts.all_code_rd -e l2_rqsts.code_rd_hit -e l2_rqsts.code_rd_miss -r 4 sleep 1

 Performance counter stats for 'CPU(s) 2' (4 runs):

  3,804,824,894  cycles                                            ( +-  0.01% )
  9,088,780,992  instructions         # 2.39  insn per cycle       ( +-  0.01% )
     60,232,927  cache-references                                  ( +-  0.01% )
            231  cache-misses         # 0.000 % of all cache refs  ( +- 28.11% )
  1,802,487,890  branches:k                                        ( +-  0.01% )
      2,434,529  branch-misses:k      # 0.14% of all branches      ( +-  0.04% )
        874,547  l2_rqsts.all_code_rd                              ( +-  2.29% )
        742,335  l2_rqsts.code_rd_hit                              ( +-  1.85% )
        132,198  l2_rqsts.code_rd_miss                             ( +-  4.78% )
#+END_EXAMPLE
