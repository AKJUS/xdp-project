# -*- fill-column: 76; -*-
#+Title: Design: Having XDP programs per RX-queue

From the very beginning of the XDP design it was envisoned (by Jesper) that
it should be possible to assign an *XDP program per NIC hardware RX-queue*
number.  This idea was rejected by upstream, due to usability concerns.
*Today there is single XDP program per netdev*.

This document is an attempt to re-visit this idea, argue why it makes sense,
and come-up with a design and sematics that can be accepted upstream.

* Background for XDP per RX-queue

Why XDP per RX-queue makes sense.  Below sections explain the idea behind
and why there is a need for supporting XDP programs attached to individual
RX-queue of the NIC.  And some of the steps already taken towards this goal.

** Original idea

Original idea behind per RX-queue handling comes from Van Jacobson's
NetChannels ([[http://www.lemis.com/grog/Documentation/vj/lca06vj.pdf][PDF]]), as this is the building block for creating a SPSC channel
into a socket or user-application. (SPSC = Single Producer Single Consumer).

** Small step the direction: xdp_rxq_info

In kernel [[https://git.kernel.org/torvalds/c/aecd67b60722d][v4.16]] struct xdp_rxq_info was introduce, which contains
information specific to each RX-queue in the driver.  E.g. this struct
contains the RX =queue_index= number.

#+BEGIN_SRC C
struct xdp_rxq_info {
	struct net_device *dev;
	u32 queue_index;
	u32 reg_state;
	struct xdp_mem_info mem;
};
#+END_SRC

In kernel [[https://git.kernel.org/torvalds/c/5ab073ffd3264][v4.18]] both =xdp_mem_type= was introduce and type [[https://git.kernel.org/torvalds/c/02b55e5657c3a][MEM_TYPE_ZERO_COPY]]
was added. This is specifically used by XDP programs that want to redirect
into AF_XDP sockets, as XDP redirect core code need to identify AF_XDP
zero-copy frames.

But also the *RX queue_index is needed by the XDP developer* using AF_XDP
sockets, as AF_XDP sockets have a strict SPSC relationship with a specific
RX-queue.  Thus (in the single/global netdev XDP program model) the XDP
developer must make sure that correct =ctx->rx_queue_index= is
XDP_REDIRECT'ed into the corresponding XSKMAP index (else frame is dropped
in =__xsk_map_redirect()= by dev and queue_id check in =xsk_rcv()=).

#+BEGIN_SRC C
enum xdp_mem_type {
	MEM_TYPE_PAGE_SHARED = 0, /* Split-page refcnt based model */
	MEM_TYPE_PAGE_ORDER0,     /* Orig XDP full page model */
	MEM_TYPE_PAGE_POOL,
	MEM_TYPE_ZERO_COPY,
	MEM_TYPE_MAX,
};

struct xdp_mem_info {
	u16 type; /* enum xdp_mem_type */
	u16 id;
};
#+END_SRC

** Bottleneck in single/global netdev XDP program

As XDP grows, and more use-cases are added, then I fear that the single XDP
program per netdev is going to be a performance bottleneck.  As the single
XDP program, will have to perform a lot of common checks before it knows
what use-case this packet match. E.g. as described above AF_XDP redirect
requires reading =ctx->rx_queue_index= and core =xsk_rcv()= function also
need to (re)check this is correct.

With an XDP program per RX-queue, we can instead leverage the hardware to
pre-filter/sort packets, and thus simplify the XDP programs. For AF_XDP
zero-copy we already depend on NIC hardware filters being setup.  The
optimization for AF_XDP is, that the checks of rx_queue_index (and dev) can
instead be moved to setup time, instead of runtime fast-path.

The second level optimization is to store extra info per RX-queue that
allows us do take a more direct action.  E.g. in case of AF_XDP storing the
=xsk_sock= allows to basically call =xsk_rcv()= directly, which in-return
allow us to skip part of the XDP redirect-core code. (p.s. do remeber to
handle the flush at NAPI-end).

** Depending on NIC hardware filter setup

For XDP progs per RX-queue to make sense, we do need to setup NIC hardware
filters to steer trafic to specific RX-queues.  AF_XDP zero-copy already
have this dependency.

There are several ways to configure NIC hardware filter, e.g. ethtool or TC
hardware offloads.  It is generally out of scope for XDP to do this setup
itself.  It is a setup dependency that need to be handled (outside and)
before attaching the XDP program.


* Interface semantics

The initial reason XDP programs per RX-queue was rejected was usability
concerns.  Thus, it is important that we define and agree on the behavior
and semantic meaning of the user-interface.

** Proposal#1: RXQ-prog takes precedence

Proposal#1 (Jesper): The RX-queue xdp_prog (RXQ-prog) takes precedence over
the global xdp_prog (global-prog) installed on the netdev.  More
specifically:
 - If global-prog is already loaded, RXQ-prog takes-over/preplace for this
   specific/single RXQ.
 - When RXQ-prog is unloaded, global-prog takes-over (if installed)

Details: What happens, if someone unloads the global-prog?
 - Proposal#1.1 (Jesper): Unload global-prog keeps RXQ-prog's intact.
 - Argument: There is a reason some application installed this specific
   RXQ-prog, it will be unexpected for all these application if a global
   unload (installing NULL xdp_prog) ignore/clear their specific setup.

For completeness: What happens, if someone load a global-prog, while
existing RXQ-prog's exist:
 - Then the RXQ-prog still take precedence.
 - The global-prog is only install on the RXQs that doesn't have an RXQ-prog
   associated.

** Covering future semantics

It is part of the future optimization plans to change the RXQ-prog, such
that it isn't an actual BPF-prog any-longer.  E.g. in case of AF_XDP
sockets, where all frames from a specific RXQ need to be redirected into a
single =xsk_sock=, then we can avoid invoking BPF (which is a retpoline
indirect call) and instead call =xsk_rcv()= directly.

How do we keep the userspace tools and semantics the same?

E.g. when listing the RXQ-prog's we should/must still display that an XDP
program/hook is running on the RXQ ?  (if we are really nice, we can display
that this is a "builtin" type X)

E.g. when sysadm force-fully unload/remove the RXQ-prog from a queue, but
this is a "builtin" type, then we should likely maintain the semantics and
allow the sysadm to unload/remove this XDP-"hook".

** Introspection tools

For the user/sysadm it is also very important that the existing tools for
listing XDP program are extended with support for listing the RXQ-prog's.

In proposal#1.1: Clearing the global-prog does not clear the RXQ-prog's, but
we can extend the tools with a new option to force-fully unload all XDP
programs, although this is disruptive to the applications.

TODO: Add sections describing ideas/plans on how-to extend the different
tools (e.g. iproute2 and bpftool).

* Code assesment

** Initial code assesment

Most of the XDP drivers already have an xdp_prog pointer stored in some per
RX-queue data-structure.  Thus, part of the driver code should be ready to
handle per RX-queue xdp_prog.

The general code idea is that the driver simply fetch and run xdp_prog
assigned to its RX-queue data-structure.  *Runtime* the driver doesn't
really care if this is a global or RX-queue specific program. (Setup-time
the driver or preferably core can add extra constraints checks for RX-queue
specific programs for optimization reasons).

The general question is with two types of xdp_prog's a global and RX-queue
specific, where do we store these?  As these are going to be a need to
reapply e.g. the global xdp_prog in case a RX-queue xdp_prog is removed.

And it is possible/good to to keep this info centrally in e.g. net_device,
to simplify the driver interface?


** Refactor idea: move xdp_rxq_info to net_device/netdev_rx_queue

Should we move =xdp_rxq_info= into net_device->_rx[] which is =struct
netdev_rx_queue=.  (Saeed actually proposed this originally).

#+BEGIN_SRC C
/* This structure contains an instance of an RX queue. */
struct netdev_rx_queue {
#ifdef CONFIG_RPS
	struct rps_map __rcu		*rps_map;
	struct rps_dev_flow_table __rcu	*rps_flow_table;
#endif
	struct kobject			kobj;
	struct net_device		*dev;
	struct xdp_rxq_info		xdp_rxq;
#ifdef CONFIG_XDP_SOCKETS
	struct xdp_umem                 *umem;
#endif
} ____cacheline_aligned_in_smp;
#+END_SRC

As can be seen it already contains an =xdp_rxq_info= member =xdp_rxq=, which
is used by generic XDP.  But given (Daniel was wise enough) to add a
restriction that XDP-native and XDP-generic cannot co-exist on the same
net_device, thus this member could also be used by native-XDP.

** Refactor idea: xdp/bpf_prog into netdev_rx_queue/net_device

The "global" bpf_prog in generic-XDP is stored in =net_device= member
=xdp_prog=.  For generic-XDP to gain XDP-prog per RX-queue support, we could
extend =netdev_rx_queue= with a =xdp_prog= member (type struct =bpf_prog=).

It would be interesting to investigate if it is possible to make drivers
(native-XDP) also use =net_device->xdp_prog= or =netdev_rx_queue->xdp_prog=
instead of storing this in driver local data structures. (As XDP-native and
XDP-generic cannot co-exist, this should be possible).

** Code gotchas: driver runtime changing num queues

We need to check what happens when the driver change the number of queues
runtime.  This can happen via ethtool:

#+BEGIN_EXAMPLE
ethtool --help | grep Channels
        ethtool -l|--show-channels DEVNAME	Query Channels
        ethtool -L|--set-channels DEVNAME	Set Channels
#+END_EXAMPLE

Drivers already need to handle this, and last-time I checked this was
handled correctly in drivers.  Still, this is a code area we need to take
into account.

AFAIK this also affect =net_device->_rx[]= and =dev->real_num_rx_queues=,
which might be relevant according to above refactor-ideas.


* Code notes for AF_XDP sockets

** AF_XDP bind

The AF_XDP socket bind call is an interesting hook point, that could be
leveraged.

The xdp_umem_assign_dev() call in xsk_bind() call XDP setup-NDO:
#+BEGIN_SRC diff
diff --git a/net/xdp/xsk.c b/net/xdp/xsk.c
index 07156f43d295..6004634858b3 100644
--- a/net/xdp/xsk.c
+++ b/net/xdp/xsk.c
@@ -473,7 +473,7 @@ static int xsk_bind(struct socket *sock, struct sockaddr *addr, int addr_len)
                              xs->umem->chunk_mask);
                xskq_set_umem(xs->umem->cq, xs->umem->size,
                              xs->umem->chunk_mask);
-
+// Notice: end-up calling into dev-XDP-setup call dev->netdev_ops->ndo_bpf()
                err = xdp_umem_assign_dev(xs->umem, dev, qid, flags);
                if (err)
                        goto out_unlock;
#+END_SRC

Inside =xdp_umem_assign_dev()= it also register =umem= in =netdev->_rx[]=
(and =_tx[]=):

#+BEGIN_SRC diff
diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index a264cf2accd0..8424368f834e 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -87,7 +87,7 @@ int xdp_umem_assign_dev(struct xdp_umem *umem, struct net_device *dev,
                err = -EBUSY;
                goto out_rtnl_unlock;
        }
-
+       // Registers itself in netdev->_rx[] and _tx[]
        xdp_reg_umem_at_qid(dev, umem, queue_id);
        umem->dev = dev;
        umem->queue_id = queue_id;
@@ -104,7 +104,7 @@ int xdp_umem_assign_dev(struct xdp_umem *umem, struct net_device *dev,
        bpf.command = XDP_SETUP_XSK_UMEM;
        bpf.xsk.umem = umem;
        bpf.xsk.queue_id = queue_id;
-
+       // Calling XDP setup-NDO:
        err = dev->netdev_ops->ndo_bpf(dev, &bpf);
        if (err)
                goto err_unreg_umem;
#+END_SRC

