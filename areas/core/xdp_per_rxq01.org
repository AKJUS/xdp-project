# -*- fill-column: 76; -*-
#+Title: Design: Having XDP programs per RX-queue

From the very beginning of the XDP design it was envisoned (by Jesper) that
it should be possible to assign an *XDP program per NIC hardware RX-queue*
number.  This idea was rejected by upstream, due to usability concerns.
*Today there is single XDP program per netdev*.

This document is an attempt to re-visit this idea, argue why it makes sense,
and come-up with a design and sematics that can be accepted upstream.

* Original idea

Original idea behind per RX-queue handling comes from Van Jacobson's
NetChannels ([[http://www.lemis.com/grog/Documentation/vj/lca06vj.pdf][PDF]]), as this is the building block for creating a SPSC channel
into a socket or user-application. (SPSC = Single Producer Single Consumer).

* Small step the direction: xdp_rxq_info

In kernel [[https://git.kernel.org/torvalds/c/aecd67b60722d][v4.16]] struct xdp_rxq_info was introduce, which contains
information specific to each RX-queue in the driver.  E.g. this struct
contains the RX queue_index number.

#+BEGIN_SRC C
struct xdp_rxq_info {
	struct net_device *dev;
	u32 queue_index;
	u32 reg_state;
	struct xdp_mem_info mem;
};
#+END_SRC

In kernel [[https://git.kernel.org/torvalds/c/5ab073ffd3264][v4.18]] both =xdp_mem_type= was introduce and type [[https://git.kernel.org/torvalds/c/02b55e5657c3a][MEM_TYPE_ZERO_COPY]]
was added. This is specifically used by XDP programs that want to redirect
into AF_XDP sockets, as XDP redirect core code need to identify AF_XDP
zero-copy frames.

But also the *RX queue_index is needed by the XDP developer* using AF_XDP
sockets, as AF_XDP sockets have a strict SPSC relationship with a specific
RX-queue.  Thus (in the single/global netdev XDP program model) the XDP
developer must make sure that correct =ctx->rx_queue_index= is
XDP_REDIRECT'ed into the corresponding XSKMAP index (else frame is dropped
in =__xsk_map_redirect()= by dev and queue_id check in =xsk_rcv()=).

#+BEGIN_SRC C
enum xdp_mem_type {
	MEM_TYPE_PAGE_SHARED = 0, /* Split-page refcnt based model */
	MEM_TYPE_PAGE_ORDER0,     /* Orig XDP full page model */
	MEM_TYPE_PAGE_POOL,
	MEM_TYPE_ZERO_COPY,
	MEM_TYPE_MAX,
};

struct xdp_mem_info {
	u16 type; /* enum xdp_mem_type */
	u16 id;
};
#+END_SRC

* Bottleneck in single/global netdev XDP program

As XDP grows, and more use-cases are added, then I fear that the single XDP
program per netdev is going to be a performance bottleneck.  As the single
XDP program, will have to perform a lot of common checks before it knows
what use-case this packet match. E.g. as described above AF_XDP redirect
requires reading =ctx->rx_queue_index= and core =xsk_rcv()= function also
need to (re)check this is correct.

With an XDP program per RX-queue, we can instead leverage the hardware to
pre-filter/sort packets, and thus simplify the XDP programs. For AF_XDP
zero-copy we already depend on NIC hardware filters being setup.  The
optimization for AF_XDP is, that the checks of rx_queue_index (and dev) can
instead be moved to setup time, instead of runtime fast-path.

The second level optimization is to store extra info per RX-queue that
allows us do take a more direct action.  E.g. in case of AF_XDP storing the
=xsk_sock= allows to basically call =xsk_rcv()= directly, which in-return
allow us to skip part of the XDP redirect-core code. (p.s. do remeber to
handle the flush at NAPI-end).

* Depending on NIC hardware filter setup

For XDP progs per RX-queue to make sense, we do need to setup NIC hardware
filters to steer trafic to specific RX-queues.  AF_XDP zero-copy already
have this dependency.

There are several ways to configure NIC hardware filter, e.g. ethtool or TC
hardware offloads.  It is generally out of scope for XDP to do this setup
itself.  It is a setup dependency that need to be handled (outside and)
before attaching the XDP program.

* Notes

** Refactor idea: move xdp_rxq_info to net_device/netdev_rx_queue

Should we move =xdp_rxq_info= into net_device->_rx[] which is =struct
netdev_rx_queue=.  (Saeed actually proposed this originally).

#+BEGIN_SRC C
/* This structure contains an instance of an RX queue. */
struct netdev_rx_queue {
#ifdef CONFIG_RPS
	struct rps_map __rcu		*rps_map;
	struct rps_dev_flow_table __rcu	*rps_flow_table;
#endif
	struct kobject			kobj;
	struct net_device		*dev;
	struct xdp_rxq_info		xdp_rxq;
#ifdef CONFIG_XDP_SOCKETS
	struct xdp_umem                 *umem;
#endif
} ____cacheline_aligned_in_smp;
#+END_SRC

As can be seen it already contains an =xdp_rxq_info= member =xdp_rxq=, which
is used by generic XDP.  But given (Daniel was wise enough) to add a
restriction that XDP-native and XDP-generic cannot co-exist on the same
net_device, thus this member could also be used by native-XDP.

** Refactor idea: xdp/bpf_prog into netdev_rx_queue/net_device

The "global" bpf_prog in generic-XDP is stored in =net_device= member
=xdp_prog=.  For generic-XDP to gain XDP-prog per RX-queue support, we could
extend =netdev_rx_queue= with a =xdp_prog= member (type struct =bpf_prog=).

It would be interesting to investigate if it is possible to make drivers
(native-XDP) also use =net_device->xdp_prog= or =netdev_rx_queue->xdp_prog=
instead of storing this in driver local data structures. (As XDP-native and
XDP-generic cannot co-exist, this should be possible).

