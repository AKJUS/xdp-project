# -*- fill-column: 76; -*-
#+Title: XDP checksum evaluate and extend options
#+Author: Jesper Dangaard Brouer
#+Options: ^:nil

XDP need to be extended to get info on the packet checksum from NIC
hardware. This document evaluate and test different options.

* Background knowledge: Packet checksums

Most people can *skip this section*. This is a written as reminder on what
the packet checksum means and some details to refresh/remember.

Most Internet protocols use the same checksum algorithm. Called the 16-bit
one's complement sum. This include IP, ICMP, IGMP, UDP and TCP, although UDP
and TCP include fields from the IP header (a pseudo header, more detail
below). (As counter example SCTP uses a 32-bit checksum).

** More checksums in one packet

Remember that packets usually contain several checksums for the different
layers (or encapsulations). For example IP/UDP and IP/TCP packets contains
two checksums.

(We ignore, the Ethernet FCS (frame check sequence), which a CRC
32-bit/4Bytes place at the end of the payload, because XDP at driver L2
frames with bad FCS will not be seen).

The IP-header contains a header (L3) checksum, that is calculated over the
IP header only.

Both UDP and TCP have (16-bit ones complement) checksums that cover their
headers and their data. Remember that this L4 checksum is stored at
different offsets in the L4-header.

Pitfall/detail: UDP/TCP length (in bytes) can be an odd number, but 16-bit
checksum imply 2 bytes (16-bit) to calculate over. The trick is to pad/add a
byte with zero.

*Remember*: Both UDP and TCP include a 12-bytes (for IPv4) pseudo header.

Quote [[https://tools.ietf.org/html/rfc768][RFC-768]]:
#+begin_quote
The pseudo  header  conceptually prefixed to the UDP header contains the
source  address,  the destination  address,  the protocol,  and the  UDP
length.   This information gives protection against misrouted datagrams.
This checksum procedure is the same as is used in TCP.
#+end_quote

The pseudo header:
#+begin_example
 0      7 8     15 16    23 24    31
+--------+--------+--------+--------+
|         source address            |
+--------+--------+--------+--------+
|        destination address        |
+--------+--------+--------+--------+
|  zero  |protocol|   UDP length    |
+--------+--------+--------+--------+
#+end_example


* Overview of issues and use-cases

** Issue/use-case: xdp_frame to SKB

When creating an SKB (=struct sk_buff=) based on =xdp_frame=, then some
optional fields (originating from hardware offload-hints) are lacking.
One of these are the *checksum* "indication".

This result in creating an SKB with "CHECKSUM_NONE" (=skb->summed=).

This issue is (obviously) that network stack have to calculate and check the
packet checksum in software.

Creating SKBs this way happens when XDP-redirecting frames into CPUMAP and
=veth= devices (container use-case).

** Issue/use-case: xdp_frame to Guest-OS

When XDP redirecting into a Guest-OS the same issue occurs, as the xdp_frame
don't carry *checksum* "indication" from HW.

** Use-case: DDoS mitigation

When NIC HW detects a L3 or L4 checksum issue, the packet is (usually) still
allowed to continue into other layers of the network stack.

If the XDP-prog had access to the HW checksum "indication", then it could
drop packets earlier, as part of the DDoS mitigation step.

The *challenge here* is adding this feature *MUST NOT* cause a *slowdown*
for the baseline XDP (DROP) use-cases that don't use/want this feature.

* Measurements: Assess overhead

Quantify: What is the overhead of this missing HW checksum indication?

** UDP checksum overhead: UdpNoPorts drop case

UDP packets can opt-out of checksumming by setting the checksum filed to
zero, which the kernel *pktgen* tool does.

To solve that, here is a =trafgen= based config that generated large UDP
packets with a correct UDP checksum:
- https://github.com/netoptimizer/network-testing/blob/master/trafgen/udp_example03_checksum.trafgen

To keep the kernel code path as short as possible, I'm sending to correct
IP+MAC but there is no listen UDP socket, thus packets are getting dropped
with nstat counter UdpNoPorts.

*UPDATE*: The performance slowdown is not only due checksum missing, it is
also caused by the test-case that cause =page_pool= in mlx5 to run dry, and
do page allocations. Add result with trafgen use zero as UDP checksum
("zero-csum").

Performance results:
| Description of test                | nstat UdpNoPorts: pps     |
|------------------------------------+---------------------------|
| Normal netstack handling           | UdpNoPorts: 3,343,400 pps |
| XDP-redirect into veth with csum   | UdpNoPorts: 2,178,586 pps |
| XDP-redirect into veth *zero-csum* | UdpNoPorts: 2,723,200 pps |
|                                    |                           |

Calculate *overhead* (1500 Byte packet): 2,178,586 pps vs 2,723,200 pps
 - +544,614 pps = (2723200 - 2178586)
 - +91.79 ns slower = ((1/2178586-1/2723200)*10^9)
 - 25% increase in pps if avoid checksum calc = ((2723200/2178586)-1)*100

The *test setup* with XDP-redirect into veth (that does XDP_PASS) is the
same as described in [[file:../mem/page_pool06_alloc_pages_bulk.org]].
Side-note: Kernel used was the devel version with some page allocator
optimizations (also described in linked file).

*** perf diff results UDP checksum

The perf diff result (10 sec measurement) with csum vs. zero-csum:

#+begin_example
# Event 'cycles'
#
# Baseline  Delta Abs  Shared Object                                      Symbol                              >
# ........  .........  .................................................  ....................................>
#
              +14.72%  [kernel.vmlinux]                                   [k] do_csum
     3.46%     -0.98%  [kernel.vmlinux]                                   [k] __udp4_lib_lookup
               +0.97%  [kernel.vmlinux]                                   [k] __skb_checksum_complete
     2.43%     -0.84%  [kernel.vmlinux]                                   [k] __xdp_release_frame
               +0.67%  [kernel.vmlinux]                                   [k] __skb_checksum
     2.87%     -0.65%  [kernel.vmlinux]                                   [k] ip_rcv_core.isra.0
     3.37%     -0.56%  [kernel.vmlinux]                                   [k] __netif_receive_skb_core
     1.94%     -0.50%  [kernel.vmlinux]                                   [k] dev_gro_receive
     2.67%     -0.49%  [kernel.vmlinux]                                   [k] __list_del_entry_valid
     2.09%     -0.47%  [kernel.vmlinux]                                   [k] __list_add_valid
     2.44%     -0.47%  bpf_prog_943df0a1ce7ea5c2_xdp_prognum0             [k] bpf_prog_943df0a1ce7ea5c2_xdp_pr>
     1.72%     -0.43%  [kernel.vmlinux]                                   [k] __icmp_send
     4.01%     -0.43%  [veth]                                             [k] 0x0000000000004c05
     2.85%     -0.40%  [kernel.vmlinux]                                   [k] kmem_cache_free
     1.72%     -0.39%  [kernel.vmlinux]                                   [k] bpf_xdp_redirect_map
     1.37%     -0.37%  [kernel.vmlinux]                                   [k] nf_hook_slow
     0.90%     -0.34%  [kernel.vmlinux]                                   [k] free_pcp_prepare
     2.01%     -0.33%  [kernel.vmlinux]                                   [k] nf_hook_slow_list
     1.17%     -0.33%  [kernel.vmlinux]                                   [k] fib_validate_source
     1.95%     -0.32%  [kernel.vmlinux]                                   [k] free_unref_page_commit
     1.49%     -0.28%  [kernel.vmlinux]                                   [k] ip_route_use_hint
     1.38%     -0.28%  [kernel.vmlinux]                                   [k] netif_receive_skb_list_internal
     0.49%     -0.27%  [kernel.vmlinux]                                   [k] skb_release_head_state
               +0.26%  [kernel.vmlinux]                                   [k] csum_partial
     0.97%     -0.26%  [kernel.vmlinux]                                   [k] ip_list_rcv
     0.69%     -0.24%  [kernel.vmlinux]                                   [k] gro_normal_one
     1.11%     -0.24%  [kernel.vmlinux]                                   [k] ip_protocol_deliver_rcu
     1.47%     -0.24%  [kernel.vmlinux]                                   [k] __slab_free
     0.88%     -0.23%  [kernel.vmlinux]                                   [k] free_unref_page
     1.39%     -0.23%  [mlx5_core]                                        [k] mlx5e_port_ptp_open
     0.47%     -0.22%  [kernel.vmlinux]                                   [k] kfree_skb
     1.22%     -0.22%  [kernel.vmlinux]                                   [k] ip_sublist_rcv
     3.10%     -0.21%  [kernel.vmlinux]                                   [k] __udp4_lib_rcv
     0.64%     -0.21%  [kernel.vmlinux]                                   [k] page_frag_free
     1.39%     -0.21%  [kernel.vmlinux]                                   [k] napi_gro_receive
     1.35%     -0.20%  [kernel.vmlinux]                                   [k] xdp_do_redirect
     1.12%     -0.20%  [kernel.vmlinux]                                   [k] free_unref_page_prepare.part.0
     1.58%     -0.20%  bpf_prog_a55118bafe28d557_xdp_redirect_map_native  [k] bpf_prog_a55118bafe28d557_xdp_re>
#+end_example

*** perf diff result against netstack

The perf diff result against netstack (10 sec measurement) below:

#+begin_example
# Event 'cycles'
#
# Baseline  Delta Abs  Shared Object                                      Symbol                                               
# ........  .........  .................................................  .....................................................
#
              +11.93%  [kernel.vmlinux]                                   [k] do_csum
               +3.40%  [veth]                                             [k] 0x0000000000004c00
     4.16%     -2.54%  [mlx5_core]                                        [k] mlx5e_fec_admin_field
     4.99%     -2.41%  [kernel.vmlinux]                                   [k] kmem_cache_free
               +1.92%  [kernel.vmlinux]                                   [k] memset_erms
               +1.88%  bpf_prog_943df0a1ce7ea5c2_xdp_prognum0             [k] bpf_prog_943df0a1ce7ea5c2_xdp_prognum0
     3.28%     -1.85%  [kernel.vmlinux]                                   [k] dev_gro_receive
     1.85%     -1.85%  [kernel.vmlinux]                                   [k] kmem_cache_alloc
               +1.80%  [kernel.vmlinux]                                   [k] __xdp_build_skb_from_frame
     4.47%     -1.72%  [kernel.vmlinux]                                   [k] __netif_receive_skb_core
     5.08%     -1.64%  [kernel.vmlinux]                                   [k] __udp4_lib_rcv
               +1.62%  [kernel.vmlinux]                                   [k] free_unref_page_commit
               +1.47%  [kernel.vmlinux]                                   [k] __xdp_release_frame
     4.18%     -1.42%  [kernel.vmlinux]                                   [k] __udp4_lib_lookup
               +1.35%  [kernel.vmlinux]                                   [k] bpf_xdp_redirect_map
               +1.32%  bpf_prog_a55118bafe28d557_xdp_redirect_map_native  [k] bpf_prog_a55118bafe28d557_xdp_redirect_map_native
     3.56%     -1.28%  [kernel.vmlinux]                                   [k] ip_rcv_core.isra.0
               +1.20%  [kernel.vmlinux]                                   [k] __alloc_pages_bulk
               +1.16%  [kernel.vmlinux]                                   [k] dev_map_enqueue
     1.74%     -1.15%  [mlx5_core]                                        [k] mlx5e_tx_reporter_dump
               +1.08%  [kernel.vmlinux]                                   [k] kmem_cache_alloc_bulk
               +1.06%  [kernel.vmlinux]                                   [k] free_unref_page_prepare.part.0
     0.07%     +1.03%  [kernel.vmlinux]                                   [k] __slab_free
               +1.02%  [kernel.vmlinux]                                   [k] xdp_do_redirect
               +0.99%  [kernel.vmlinux]                                   [k] dma_map_page_attrs
               +0.97%  [kernel.vmlinux]                                   [k] __skb_checksum_complete
     2.67%     -0.96%  [kernel.vmlinux]                                   [k] nf_hook_slow_list
     2.25%     -0.87%  [kernel.vmlinux]                                   [k] __icmp_send
     1.50%     -0.80%  [mlx5_core]                                        [k] mlx5e_tx_reporter_build_diagnose_output_sq_common
               +0.78%  [kernel.vmlinux]                                   [k] build_skb_around
     0.42%     +0.77%  [kernel.vmlinux]                                   [k] eth_type_trans
     1.27%     -0.75%  [kernel.vmlinux]                                   [k] __build_skb_around
               +0.75%  [kernel.vmlinux]                                   [k] free_unref_page
               +0.74%  [kernel.vmlinux]                                   [k] __rmqueue_pcplist
     2.02%     -0.71%  [kernel.vmlinux]                                   [k] ip_rcv_finish_core.isra.0
     2.04%     -0.70%  [kernel.vmlinux]                                   [k] ip_route_use_hint
     0.62%     +0.68%  [mlx5_core]                                        [k] mlx5e_port_ptp_open
               +0.68%  [kernel.vmlinux]                                   [k] __skb_checksum
               +0.68%  [kernel.vmlinux]                                   [k] __page_pool_alloc_pages_slow
     1.80%     -0.66%  [kernel.vmlinux]                                   [k] netif_receive_skb_list_internal
#+end_example

So, I now have a test that shows the problem. It is very clear that +11.93%
[k] do_csum function is taking too much time.

Example call-stack for =do_csum= :
#+begin_example
ksoftirqd/2    24 [002] 68022.158164:    1133633   cycles: 
        ffffffff81506047 do_csum+0x77 (/boot/vmlinux-5.12.0-rc2-mel-git-alloc_pages_bulk+)
        ffffffff8150614d csum_partial+0xd (/boot/vmlinux-5.12.0-rc2-mel-git-alloc_pages_bulk+)
        ffffffff8178d5fa __skb_checksum+0x6a (/boot/vmlinux-5.12.0-rc2-mel-git-alloc_pages_bulk+)
        ffffffff8178dd91 __skb_checksum_complete+0x31 (/boot/vmlinux-5.12.0-rc2-mel-git-alloc_pages_bulk+)
        ffffffff81873d24 __udp4_lib_rcv+0xb84 (/boot/vmlinux-5.12.0-rc2-mel-git-alloc_pages_bulk+)
        ffffffff818361e5 ip_protocol_deliver_rcu+0xc5 (/boot/vmlinux-5.12.0-rc2-mel-git-alloc_pages_bulk+)
        ffffffff81836325 ip_local_deliver_finish+0x55 (/boot/vmlinux-5.12.0-rc2-mel-git-alloc_pages_bulk+)
        ffffffff8183639e ip_local_deliver+0x5e (/boot/vmlinux-5.12.0-rc2-mel-git-alloc_pages_bulk+)
        ffffffff8183657c ip_sublist_rcv_finish+0x7c (/boot/vmlinux-5.12.0-rc2-mel-git-alloc_pages_bulk+)
        ffffffff81836719 ip_sublist_rcv+0x189 (/boot/vmlinux-5.12.0-rc2-mel-git-alloc_pages_bulk+)
        ffffffff818369aa ip_list_rcv+0x12a (/boot/vmlinux-5.12.0-rc2-mel-git-alloc_pages_bulk+)
        ffffffff817aabb2 __netif_receive_skb_list_core+0x292 (/boot/vmlinux-5.12.0-rc2-mel-git-alloc_pages_bulk+)
        ffffffff817aad91 netif_receive_skb_list_internal+0x1c1 (/boot/vmlinux-5.12.0-rc2-mel-git-alloc_pages_bulk+)
        ffffffff817aaf99 gro_normal_list.part.0+0x19 (/boot/vmlinux-5.12.0-rc2-mel-git-alloc_pages_bulk+)
        ffffffff817ab901 napi_gro_receive+0x61 (/boot/vmlinux-5.12.0-rc2-mel-git-alloc_pages_bulk+)
        ffffffffa0031798 veth_xdp_rcv_skb+0x558 (/lib/modules/5.12.0-rc2-mel-git-alloc_pages_bulk+/kernel/drivers/net/veth.ko)
#+end_example

Decode =__udp4_lib_rcv+0xb84=:
#+begin_example
$ ./scripts/faddr2line vmlinux __udp4_lib_rcv+0xb84
__udp4_lib_rcv+0xb84/0xb90:
__udp_lib_checksum_complete at include/net/udp.h:112
(inlined by) udp_lib_checksum_complete at include/net/udp.h:119
(inlined by) udp_lib_checksum_complete at include/net/udp.h:116
(inlined by) __udp4_lib_rcv at net/ipv4/udp.c:2408
#+end_example

** UDP checksum overhead: socket delivery

As DaveM have [[https://netdevconf.info/1.1/proceedings/slides/miller-hardware-checksumming.pdf][pointed]] out before,
when the checksum check is done combined with a packet copy, then the
overhead is much less.

We can test this fairly easily with our UDP trafgen test, because UDP allows
us to disable checksumming via providing zero as checksum value. Plus we can
run a UDP sink program in userspace that consumes the packets (which cause a
copy into userspace via UDP socket).

*** Check code path with perf

Step one: Check code path do combine copy and checksum.

In perf top we clearly see =csum_partial_copy_generic= which should confirm
this is happening. (Included CPU column below to show RX happens on CPU-3
and udp_sink program runs on CPU-4).

#+begin_example
# Overhead  CPU  Symbol
# ........  ...  .....................................................
#
    10.96%  004  [k] syscall_exit_to_user_mode                        
     7.76%  004  [k] csum_partial_copy_generic                        
     5.14%  004  [k] entry_SYSCALL_64                                 
     4.35%  004  [k] syscall_return_via_sysret                        
     3.81%  003  [k] memset_erms                                      
     2.62%  004  [k] page_frag_free                                   
     1.85%  003  [k] __list_del_entry_valid                           
     1.57%  004  [k] free_pcppages_bulk                               
     1.41%  003  [k] kmem_cache_alloc_bulk                            
     1.38%  003  [k] __udp_enqueue_schedule_skb                       
     1.27%  003  [k] rmqueue_bulk.constprop.0                         
     1.26%  004  [k] skb_copy_and_csum_datagram_msg                   
     1.22%  003  [k] __netif_receive_skb_core                         
     1.17%  003  [k] __udp4_lib_lookup                                
     1.12%  004  [k] kmem_cache_free                                  
     1.10%  004  [k] csum_and_copy_to_iter                            
     1.09%  003  [k] __udp4_lib_rcv                                   
     1.04%  003  [k] ip_rcv_core.isra.0                               
     1.01%  003  [k] udp4_lib_lookup2                                 
     0.99%  003  [k] bpf_prog_943df0a1ce7ea5c2_xdp_prognum0           
     0.97%  003  [k] udp_queue_rcv_one_skb                            
     0.96%  004  [k] __slab_free                                      
     0.91%  003  [k] __xdp_release_frame                              
     0.86%  003  [k] sock_def_readable                                
     0.86%  003  [k] dev_gro_receive                                  
     0.85%  003  [k] __xdp_build_skb_from_frame                       
     0.84%  004  [k] free_unref_page_commit                           
     0.79%  003  [k] __list_add_valid                                 
     0.77%  004  [k] udp_rmem_release                                 
     0.77%  003  [k] __cgroup_bpf_run_filter_skb                      
     0.77%  004  [.] __libc_recv                                      
     0.75%  004  [k] __skb_datagram_iter                              
     0.72%  004  [k] udp_recvmsg                                      
     0.69%  003  [k] bpf_prog_a55118bafe28d557_xdp_redirect_map_native
     0.68%  003  [k] nf_hook_slow_list                                
     0.67%  003  [k] build_skb_around                                 
#+end_example

Perf call stack for: csum_partial_copy_generic
#+begin_example
udp_sink 90008 [004] 161320.394594:    1218198   cycles:
        ffffffff81505e43 csum_partial_copy_generic+0x53 (/boot/vmlinux-5.12.0-rc2-mel-git-alloc_pages_bulk+)
        ffffffff81506243 csum_and_copy_to_user+0x43 (/boot/vmlinux-5.12.0-rc2-mel-git-alloc_pages_bulk+)
        ffffffff814b8f21 csum_and_copy_to_iter+0xb1 (/boot/vmlinux-5.12.0-rc2-mel-git-alloc_pages_bulk+)
        ffffffff81796287 __skb_datagram_iter+0x2b7 (/boot/vmlinux-5.12.0-rc2-mel-git-alloc_pages_bulk+)
        ffffffff81796585 skb_copy_and_csum_datagram_msg+0x85 (/boot/vmlinux-5.12.0-rc2-mel-git-alloc_pages_bulk+)
        ffffffff81871680 udp_recvmsg+0x240 (/boot/vmlinux-5.12.0-rc2-mel-git-alloc_pages_bulk+)
        ffffffff8187fbe2 inet_recvmsg+0xf2 (/boot/vmlinux-5.12.0-rc2-mel-git-alloc_pages_bulk+)
        ffffffff817848b6 __sys_recvfrom+0x166 (/boot/vmlinux-5.12.0-rc2-mel-git-alloc_pages_bulk+)
        ffffffff817848f5 __x64_sys_recvfrom+0x25 (/boot/vmlinux-5.12.0-rc2-mel-git-alloc_pages_bulk+)
        ffffffff819a4c83 do_syscall_64+0x33 (/boot/vmlinux-5.12.0-rc2-mel-git-alloc_pages_bulk+)
        ffffffff81a0007c entry_SYSCALL_64+0x7c (/boot/vmlinux-5.12.0-rc2-mel-git-alloc_pages_bulk+)
            7faf7b340960 __libc_recv+0x20 (/usr/lib64/libc-2.30.so)
#+end_example

*** Measurement: Trafgen UDP checksum

The cmdline for [[https://github.com/netoptimizer/network-testing/blob/master/src/udp_sink.c][udp_sink]]:
#+begin_src sh
$ taskset -c 4 ./udp_sink -l6666 --repeat 10000 --recv
#+end_src

Output:
#+begin_example
$ sudo taskset -c 4 ./udp_sink -l6666 --repeat 10000 --recv
          	run      count   	ns/pkt	pps		cycles	payload
recv      	run:  0	 1000000	735.43	1359742.86	2647	1472	 demux:1
recv      	run:  1	 1000000	728.99	1371768.87	2624	1472	 demux:1
recv      	run:  2	 1000000	730.43	1369047.89	2629	1472	 demux:1
#+end_example

*** Measurement: Trafgen UDP checksum zero

Change trafgen conf to use UDP checksum zero:
#+begin_example
recv      	run: 339	 1000000	679.13	1472481.98	2444	1472	 demux:1
recv      	run: 340	 1000000	676.17	1478920.63	2434	1472	 demux:1
recv      	run: 341	 1000000	676.15	1478969.08	2434	1472	 demux:1
#+end_example

*** Measurements: compare results

| UDP socket delivery | ns/pkt    |        pps | pps readable  |
|---------------------+-----------+------------+---------------|
| checksum            | 730.43 ns | 1369047.89 | 1,369,048 pps |
| no-checksum         | 676.15 ns | 1478969.08 | 1,478,969 pps |
|---------------------+-----------+------------+---------------|
| diff                |  54.28 ns | -109921.19 |  -109,921 pps |

Performance increase with 8% if we can avoid the checksumming step
(((1478969.08/1369047.89)-1)*100).

The UdpNoPorts had +91.79 ns slower compared to 54.28 ns.  Thus, we can
conclude that it is there are less overhead when combining copy and
checksumming, but the overhead is still significant.

*** Measurements: perf diff

#+begin_example
# Event 'cycles'
#
# Baseline  Delta Abs    Symbol
# ........  .........    .....................................................
#
               +7.73%    [k] csum_partial_copy_generic
     5.69%     -5.68%    [k] copy_user_enhanced_fast_string
     2.00%     -1.27%    [k] udp_recvmsg
               +1.13%    [k] csum_and_copy_to_iter
               +1.10%    [k] skb_copy_and_csum_datagram_msg
    12.00%     -0.98%    [k] syscall_exit_to_user_mode
     0.97%     -0.97%    [k] _copy_to_iter
     1.53%     -0.79%    [k] skb_release_data
               +0.73%    [k] __skb_datagram_iter
               +0.59%    [k] do_csum
     2.41%     +0.56%    [k] page_frag_free
     5.54%     -0.46%    [k] entry_SYSCALL_64
     4.68%     -0.37%    [k] syscall_return_via_sysret
               +0.35%    [k] csum_and_copy_to_user
     4.26%     -0.35%    [k] memset_erms
     1.35%     -0.19%    [k] rmqueue_bulk.constprop.0
     1.29%     -0.19%    [k] __udp4_lib_lookup
               +0.16%    [k] csum_partial
     1.74%     -0.16%    [k] free_pcppages_bulk
     1.69%     -0.15%    [k] kmem_cache_alloc_bulk
     1.03%     +0.10%    [k] __udp4_lib_rcv
     1.20%     -0.10%    [k] __slab_free
     0.13%     +0.09%    [k] __x86_retpoline_rax
     0.71%     -0.09%    [k] eth_type_trans
     0.90%     +0.08%    [k] udp_queue_rcv_one_skb
     1.02%     -0.08%    [k] ip_rcv_core.isra.0
#+end_example
