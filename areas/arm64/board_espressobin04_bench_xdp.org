# -*- fill-column: 79; -*-
#+Title: Benchmarks for branch mvneta_04_page_pool_xdp

Benchmark investigations and experiments on git branch
mvneta_04_page_pool_recycle_xdp located here:
 - https://github.com/apalos/bpf-next/commits/mvneta_04_page_pool_recycle_xdp

* XDP_DROP bench with xdp1

#+BEGIN_EXAMPLE
root@espressobin:~/samples/bpf# ./xdp1 3 &
proto 0:     660053 pkt/s
proto 0:     660147 pkt/s
proto 0:     658249 pkt/s
proto 0:     659691 pkt/s

/root/bin/perf stat -C0 -r 3 -e L1-icache-load -e L1-icache-load-misses \
   -e L1-dcache-loads -e L1-dcache-load-misses \
   -e L1-dcache-stores -e L1-dcache-store-misses \
   sleep 1

 Performance counter stats for 'CPU(s) 0' (3 runs):

  201678946  L1-icache-load                                       ( +-  0.01% )
     523274  L1-icache-load-misses  # 0.26% of all L1-icache hits ( +-  0.20% )
  120263097  L1-dcache-loads                                      ( +-  0.01% )
    2112043  L1-dcache-load-misses  # 1.76% of all L1-dcache hits ( +-  0.20% )
  120263364  L1-dcache-stores                                     ( +-  0.01% )
    2112046  L1-dcache-store-misses                               ( +-  0.20% )

/root/bin/perf stat -C0 -r 3 -e instructions -e cycles -e bus-cycles \
  -e cache-misses -e cache-references -e branches -e branch-misses sleep 1

 Performance counter stats for 'CPU(s) 0' (3 runs):

  324082590  instructions     #    0.32  insn per cycle           ( +-  0.01% )
 1003857272  cycles                                               ( +-  0.00% )
  250964427  bus-cycles                                           ( +-  0.00% )
    2105450  cache-misses     #    1.744 % of all cache refs      ( +-  0.11% )
  120754222  cache-references                                     ( +-  0.02% )
   31402039  branches                                             ( +-  0.02% )
     936484  branch-misses    #    2.98% of all branches          ( +-  0.25% )

#+END_EXAMPLE

** XDP_DROP remove prefetch of data

The amount of cache-misses are too high, remove the prefetch on packet data:

#+BEGIN_SRC diff
diff --git a/drivers/net/ethernet/marvell/mvneta.c b/drivers/net/ethernet/marvell/mvneta.c
index b37264750090..3b1624959b89 100644
--- a/drivers/net/ethernet/marvell/mvneta.c
+++ b/drivers/net/ethernet/marvell/mvneta.c
@@ -2051,7 +2051,7 @@ static int mvneta_rx_swbm(struct napi_struct *napi,
                                                      rx_bytes,
                                                      DMA_FROM_DEVICE);
                        /* Prefetch header */
-                       prefetch(data);
+                       // prefetch(data); // perf show issue here
 
                        rx_desc->buf_phys_addr = 0;
                        xdp.data_hard_start = data;
#+END_SRC

Perf stat data *after* removing prefetch:

Shows cache-misses reduced
 * before 2,105,450
 * after    950,486
 * reduction: 1,154,964
 * 1154964 / 660053 pps = 1.7498 cache-miss per packet reduction
 * Indication that prefetch tries to prefetch more than one cache-line

#+BEGIN_EXAMPLE
root@espressobin:~/samples/bpf# ./xdp1 3 &
proto 0:     666439 pkt/s
proto 0:     667531 pkt/s
proto 0:     668113 pkt/s
proto 0:     668110 pkt/s

 # export PATH=/root/bin:$PATH
 # perf stat -C0 -r 3 -e L1-icache-load -e L1-icache-load-misses -e L1-dcache-loads -e L1-dcache-load-misses -e L1-dcache-stores -e L1-dcache-store-misses sleep 1

 Performance counter stats for 'CPU(s) 0' (3 runs):

  204788837  L1-icache-load                                      ( +-  0.02% )
     482820  L1-icache-load-misses # 0.24% of all L1-icache hits ( +-  0.34% )
  121029887  L1-dcache-loads                                     ( +-  0.01% )
     950486  L1-dcache-load-misses # 0.79% of all L1-dcache hits ( +-  0.07% )
  121027251  L1-dcache-stores                                    ( +-  0.01% )
     950480  L1-dcache-store-misses                              ( +-  0.07% )

  1.0039657 +- 0.0000370 seconds time elapsed  ( +-  0.00% )
#+END_EXAMPLE

From the L1-icache-load (204788837) we can estimate/deduce the size of
our active code, knowing the packets-per-sec (667531 pps).  Each
packet basically activate the same code path per 64 packets, and then
there is some NAPI code and refill code, but it will only happen once
every 64 packets.

Code size estimate:
 - 204788837/667531 306.79 cache-lines per packet
 - Cache-line size 64 bytes: 306.79 * 64 = 19635 bytes code in use?
 - Unsure about espressobin I-cache size, but at-least 32 KB (?)
 - Maybe always fetch two cache-lines: 306.79 * 64 * 2 = 39269 bytes ?

#+BEGIN_EXAMPLE
# /root/bin/perf stat -C0 -r 3 -e instructions -e cycles -e bus-cycles -e cache-misses -e cache-references -e branches -e branch-misses sleep 1

 Performance counter stats for 'CPU(s) 0' (3 runs):

  326643916  instructions    #    0.33  insn per cycle           ( +-  0.02% )
 1003829805  cycles                                              ( +-  0.00% )
  250957824  bus-cycles                                          ( +-  0.00% )
     952515  cache-misses    #    0.787 % of all cache refs      ( +-  0.15% )
  121039240  cache-references                                    ( +-  0.02% )
   31649922  branches                                            ( +-  0.02% )
     939670  branch-misses   #    2.97% of all branches          ( +-  0.07% )

#+END_EXAMPLE


* Testing some ideas

** Recompile with out RCU/preempt (CONFIG_PREEMPT_NONE)

Kernel .config files used located here:
 - Before: [[file:configs/jesper_config01-with-preempt]]
 - After:  [[file:configs/jesper_config02_PREEMPT_NONE]]

Did some branch-miss profiling, and it shows branch-misses in RCU
read-side.  Plus the calls to RCU-read-side also consume I-cache.
Thus, experiment with compiling kernel without preempt as that
basically removed the need for RCU-read-side code.

CONFIG_PREEMPT_NONE: No Forced Preemption (Server):

#+BEGIN_EXAMPLE
No Forced Preemption (Server)
CONFIG_PREEMPT_NONE:                                                │
  │                                                                 │
  │ This is the traditional Linux preemption model, geared towards  │
  │ throughput. It will still provide good latencies most of the    │
  │ time, but there are no guarantees and occasional longer delays  │
  │ are possible.                                                   │
  │                                                                 │
  │ Select this option if you are building a kernel for a server or │
  │ scientific/computation system, or if you want to maximize the   │
  │ raw processing power of the kernel, irrespective of scheduling  │
  │ latencies.

 Prompt: No Forced Preemption (Server)          │
  │   Location:                                 │
  │     -> General setup                        │
  │       -> Preemption Model (<choice> [=y])   │
#+END_EXAMPLE

Up and running:

#+BEGIN_EXAMPLE
# uname -a
Linux espressobin 4.20.0-rc1-mvneta_04+ #25 SMP Mon Dec 3 11:33:18 CET 2018 aarch64 aarch64 aarch64 GNU/Linux
#+END_EXAMPLE

This does improve performance:
 - Before: 668113
 - After:  693440
 - 693440-668113 = +25327 pps
 - (1/668113-1/693440)*10^9 = 54.66 ns

#+BEGIN_EXAMPLE
# ./xdp1 3 &
proto 0:     466516 pkt/s
proto 0:     693440 pkt/s
proto 0:     693822 pkt/s
proto 0:     693735 pkt/s
proto 0:     489783 pkt/s
^C
root@espressobin:~/samples/bpf#
#+END_EXAMPLE

Perf stats performance for CONFIG_PREEMPT_NONE measurements, L1 cache:

#+BEGIN_EXAMPLE
 Performance counter stats for 'CPU(s) 0' (3 runs):

  186193917  L1-icache-load                                      ( +-  0.02% )
     423491  L1-icache-load-misses # 0.23% of all L1-icache hits ( +-  0.33% )
  114063063  L1-dcache-loads                                     ( +-  0.02% )
    1222909  L1-dcache-load-misses # 1.07% of all L1-dcache hits ( +-  0.09% )
  114063101  L1-dcache-stores                                    ( +-  0.02% )
    1222908  L1-dcache-store-misses                              ( +-  0.09% )
#+END_EXAMPLE

Analysis: L1-icache-load were reduced significantly:
 - Before: 204788837 L1-icache-load => (204788837/667531*64) 19634 bytes code
 - After:  186193917 L1-icache-load => (186193917/693440*64) 17185 bytes code

The L1-icache-load-misses were also reduce a bit:
 - Before: 482820  L1-icache-load-misses # 0.24% of all L1-icache hits
 - After:  423491  L1-icache-load-misses # 0.23% of all L1-icache hits
 - Diff:    59329

Perf stats performance for CONFIG_PREEMPT_NONE measurements:

#+BEGIN_EXAMPLE
 Performance counter stats for 'CPU(s) 0' (3 runs):

   311683229  instructions     #    0.31  insn per cycle        ( +-  0.00% )
  1003777048  cycles                                            ( +-  0.00% )
   250944417  bus-cycles                                        ( +-  0.00% )
     1225570  cache-misses     #    1.074 % of all cache refs   ( +-  0.22% )
   114109895  cache-references                                  ( +-  0.01% )
    27106182  branches                                          ( +-  0.02% )
      226928  branch-misses    #    0.84% of all branches       ( +-  0.72% )
#+END_EXAMPLE

Analysis: The branch-misses were reduced significantly when recompiled
with CONFIG_PREEMPT_NONE, which compiles out the RCU-read-side locks:
 - Before: 31402039  branches  939670  branch-misses # 2.97% of all branches
 - After:  27106182  branches  226928  branch-misses # 0.84% of all branches
 - Diff:   -4295857  branches -712742  branch-misses

Below output from mpstat to verify general system performance.

#+BEGIN_EXAMPLE
$ mpstat -P ALL -u -I SCPU -I SUM 2
  CPU    %usr   %nice    %sys %iowait    %irq   %soft    %idle
  all    0.00    0.00    0.00    0.00    0.75   49.25    50.00
    0    0.00    0.00    0.00    0.00    1.50   98.50     0.00
    1    0.00    0.00    0.00    0.00    0.00    0.50    99.50

  CPU    intr/s
  all  11960.50
    0  11096.00
    1     66.00

  CPU  TIMER/s   NET_TX/s   NET_RX/s  IRQ_POLL/s  TASKLET/s    SCHED/s   RCU/s
    0   250.00       0.00   10836.00        0.00       0.00       7.50    2.50
    1    22.00       0.00       0.00        0.00       0.00      42.50    1.50

# Measured:
proto 0:     693651 pkt/s
#+END_EXAMPLE

From the 10836 NET_RX/s we can calculate the NAPI poll budget getting
used, here: 693651 / 10836 = 64.01 packets.  This is spot on for the
expected NAPI poll budget of 64.

** Test: Comment out code in mvneta_rx_swbm

The main NAPI poll RX funtion in driver mvneta mvneta_rx_swbm() have
special handling of "Middle or Last descriptor" inside this main loop,
which could cause I-cache issues.  Hack comment it out... and test.

The code-size delta is 60 bytes in mvneta_poll:
#+BEGIN_EXAMPLE
$ ./scripts/bloat-o-meter vmlinux2 vmlinux
add/remove: 0/3 grow/shrink: 0/1 up/down: 0/-84 (-84)
Function                                     old     new   delta
e843419@0975_0000d554_34                       8       -      -8
e843419@0929_0000cd2d_1628                     8       -      -8
e843419@087e_0000ba28_258                      8       -      -8
mvneta_poll                                 3108    3048     -60
Total: Before=15472523, After=15472439, chg -0.00%
#+END_EXAMPLE

Testing with xdp1/XDP_DROP on eth0.

Before: 696194 pkt/s
#+BEGIN_EXAMPLE
perf stat -C0 -r 3 -e L1-icache-load -e L1-icache-load-misses -e L1-dcache-loads -e L1-dcache-load-misses -e L1-dcache-stores -e L1-dcache-store-misses sleep 1

 Performance counter stats for 'CPU(s) 0' (3 runs):

         187683394      L1-icache-load                                                ( +-  0.04% )
            506294      L1-icache-load-misses     #    0.27% of all L1-icache hits    ( +-  1.11% )
         114865063      L1-dcache-loads                                               ( +-  0.03% )
           1013416      L1-dcache-load-misses     #    0.88% of all L1-dcache hits    ( +-  0.13% )
         114865277      L1-dcache-stores                                              ( +-  0.03% )
           1013417      L1-dcache-store-misses                                        ( +-  0.13% )
#+END_EXAMPLE

After: 692034 pkt/s
#+BEGIN_EXAMPLE
 Performance counter stats for 'CPU(s) 0' (3 runs):

         186047636      L1-icache-load                                                ( +-  0.06% )
            435807      L1-icache-load-misses     #    0.23% of all L1-icache hits    ( +-  0.78% )
         113598029      L1-dcache-loads                                               ( +-  0.04% )
           1264343      L1-dcache-load-misses     #    1.11% of all L1-dcache hits    ( +-  0.71% )
         113598235      L1-dcache-stores                                              ( +-  0.04% )
           1264346      L1-dcache-store-misses                                        ( +-  0.71% )
#+END_EXAMPLE

Strange results, as PPS is slightly worse (692034-696194 = -4160 pps),
but the L1-icache-load-misses, are improved (435807-506294 = -70487
I-cache-misses).

** Playing with other perf events

#+BEGIN_EXAMPLE
pipeline:
  agu_dep_stall                                     
       [Cycles there is an interlock for a load/store instruction waiting for data to calculate the address in
        the AGU]
  decode_dep_stall                                  
       [Cycles the DPU IQ is empty and there is a pre-decode error being processed]
  ic_dep_stall                                      
       [Cycles the DPU IQ is empty and there is an instruction cache miss being processed]
  iutlb_dep_stall                                   
       [Cycles the DPU IQ is empty and there is an instruction micro-TLB miss being processed]
  ld_dep_stall                                      
       [Cycles there is a stall in the Wr stage because of a load miss]
  other_interlock_stall                             
       [Cycles there is an interlock other than Advanced SIMD/Floating-point instructions or load/store
        instruction]
  other_iq_dep_stall                                
       [Cycles that the DPU IQ is empty and that is not because of a recent micro-TLB miss, instruction cache
        miss or pre-decode error]
  simd_dep_stall                                    
       [Cycles there is an interlock for an Advanced SIMD/Floating-point operation]
  st_dep_stall                                      
       [Cycles there is a stall in the Wr stage because of a store]
  stall_sb_full                                     
       [Data Write operation that stalls the pipeline because the store buffer is full]
#+END_EXAMPLE

#+BEGIN_EXAMPLE
perf stat -C0 -r 3 -e agu_dep_stall -e decode_dep_stall -e ic_dep_stall \
-e ld_dep_stall -e st_dep_stall -e iutlb_dep_stall  sleep 1

 Performance counter stats for 'CPU(s) 0' (3 runs):

   57895390      agu_dep_stall              ( +-  0.00% )
          0      decode_dep_stall         
    3819668      ic_dep_stall               ( +-  0.74% )
  486917106      ld_dep_stall               ( +-  0.00% )
  107229705      st_dep_stall               ( +-  0.01% )
     684001      iutlb_dep_stall            ( +-  0.12% )
#+END_EXAMPLE

#+BEGIN_EXAMPLE
perf stat -C0 -r 3 -e agu_dep_stall -e decode_dep_stall -e ic_dep_stall \
-e ld_dep_stall -e iutlb_dep_stall -e other_interlock_stall \
-e other_iq_dep_stall -e simd_dep_stall -e st_dep_stall -e stall_sb_full \
sleep 1

    57710381      agu_dep_stall                  ( +-  0.02% )  (59.96%)
           0      decode_dep_stall               (60.16%)
     4025870      ic_dep_stall                   ( +-  1.29% )  (60.16%)
   485093041      ld_dep_stall                   ( +-  0.01% )  (60.16%)
      685257      iutlb_dep_stall                ( +-  0.37% )  (60.16%)
    36521529      other_interlock_stall          ( +-  0.02% )  (60.16%)
     1445909      other_iq_dep_stall             ( +-  0.23% )  (59.97%)
           0      simd_dep_stall                 (59.76%)
   106689231      st_dep_stall                   ( +-  0.01% )  (59.76%)
       33677      stall_sb_full                  ( +-  0.33% )  (59.76%)

  1.00412169 +- 0.00000383 seconds time elapsed  ( +-  0.00% )
#+END_EXAMPLE

#+BEGIN_EXAMPLE
perf stat -C0 -r 3 -e instructions -e cycles \
 -e ic_dep_stall -e ld_dep_stall -e st_dep_stall \
 -e agu_dep_stall -e stall_sb_full -e other_interlock_stall \
 sleep 1

    399566325      instructions  #  0.40  insn per cycle ( +-  0.01% )  (74.51%)
   1000680946      cycles                                ( +-  0.00% )  (87.25%)
      3496508      ic_dep_stall                          ( +-  1.32% )  (87.38%)
    485837259      ld_dep_stall                          ( +-  0.00% )  (87.65%)
    106977052      st_dep_stall                          ( +-  0.01% )  (87.65%)
     57750275      agu_dep_stall                         ( +-  0.01% )  (87.65%)
        47449      stall_sb_full                         ( +-  0.16% )  (87.65%)
     36577300      other_interlock_stall                 ( +-  0.01% )  (87.52%)
#+END_EXAMPLE

#+BEGIN_EXAMPLE
perf stat -C0 -r 3 -e instructions -e cycles \
 -e ic_dep_stall -e ld_dep_stall -e st_dep_stall \
 -e agu_dep_stall -e other_interlock_stall \
 sleep 1

# options:no_touch
./xdp_rxq_info --d eth0 --action XDP_DROP

Running XDP on dev:eth0 (ifindex:3) action:XDP_DROP options:no_touch
XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      0       708595      0          
XDP-RX CPU      total   708595     

 Performance counter stats for 'CPU(s) 0' (3 runs):

   400824190      instructions     #    0.40  insn per cycle  ( +-  0.00% )
  1003962088      cycles                                      ( +-  0.00% )
     3399851      ic_dep_stall                                ( +-  1.30% )
   487539028      ld_dep_stall                                ( +-  0.01% )
   107368181      st_dep_stall                                ( +-  0.01% )
    57959951      agu_dep_stall                               ( +-  0.01% )
    36690192      other_interlock_stall                       ( +-  0.00% )

# options:read
./xdp_rxq_info --d eth0 --a XDP_DROP --read

XDP stats       CPU     pps         issue-pps  
XDP-RX CPU      0       616934      0          
XDP-RX CPU      total   616934     

 Performance counter stats for 'CPU(s) 0' (3 runs):

   367662048      instructions         # 0.37  insn per cycle  ( +-  0.01% )
  1003952335      cycles                                       ( +-  0.00% )
     3285485      ic_dep_stall                                 ( +-  0.42% )
   531033126      ld_dep_stall                                 ( +-  0.01% )
    95498417      st_dep_stall                                 ( +-  0.01% )
    54374611      agu_dep_stall                                ( +-  0.01% )
    34716242      other_interlock_stall                        ( +-  0.01% )
#+END_EXAMPLE

