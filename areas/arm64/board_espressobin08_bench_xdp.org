# -*- fill-column: 76; -*-
#+Title: Benchmarks for Espressobin driver mvneta
#+AUTHOR: Jesper Dangaard Brouer
#+EMAIL: brouer@redhat.com
#+OPTIONS: ^:nil

This document focus on benchmarking the patchset from Lorenzo Bianconi
 - [[https://patchwork.ozlabs.org/project/netdev/list/?series=134430&state=%2a][Patchset V1]]
 - Message-Id: <cover.1570307172.git.lorenzo@kernel.org>
 - Thread on [[https://lore.kernel.org/netdev/cover.1570307172.git.lorenzo@kernel.org/][lore.kernel.org]]

The patchset adds XDP support to the Marvell driver mvneta.

The benchmark will be performed on a [[https://espressobin.net/][EspressoBin]] board.

* Code being tested

Due to some adjustment to patchset V1, that involved DMA-sync to-device,
which is *VERY* expensive on this board, I've decided to create a separate
branch on a git tree (under my control), to keep this reproducible.

In flux Lorenzo GitHub tree with mvneta developement is here:
- https://github.com/LorenzoBianconi/net-next/tree/mvneta-devel

More stable tree with patches I've tested:
- Kernel.org git tree:
- https://git.kernel.org/pub/scm/linux/kernel/git/hawk/net-next-xdp.git/
- Branch: xdp_mvneta01-bench
- [[https://git.kernel.org/pub/scm/linux/kernel/git/hawk/net-next-xdp.git/log/?h=xdp_mvneta01-bench][link to branch]]

* Review notes: Issues with patchset/code

** Wrong stats

The stats accounting is wrong. Other drivers also gets this wrong, and are
inconsistent. Upstream have not fully agree on the semantics.  Thus,
claiming it is "wrong" might be too harsh.

*** Stats-issue-1: XDP_DROP are invisible

Usually drivers (e.g. ixgbe + i40e) will still keep track of XDP_DROP frames
and count them as RX-frames in net_device stats, as XDP is a software drop,
and other counters deeper in network stack will not increment, which should
give enough info. The drivers that don't account as regular RX, will usually
have an ethtool stats.

During XDP_DROP (via =xdp1=), this drivers ethtool stats look like:
#+begin_example
Show adapter(s) (eth0) statistics (ONLY that changed!)
Ethtool(eth0    ) stat:       959198 (        959,198) <= frames_65_to_127_octets /sec
Ethtool(eth0    ) stat:       959199 (        959,199) <= good_frames_received /sec
Ethtool(eth0    ) stat:     69062313 (     69,062,313) <= good_octets_received /sec
Ethtool(eth0    ) stat:       959243 (        959,243) <= p00_hist_65_127bytes /sec
Ethtool(eth0    ) stat:     69065516 (     69,065,516) <= p00_out_octets /sec
Ethtool(eth0    ) stat:       959243 (        959,243) <= p00_out_queue_0 /sec
Ethtool(eth0    ) stat:       959243 (        959,243) <= p00_out_unicast /sec
Ethtool(eth0    ) stat:       537866 (        537,866) <= rx_discard /sec
#+end_example

The packet generator doesn't send faster than 959Kpps.
It's possible to deduce the XDP_DROP rate via subtraction:
- 959199-537866 = 421333 pps


* Extra: Config settings

Enable kernel config setting: =CONFIG_ARM64_PSEUDO_NMI=

I've not setup the kernel parameter "irqchip.gicv3_pseudo_nmi" to 1, but I
want to have the option to enable this, as this allow for better perf
reports.

See why in Kernel-Recipes 2019 talk:
- [[https://kernel-recipes.org/en/2019/talks/no-nmi-no-problem-implementing-arm64-pseudo-nmi/][Implementing Arm64 Pseudo-NMI]]

* Initial benchmark: Identify DMA sync cost

The initial benchmarking clearly show, that the primary cost for this
hardware/driver is due to DMA-sync for device (kernel function
=dma_direct_sync_single_for_device= and resulting cache invalidation).

** iptables drop

#+begin_example
espressobin:~# iptables -t raw -I PREROUTING -p udp --dport 9 -j DROP
root@espressobin:~# nstat -n && sleep 1 && nstat
#kernel
IpInReceives                    151169             0.0
IpExtInOctets                   6953544            0.0
IpExtInNoECTPkts                151165             0.0
#+end_example

Perf report for iptables-raw drop:
#+begin_example
#
# Samples: 35K of event 'cycles:ppp'
# Event count (approx.): 8632918197
#
# Overhead  CPU  Command          Symbol
# ........  ...  ...............  .....................................
#
    18.89%  000  ksoftirqd/0      [k] mvneta_poll
    12.29%  000  ksoftirqd/0      [k] __pi___inval_dcache_area
            |
            ---__pi___inval_dcache_area
                   |--9.69%--dma_direct_sync_single_for_device
                   |          mvneta_rx_refill.isra.74
                   |          mvneta_poll
                   |          net_rx_action
                    --2.61%--dma_direct_sync_single_for_cpu
                              mvneta_poll

     5.80%  000  ksoftirqd/0      [k] __netif_receive_skb_core
     4.36%  000  ksoftirqd/0      [k] ipt_do_table
     4.01%  000  ksoftirqd/0      [k] eth_type_trans
     3.71%  000  ksoftirqd/0      [k] get_page_from_freelist
     2.83%  000  ksoftirqd/0      [k] dev_gro_receive
     2.44%  000  ksoftirqd/0      [k] ip_rcv_core.isra.17
     1.71%  000  ksoftirqd/0      [k] free_unref_page
     1.61%  000  ksoftirqd/0      [k] kmem_cache_alloc
     1.58%  000  ksoftirqd/0      [k] skb_release_data
     1.57%  000  ksoftirqd/0      [k] kmem_cache_free
     1.53%  000  ksoftirqd/0      [k] __netif_receive_skb_one_core
     1.51%  000  ksoftirqd/0      [k] edsa_rcv
     1.37%  000  ksoftirqd/0      [k] netif_receive_skb_internal
     1.28%  000  ksoftirqd/0      [k] __page_pool_put_page
     1.21%  000  ksoftirqd/0      [k] udp_mt
     1.18%  000  ksoftirqd/0      [k] __alloc_pages_nodemask
     1.15%  000  ksoftirqd/0      [k] dsa_switch_rcv
     1.12%  000  ksoftirqd/0      [k] __rcu_read_unlock
     1.09%  000  ksoftirqd/0      [k] ktime_get_with_offset
     1.06%  000  ksoftirqd/0      [k] __rcu_read_lock
     1.01%  000  ksoftirqd/0      [k] free_unref_page_prepare.part.77
     1.00%  000  ksoftirqd/0      [k] bpf_skb_load_helper_16
     0.99%  000  ksoftirqd/0      [k] build_skb
     0.96%  000  ksoftirqd/0      [k] dma_direct_map_page
     0.87%  000  ksoftirqd/0      [k] slabinfo_write
     0.87%  000  ksoftirqd/0      [k] mvneta_rx_refill.isra.74
     0.86%  000  ksoftirqd/0      [k] ip_rcv
     0.85%  000  ksoftirqd/0      [k] page_frag_free
     0.83%  000  ksoftirqd/0      [k] __build_skb
     0.76%  000  ksoftirqd/0      [k] __local_bh_enable_ip
     0.71%  000  ksoftirqd/0      [k] memmove
     0.69%  000  ksoftirqd/0      [k] page_pool_alloc_pages
     0.68%  000  ksoftirqd/0      [k] __page_pool_clean_page
     0.62%  000  ksoftirqd/0      [k] netif_receive_skb
     0.61%  000  ksoftirqd/0      [k] __page_pool_alloc_pages_slow
     0.59%  000  ksoftirqd/0      [k] __netif_receive_skb
#+end_example

** XDP_DROP via xdp1

#+begin_example
root@espressobin:~/samples/bpf# ./xdp1 3
proto 0:      27797 pkt/s
proto 0:     421419 pkt/s
proto 0:     421444 pkt/s
proto 0:     421393 pkt/s
proto 0:     421440 pkt/s
proto 0:     421184 pkt/s
#+end_example

Perf report during xdp1 dropping ALL packets:
#+begin_example
perf report --sort cpu,comm,symbol --no-children --stdio -g none
# Samples: 16K of event 'cycles:ppp'
# Event count (approx.): 3976182320
#
# Overhead  CPU  Command          Symbol
# ........  ...  ...............  .............................................
#
    32.96%  000  ksoftirqd/0      [k] mvneta_poll
    26.88%  000  ksoftirqd/0      [k] __pi___clean_dcache_area_poc
            |
            ---__pi___clean_dcache_area_poc
               dma_direct_sync_single_for_device
               mvneta_rx_refill.isra.74
               mvneta_poll
               net_rx_action

     7.29%  000  ksoftirqd/0      [k] 0xffff8000000b04d4  (<-- BPF-prog)
     7.09%  000  ksoftirqd/0      [k] __pi___inval_dcache_area
            |
            ---__pi___inval_dcache_area
               dma_direct_sync_single_for_cpu
               mvneta_poll
               net_rx_action

     5.82%  000  ksoftirqd/0      [k] __xdp_return
     2.49%  000  ksoftirqd/0      [k] mvneta_rx_refill.isra.74
     1.43%  000  ksoftirqd/0      [k] dma_direct_sync_single_for_cpu
     1.40%  000  ksoftirqd/0      [k] __page_pool_put_page
     1.29%  000  ksoftirqd/0      [k] dma_direct_sync_single_for_device
     1.25%  000  ksoftirqd/0      [k] page_pool_alloc_pages
     1.23%  000  ksoftirqd/0      [k] __softirqentry_text_start
     1.08%  000  ksoftirqd/0      [k] percpu_array_map_lookup_elem
     0.95%  000  ksoftirqd/0      [k] arch_sync_dma_for_device
     0.93%  000  ksoftirqd/0      [k] arch_sync_dma_for_cpu
     0.74%  000  ksoftirqd/0      [k] __rcu_read_unlock
     0.61%  000  ksoftirqd/0      [k] xdp_mem_id_cmp
     0.60%  000  ksoftirqd/0      [k] __rcu_read_lock
     0.57%  000  ksoftirqd/0      [k] xdp_return_buff
     0.40%  000  ksoftirqd/0      [k] xdp_mem_id_hashfn
#+end_example



