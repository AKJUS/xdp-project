# -*- fill-column: 76; -*-
#+TITLE: Project management for CPUMAP
#+CATEGORY: CPUMAP
#+OPTIONS: ^:nil

This document contains *org-mode tasks* and TODOs for [[https://github.com/torvalds/linux/blob/master/kernel/bpf/cpumap.c][cpumap]].

This BPF map type CPUMAP (=BPF_MAP_TYPE_CPUMAP=) is used by XDP to
=XDP_REDIRECT= into via BPF helper =bpf_redirect_map()=. This map type
redirects raw XDP frames to another CPU. The remote CPU will do
SKB-allocation and call the normal network stack.

This is a scalability and isolation mechanism, that allow separating the
early driver network XDP layer, from the rest of the netstack, and assigning
dedicated CPUs for this stage. This basically allows for 10G wirespeed
pre-filtering via bpf, on the serving end-host running Linux.

* Allocate SKB outside driver code

The revolutionary and controversial idea with cpumap redirect is to remove
the SKB allocation inside the drivers RX-path. Instead an =xdp_frame= is
created that contains enough info to allocated the SKB later outside the
driver code, in cpumap's case on a remote CPU, but the idea is to take this
even further.

*The future plans* are working towards creating Ethernet drivers that have
no SKB allocations. Instead they will send these =xdp_frame= packets (in a
bulk) to the network stack, that will handle creation of the SKBs and
populate the appropriate SKB-fields. This work is currently stalled on a
generic way to transfer info needed for the SKB-fields, this related to XDP
*metadata* and *BTF* working areas.

* TODO Project cpumap

Tasks related to =cpumap= are filed under this section.

** TODO BUG: cpumap not working for generic-XDP

The =cpumap= does not handle redirect for generic-XDP.  This have already
confused people.  For completeness this should be fixed, but there is also
an opportunity for performance improvements.

** TODO Feature: cpumap bulk alloc SKBs

It would be fairly trivial to bulk alloc SKBs via =kmem_cache_alloc_bulk()=
API, when dequeuing in kthread (=cpu_map_kthread_run=).  One point is that
when dequeuing xdp_frames from ptr_ring, a cacheline between two CPUs are
touched, and we should reduce the time window as much as possible.  Then
there is of-cause also the performance improvement of bulk alloc.

** TODO Feature: cpumap could use netif_receive_skb_list()

** TODO Feature: cpumap implement GRO handling

** DONE Implement CPUMAP redirect with connection hashing over CPUs
CLOSED: [2018-08-10 Fri]

The xdp_redirect_cpu code should default distribute packets via IP-flow
hashing to avoid creating Out-of-Order packets, if someone just runs this
out-of-the box.

Upstream in commits:
- [[https://git.kernel.org/torvalds/c/c4c202175424][c4c202175424]] ("Merge branch 'bpf-sample-cpumap-lb'")
- [[https://git.kernel.org/torvalds/c/1bca4e6b1863][1bca4e6b1863]] ("samples/bpf: xdp_redirect_cpu load balance like Suricata")
- [[https://git.kernel.org/torvalds/c/11395686586b][11395686586b]] ("samples/bpf: add Paul Hsieh's (LGPL 2.1) hash function SuperFastHash")

** TODO sample xdp_redirect_cpu should output interface-name and ifindex

This is a minor improvement. QA is giving output from xdp_redirect_cpu, but
forget to say what interface this is loaded on.  Solve by simply outputting
the interface in the output. Even better would be outputting driver name
from corresponding ifindex.

** TODO Create BPF helper for skb_set_queue_mapping

When doing CPUMAP redirect, then the SKB created have no =skb->queue_mapping=
configured. This could be set by e.g. using TC filter/action =skbedit= (see man
[[https://www.linux.org/docs/man8/tc-skbedit.html][tc-skbedit(8)]]), but we also want to allow this from TC cls_bpf. Reading
=queue_mapping= from TC cls_bpf is already possible.

This is needed as part of a larger IP-forwarding use-case, where CPUMAP redirect
can be used for solving a TC root-qdisc locking problem, where
customers/services needed to be bandwidth rate-limited (and global rate-limit of
NIC is indifferent). This can be solved by using qdisc =MQ= and attaching a
qdisc =HTB= on each leaf of =MQ=, and via an eBPF prog make sure the
customers/services grouping is maintained and hit the same CPU+MQ TXq.

