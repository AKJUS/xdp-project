#+Title: Benchmarking "traits" via kernel module

Using the prototype-kernel (out-of-tree) time_bench framework for
micro-benchmarking "traits".

The feature under test is (currently) called "traits". It is a *compressed*
*key-value* *store*, that live in the top of the XDP packet data frame, just
after the struct =xdp_frame=.

The hope is to create a *fast and flexible* API for storing "hints" associated
with the packet. This is *one* of the ideas from LPC talk:
[[https://lpc.events/event/18/contributions/1935/][Marking Packets With Rich Metadata]]
by Arthur Fabre (Cloudflare) and Jakub Sitnicki (Cloudflare).

The question is:
 - Can we optimize API for be *fast-enough to satisfy XDP speed requirements?*

This document will help guide development to be a
 - *benchmark based development process* to satisfy XDP speed requirements

* Generate: Table of Contents                                           :toc:
- [[#code-under-test][Code under test]]
  - [[#kernel-tree-and-branch-under-test][Kernel tree and branch under test]]
- [[#prototype-kernel][prototype-kernel]]
  - [[#basic-benchmark-module-for-traits][Basic benchmark module for traits]]
  - [[#build-and-push-commands][Build and push commands]]
- [[#device-under-test-dut][Device Under Test (DUT)]]
  - [[#host-broadwell][Host: broadwell]]
- [[#benchmark-basics][Benchmark basics]]
  - [[#building-blocks][Building blocks]]

* Code under test

** Kernel tree and branch under test

Tested on top of kernel tree and branch:
 - https://github.com/arthurfabre/linux/tree/afabre/traits-002-bounds-inline

* prototype-kernel

** Basic benchmark module for traits

https://github.com/netoptimizer/prototype-kernel/pull/48/commits

Module loading:
#+begin_src sh
modprobe bench_traits_simple
#+end_src

** Build and push commands

The build process is documented here:
 - https://prototype-kernel.readthedocs.io/en/latest/prototype-kernel/build-process.html

For convenience listing the commands I use here:

Building:
#+begin_src sh
  $ dirs
  ~/git/prototype-kernel/kernel
  $ make kbuilddir=~/git/kernel/arthur/ -j12
#+end_src

Pushing to remote host:
#+begin_src sh
make push_remote kbuilddir=~/git/kernel/arthur/ HOST=broadwell
#+end_src

* Device Under Test (DUT)

** Host: broadwell

CPU info from =lscpu=:
#+begin_example
CPU(s):                   6
  On-line CPU(s) list:    0-5
Vendor ID:                GenuineIntel
  Model name:             Intel(R) Xeon(R) CPU E5-1650 v4 @ 3.60GHz
    CPU family:           6
    Model:                79
    Thread(s) per core:   1
    Core(s) per socket:   6
    CPU(s) scaling MHz:   35%
    CPU max MHz:          4000.0000
    CPU min MHz:          1200.0000
#+end_example

Notice disabled Hyper-Threading.

* Benchmark basics

XDP speed requirements are in the nanosec time range.

The packet rate determines the inter-gap between packets arriving for processing
by the Operating System (OS). This inter-gap directly translates into a
time-budget the OS have before the next packet arrive.

| Link speed | Packet rate           | Time-budget   |
|            | at smallest pkts size | per packet    |
|------------+-----------------------+---------------|
|  10 Gbit/s |  14,880,952 pps       | 67.2 nanosec  |
|  25 Gbit/s |  37,202,381 pps       | 26.88 nanosec |
| 100 Gbit/s | 148,809,523 pps       |  6.72 nanosec |

** Building blocks

The =bench_traits_simple= module contains some baseline tests, that measures
some of the building blocks, such that we get a sense the time scale
requirements.

*** for-loop

The tests usually consist of a for-loop getting measured. (Wrapped by
=time_bench_start()= and =time_bench_stop()=). One baseline test is an empty
for-loop for seeing what overhead that adds.

On host: broadwell:
#+begin_example
time_bench: Type:for_loop Per elem: 0 cycles(tsc) 0.265 ns (step:0)
- (measurement period time:0.027240766 sec time_interval:27240766)
- (invoke count:100000000 tsc_interval:98066760)
#+end_example

The nanosec cost is 0.265 ns and cycles(tsc) gets rounded down. From extra info
tsc_interval:98066760 and count:100000000 calc cycles is 0.98, which is very
close to 1 cycle. This is a 3.6GHz CPU, so 0.265*3.6 is 0.9540 cycles.

*** function calls

The function call overhead also consumes a surprisingly large part of the XDP
budget at the nanosec scale. Due to CPU side-channel mitigations, especially
calling via a function pointer is expensive.

On host: broadwell:
#+begin_example
time_bench: Type:function_call_cost Per elem: 4 cycles(tsc) 1.266 ns (step:0)
 - (measurement period time:0.126639966 sec time_interval:126639966)
 - (invoke count:100000000 tsc_interval:455908107)
time_bench: Type:func_ptr_call_cost Per elem: 30 cycles(tsc) 8.463 ns (step:0)
- (measurement period time:0.846375884 sec time_interval:846375884)
- (invoke count:100000000 tsc_interval:3046986747)
#+end_example

Doing a normal C function calls is not very expensive:
 - Type:function_call_cost = 4 cycles(tsc) 1.266 ns

The function pointer call is affected by mitigations:
 - Type:func_ptr_call_cost = 30 cycles(tsc) 8.463 ns
